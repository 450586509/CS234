{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_rl_seq2seq.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/08_rl_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDydjcUQyNI5",
        "colab_type": "text"
      },
      "source": [
        "#### 任务 \n",
        "Hebrew->English machine translation for words and short phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVRYmV1dyEMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If True, only translates phrases shorter than 20 characters (way easier).\n",
        "EASY_MODE = True\n",
        "# Useful for initial coding.\n",
        "# If false, works with all phrases (please switch to this mode for homework assignment)\n",
        "\n",
        "MODE = \"he-to-en\"  # way we translate. Either \"he-to-en\" or \"en-to-he\"\n",
        "# maximal length of _generated_ output, does not affect training\n",
        "MAX_OUTPUT_LENGTH = 50 if not EASY_MODE else 20\n",
        "REPORT_FREQ = 100  # how often to evaluate validation score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5WR2Gde6tfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cHHaS1Qz-no",
        "colab_type": "text"
      },
      "source": [
        "数据预处理\n",
        "\n",
        "\n",
        "数据的保存格式为：{ word1:[translation1,translation2,...], word2:[...],...}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9BiinrTx3kQ",
        "colab_type": "code",
        "outputId": "ea5123a1-a49e-495d-ee42-d35d8b63a626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "word_to_translation = defaultdict(list)  # our dictionary\n",
        "\n",
        "bos = '_'\n",
        "eos = ';'\n",
        "\n",
        "with open(\"main_dataset_small.txt\") as fin:\n",
        "    for line in fin:\n",
        "        en, he = line[:-1].lower().replace(bos, ' ').replace(eos,                                         ' ').split('\\t')\n",
        "        word, trans = (he, en) if MODE == 'he-to-en' else (en, he)\n",
        "        if len(word) < 3:\n",
        "            continue\n",
        "        if EASY_MODE:\n",
        "            if max(len(word), len(trans)) > 20:\n",
        "                continue\n",
        "        word_to_translation[word].append(trans)\n",
        "print(\"size = \", len(word_to_translation))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size =  486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFSd8pU50OKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get all unique lines in source language\n",
        "all_words = np.array(list(word_to_translation.keys()))\n",
        "# get all unique lines in translation language\n",
        "all_translations = np.array(\n",
        "    [ts for all_ts in word_to_translation.values() for ts in all_ts])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTe_bEzA07zU",
        "colab_type": "text"
      },
      "source": [
        "利用sklearn.model_selection中的工具分隔数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyVQwX1Z00tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_words, test_words = train_test_split(\n",
        "    all_words, test_size=0.1, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbMPM2Rc1DHR",
        "colab_type": "text"
      },
      "source": [
        "### 构造Vocab类。\n",
        "- 把字符转为数字\n",
        "- 把数字转为字符"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tci26jEvpq_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkU65bqC2Nhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokens, bos=\"__BOS__\", eos=\"__EOS__\", sep=''):\n",
        "        \"\"\"\n",
        "        A special class that handles tokenizing and detokenizing\n",
        "        \"\"\"\n",
        "        assert bos in tokens, eos in tokens\n",
        "        self.tokens = tokens\n",
        "        self.token_to_ix = {t: i for i, t in enumerate(tokens)}\n",
        "\n",
        "        self.bos = bos\n",
        "        self.bos_ix = self.token_to_ix[bos]\n",
        "        self.eos = eos\n",
        "        self.eos_ix = self.token_to_ix[eos]\n",
        "        self.sep = sep\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_lines(lines, bos=\"__BOS__\", eos=\"__EOS__\", sep=''):\n",
        "        flat_lines = sep.join(list(lines))\n",
        "        flat_lines = list(flat_lines.split(sep)) if sep else list(flat_lines)\n",
        "        tokens = list(set(sep.join(flat_lines)))\n",
        "        tokens = [t for t in tokens if t not in (bos, eos) and len(t) != 0]\n",
        "        tokens = [bos, eos] + tokens\n",
        "        return Vocab(tokens, bos, eos, sep)\n",
        "\n",
        "    def tokenize(self, string):\n",
        "        \"\"\"converts string to a list of tokens\"\"\"\n",
        "        tokens = list(filter(len, string.split(self.sep))) \\\n",
        "            if self.sep != '' else list(string)\n",
        "        return [self.bos] + tokens + [self.eos]\n",
        "\n",
        "    def to_matrix(self, lines, max_len=None):\n",
        "        \"\"\"\n",
        "        convert variable length token sequences into  fixed size matrix\n",
        "        example usage:\n",
        "        >>>print( as_matrix(words[:3],source_to_ix))\n",
        "        [[15 22 21 28 27 13 -1 -1 -1 -1 -1]\n",
        "         [30 21 15 15 21 14 28 27 13 -1 -1]\n",
        "         [25 37 31 34 21 20 37 21 28 19 13]]\n",
        "        \"\"\"\n",
        "        max_len = max_len or max(map(len, lines)) + 2  # 2 for bos and eos\n",
        "\n",
        "        matrix = np.zeros((len(lines), max_len), dtype='int32') + self.eos_ix\n",
        "        #print(\"matrix[0]={}\".format(matrix[0]))\n",
        "        for i, seq in enumerate(lines):\n",
        "            tokens = self.tokenize(seq)\n",
        "            row_ix = list(map(self.token_to_ix.get, tokens))[:max_len]\n",
        "            matrix[i, :len(row_ix)] = row_ix\n",
        "\n",
        "        return matrix\n",
        "\n",
        "    def to_lines(self, matrix, crop=True):\n",
        "        \"\"\"\n",
        "        Convert matrix of token ids into strings\n",
        "        :param matrix: matrix of tokens of int32, shape=[batch,time]\n",
        "        :param crop: if True, crops BOS and EOS from line\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        lines = []\n",
        "        for line_ix in map(list, matrix):\n",
        "            if crop:\n",
        "                if line_ix[0] == self.bos_ix:\n",
        "                    line_ix = line_ix[1:]\n",
        "                if self.eos_ix in line_ix:\n",
        "                    line_ix = line_ix[:line_ix.index(self.eos_ix)]\n",
        "            line = self.sep.join(self.tokens[i] for i in line_ix)\n",
        "            lines.append(line)\n",
        "        return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahuQvfEO1DqM",
        "colab_type": "code",
        "outputId": "21ea3b15-ce82-4553-b527-34ecad467294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "\n",
        "inp_voc = Vocab.from_lines(''.join(all_words), bos=bos, eos=eos, sep='')\n",
        "out_voc = Vocab.from_lines(''.join(all_translations), bos=bos, eos=eos, sep='')\n",
        "\n",
        "# Here's how you cast lines into ids and backwards.\n",
        "batch_lines = all_words[:5]\n",
        "batch_ids = inp_voc.to_matrix(batch_lines)\n",
        "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
        "\n",
        "print(\"lines\")\n",
        "print(batch_lines)\n",
        "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
        "print(batch_ids)\n",
        "print(\"\\nback to words\")\n",
        "print(batch_lines_restored)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines\n",
            "['אנרכיזם' 'אוטיזם קלאסי' 'אלבדו' 'אלבמה' 'אכילס']\n",
            "\n",
            "words to ids (0 = bos, 1 = eos):\n",
            "[[ 0 44 42 36 10 14 47 59  1  1  1  1  1  1]\n",
            " [ 0 44 11 24 14 47 59 40 13  7 44 30 14  1]\n",
            " [ 0 44  7  9 46 11  1  1  1  1  1  1  1  1]\n",
            " [ 0 44  7  9 16 34  1  1  1  1  1  1  1  1]\n",
            " [ 0 44 10 14  7 30  1  1  1  1  1  1  1  1]]\n",
            "\n",
            "back to words\n",
            "['אנרכיזם', 'אוטיזם קלאסי', 'אלבדו', 'אלבמה', 'אכילס']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBaw-SjEpEr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59u-B03Q2FDR",
        "colab_type": "code",
        "outputId": "f43fe5a5-3ae2-4089-80e7-c18af98e0326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=[8, 4])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"words\")\n",
        "plt.hist(list(map(len, all_words)), bins=20)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('translations')\n",
        "plt.hist(list(map(len, all_translations)), bins=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 7., 18., 35., 41., 45., 71.,  0., 68., 25., 28., 23., 20., 23.,\n",
              "         0., 17., 14., 17., 16., 13., 10.]),\n",
              " array([ 3.  ,  3.85,  4.7 ,  5.55,  6.4 ,  7.25,  8.1 ,  8.95,  9.8 ,\n",
              "        10.65, 11.5 , 12.35, 13.2 , 14.05, 14.9 , 15.75, 16.6 , 17.45,\n",
              "        18.3 , 19.15, 20.  ]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAEICAYAAACHwyd6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s0a-8ap3KfB",
        "colab_type": "text"
      },
      "source": [
        "### 3.实现Encoder-Decoder模型\n",
        "\n",
        "seq2seq模型model提供以下接口：\n",
        "- model.symbolic_translate(inp, **flags)-> out, logp：输入为：\n",
        "- model.symbolic_score(inp, out, **flags) -> logp\n",
        "- model.weights：所有层的权重。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ402oKX2VJ9",
        "colab_type": "code",
        "outputId": "0d737fdf-6aaf-4fa5-dd0c-d4400538079a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as L\n",
        "%tensorflow_version 1.x \n",
        "tf.reset_default_graph()\n",
        "s = tf.InteractiveSession()\n",
        "\n",
        "# ^^^ if you get \"variable *** already exists\": re-run this cell again"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcem20hJAr5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KerasBasicTranslationModel:\n",
        "  def __init__(inp_voc, out_voc, emb_size, hid_size):\n",
        "    self.inp_voc = inp_voc\n",
        "    self.out_voc = out_voc\n",
        "    self.hidden_size = hid_size\n",
        "    #self.inp_voc = inp_voc\n",
        "    #self.out_voc = out_voc\n",
        "    with tf.variable_scope(name):\n",
        "      self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
        "      self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
        "      self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
        "      self.dec_start = L.Dense(hid_size)\n",
        "      self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
        "      self.logit_layer = L.Dense(len(out_voc))\n",
        "\n",
        "      inp = tf.placeholder('int32', [None, None])\n",
        "      out = tf.placeholder('int32', [None, None])\n",
        "      encoder_states = self.encode(inp)\n",
        "      self.decoder_outputs = self.decode(out, encoder_states)\n",
        "      self.decode_out = self.logit_layer(self.decoder_outputs)\n",
        "      self.decode_logits = tf.nn.softmax(self.decode_out, axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "  def encode(inp):\n",
        "      inp_emb = self.emb_inp(inp)\n",
        "      encode_layer = LSTM(self.hidden_size, return_state=True)\n",
        "      encoder_outputs, state_h, state_c = encode_layer(inp_emb)\n",
        "      return [state_h, state_c]\n",
        "\n",
        "  def decode(out, encoder_states):\n",
        "      decoder_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True)\n",
        "      decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "      return decoder_outputs\n",
        "\n",
        "  def symbolic_score(self, inp, out, eps=1e-30, **flags):\n",
        "    #:return: log-probabilities of all possible english characters of shape [bath,time,n_tokens]\n",
        "    #decode时，每一步的概率。\n",
        "    return self.decode_logits\n",
        "\n",
        "\n",
        "  def symbolic_translate(\n",
        "            self,\n",
        "            inp,\n",
        "            greedy=False,\n",
        "            max_len=None,\n",
        "            eps=1e-30,\n",
        "            **flags):\n",
        "    pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j0begV-4587",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 本部分实现单层的GRU encoder-decoder模型。\n",
        "# 注意事项：\n",
        "# 1: when using several recurrent layers TF can mixed up the weights of different recurrent layers.\n",
        "# In that case, make sure you both create AND use each rnn/gru/lstm/custom layer in a unique variable scope\n",
        "# e.g. with tf.variable_scope(\"first_lstm\"): new_cell, new_out = self.lstm_1(...)\n",
        "#      with tf.variable_scope(\"second_lstm\"): new_cell2, new_out2 = self.lstm_2(...)\n",
        "# Note 2: everything you need for decoding should be stored in model state (output list of both encode and decode)\n",
        "# e.g. for attention, you should store all encoder sequence and input mask\n",
        "# there in addition to lstm/gru states.\n",
        "\n",
        "class BasicTranslationModel:\n",
        "    def __init__(self, name, inp_voc, out_voc,\n",
        "                 emb_size, hid_size,):\n",
        "\n",
        "        self.name = name\n",
        "        self.inp_voc = inp_voc\n",
        "        self.out_voc = out_voc\n",
        "\n",
        "        with tf.variable_scope(name):\n",
        "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
        "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
        "            self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
        "            self.dec_start = L.Dense(hid_size)\n",
        "            self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
        "            self.logits = L.Dense(len(out_voc))\n",
        "\n",
        "            # run on dummy output to .build all layers (and therefore create\n",
        "            # weights)\n",
        "            inp = tf.placeholder('int32', [None, None])\n",
        "            out = tf.placeholder('int32', [None, None])\n",
        "            h0 = self.encode(inp)\n",
        "            h1 = self.decode(h0, out[:, 0])\n",
        "            # h2 = self.decode(h1,out[:,1]) etc.\n",
        "\n",
        "        self.weights = tf.get_collection(\n",
        "            tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :return: a list of initial decoder state tensors\n",
        "        \"\"\"\n",
        "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
        "        inp_emb = self.emb_inp(inp)\n",
        "\n",
        "        _, enc_last = tf.nn.dynamic_rnn(\n",
        "            self.enc0, inp_emb,\n",
        "            sequence_length=inp_lengths,\n",
        "            dtype=inp_emb.dtype)\n",
        "\n",
        "        dec_start = self.dec_start(enc_last)\n",
        "        return [dec_start]\n",
        "\n",
        "    def decode(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits\n",
        "        :param prev_state: a list of previous decoder state tensors\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n",
        "        \"\"\"\n",
        "\n",
        "        [prev_dec] = prev_state\n",
        "\n",
        "        prev_emb = self.emb_out(prev_tokens[:, None])[:, 0]\n",
        "\n",
        "        new_dec_out, new_dec_state = self.dec0(prev_emb, prev_dec)\n",
        "\n",
        "        output_logits = self.logits(new_dec_out)\n",
        "\n",
        "        return [new_dec_state], output_logits\n",
        "\n",
        "    def symbolic_score(self, inp, out, eps=1e-30, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic int32 matrices of hebrew words and their english translations.\n",
        "        Computes the log-probabilities of all possible english characters given english prefices and hebrew word.\n",
        "        :param inp: input sequence, int32 matrix of shape [batch,time]\n",
        "        :param out: output sequence, int32 matrix of shape [batch,time]\n",
        "        :return: log-probabilities of all possible english characters of shape [bath,time,n_tokens]\n",
        "        NOTE: log-probabilities time axis  is synchronized with out\n",
        "        In other words, logp are probabilities of __current__ output at each tick, not the next one\n",
        "        therefore you can get likelihood as logprobas * tf.one_hot(out,n_tokens)\n",
        "        \"\"\"\n",
        "        first_state = self.encode(inp, **flags)\n",
        "\n",
        "        batch_size = tf.shape(inp)[0]\n",
        "        bos = tf.fill([batch_size], self.out_voc.bos_ix)\n",
        "        first_logits = tf.log(tf.one_hot(bos, len(self.out_voc)) + eps)\n",
        "\n",
        "        def step(blob, y_prev):\n",
        "            h_prev = blob[:-1]\n",
        "            h_new, logits = self.decode(h_prev, y_prev, **flags)\n",
        "            return list(h_new) + [logits]\n",
        "\n",
        "        results = tf.scan(step, initializer=list(first_state) + [first_logits],\n",
        "                          elems=tf.transpose(out))\n",
        "\n",
        "        # gather state and logits, each of shape [time,batch,...]\n",
        "        states_seq, logits_seq = results[:-1], results[-1]\n",
        "\n",
        "        # add initial state and logits\n",
        "        logits_seq = tf.concat((first_logits[None], logits_seq), axis=0)\n",
        "\n",
        "        # convert from [time,batch,...] to [batch,time,...]\n",
        "        logits_seq = tf.transpose(logits_seq, [1, 0, 2])\n",
        "\n",
        "        return tf.nn.log_softmax(logits_seq)\n",
        "\n",
        "    def symbolic_translate(\n",
        "            self,\n",
        "            inp,\n",
        "            greedy=False,\n",
        "            max_len=None,\n",
        "            eps=1e-30,\n",
        "            **flags):\n",
        "        \"\"\"\n",
        "        takes symbolic int32 matrix of hebrew words, produces output tokens sampled\n",
        "        from the model and output log-probabilities for all possible tokens at each tick.\n",
        "        :param inp: input sequence, int32 matrix of shape [batch,time]\n",
        "        :param greedy: if greedy, takes token with highest probablity at each tick.\n",
        "            Otherwise samples proportionally to probability.\n",
        "        :param max_len: max length of output, defaults to 2 * input length\n",
        "        :return: output tokens int32[batch,time] and\n",
        "                 log-probabilities of all tokens at each tick, [batch,time,n_tokens]\n",
        "        \"\"\"\n",
        "        first_state = self.encode(inp, **flags)\n",
        "\n",
        "        batch_size = tf.shape(inp)[0]\n",
        "        bos = tf.fill([batch_size], self.out_voc.bos_ix)\n",
        "        first_logits = tf.log(tf.one_hot(bos, len(self.out_voc)) + eps)\n",
        "        max_len = tf.reduce_max(tf.shape(inp)[1]) * 2\n",
        "\n",
        "        def step(blob, t):\n",
        "            h_prev, y_prev = blob[:-2], blob[-1]\n",
        "            h_new, logits = self.decode(h_prev, y_prev, **flags)\n",
        "            y_new = (\n",
        "                tf.argmax(logits, axis=-1) if greedy\n",
        "                else tf.multinomial(logits, 1)[:, 0]\n",
        "            )\n",
        "            return list(h_new) + [logits, tf.cast(y_new, y_prev.dtype)]\n",
        "\n",
        "        results = tf.scan(\n",
        "            step,\n",
        "            initializer=list(first_state) + [first_logits, bos],\n",
        "            elems=[tf.range(max_len)],\n",
        "        )\n",
        "\n",
        "        # gather state, logits and outs, each of shape [time,batch,...]\n",
        "        states_seq, logits_seq, out_seq = (\n",
        "            results[:-2], results[-2], results[-1]\n",
        "        )\n",
        "\n",
        "        # add initial state, logits and out\n",
        "        logits_seq = tf.concat((first_logits[None], logits_seq), axis=0)\n",
        "        out_seq = tf.concat((bos[None], out_seq), axis=0)\n",
        "        states_seq = [\n",
        "            tf.concat((init[None], states), axis=0)\n",
        "            for init, states in zip(first_state, states_seq)\n",
        "        ]\n",
        "\n",
        "        # convert from [time,batch,...] to [batch,time,...]\n",
        "        logits_seq = tf.transpose(logits_seq, [1, 0, 2])\n",
        "        out_seq = tf.transpose(out_seq)\n",
        "        states_seq = [\n",
        "            tf.transpose(states, [1, 0] + list(range(2, states.shape.ndims)))\n",
        "            for states in states_seq\n",
        "        ]\n",
        "\n",
        "        return out_seq, tf.nn.log_softmax(logits_seq)\n",
        "\n",
        "\n",
        "### Utility functions ###\n",
        "\n",
        "def initialize_uninitialized(sess=None):\n",
        "    \"\"\"\n",
        "    Initialize unitialized variables, doesn't affect those already initialized\n",
        "    :param sess: in which session to initialize stuff. Defaults to tf.get_default_session()\n",
        "    \"\"\"\n",
        "    sess = sess or tf.get_default_session()\n",
        "    global_vars = tf.global_variables()\n",
        "    is_not_initialized = sess.run(\n",
        "        [tf.is_variable_initialized(var) for var in global_vars]\n",
        "    )\n",
        "    not_initialized_vars = [\n",
        "        v for (v, f)\n",
        "        in zip(global_vars, is_not_initialized)\n",
        "        if not f\n",
        "    ]\n",
        "\n",
        "    if len(not_initialized_vars):\n",
        "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
        "\n",
        "\n",
        "def infer_length(seq, eos_ix, time_major=False, dtype=tf.int32):\n",
        "    \"\"\"\n",
        "    compute length given output indices and eos code\n",
        "    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n",
        "    :param eos_ix: integer index of end-of-sentence token\n",
        "    :returns: lengths, int32 vector of shape [batch]\n",
        "    \"\"\"\n",
        "    axis = 0 if time_major else 1\n",
        "    is_eos = tf.cast(tf.equal(seq, eos_ix), dtype)\n",
        "    count_eos = tf.cumsum(is_eos, axis=axis, exclusive=True)\n",
        "    lengths = tf.reduce_sum(tf.cast(tf.equal(count_eos, 0), dtype), axis=axis)\n",
        "    return lengths\n",
        "\n",
        "\n",
        "def infer_mask(seq, eos_ix, time_major=False, dtype=tf.float32):\n",
        "    \"\"\"\n",
        "    compute mask given output indices and eos code\n",
        "    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n",
        "    :param eos_ix: integer index of end-of-sentence token\n",
        "    :returns: mask, float32 matrix with '0's and '1's of same shape as seq\n",
        "    \"\"\"\n",
        "    axis = 0 if time_major else 1\n",
        "    lengths = infer_length(seq, eos_ix, time_major=time_major)\n",
        "    mask = tf.sequence_mask(lengths, maxlen=tf.shape(seq)[axis], dtype=dtype)\n",
        "    if time_major:\n",
        "        mask = tf.transpose(mask)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def select_values_over_last_axis(values, indices):\n",
        "    \"\"\"\n",
        "    Auxiliary function to select logits corresponding to chosen tokens.\n",
        "    :param values: logits for all actions: float32[batch,tick,action]\n",
        "    :param indices: action ids int32[batch,tick]\n",
        "    :returns: values selected for the given actions: float[batch,tick]\n",
        "    \"\"\"\n",
        "    assert values.shape.ndims == 3 and indices.shape.ndims == 2\n",
        "    batch_size, seq_len = tf.shape(indices)[0], tf.shape(indices)[1]\n",
        "    batch_i = tf.tile(tf.range(0, batch_size)[:, None], [1, seq_len])\n",
        "    time_i = tf.tile(tf.range(0, seq_len)[None, :], [batch_size, 1])\n",
        "    indices_nd = tf.stack([batch_i, time_i, indices], axis=-1)\n",
        "\n",
        "    return tf.gather_nd(values, indices_nd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCK4NAw548gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BasicTranslationModel('model', inp_voc, out_voc,emb_size=64, hid_size=128)\n",
        "\n",
        "s.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxkbBoAO_tu-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6L1DRZu5DEr",
        "colab_type": "code",
        "outputId": "95a7139a-25ed-4e4b-be43-40d418978ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "# Play around with symbolic_translate and symbolic_score\n",
        "inp = tf.placeholder_with_default(np.random.randint(\n",
        "    0, 10, [3, 5], dtype='int32'), [None, None])\n",
        "out = tf.placeholder_with_default(np.random.randint(\n",
        "    0, 10, [3, 5], dtype='int32'), [None, None])\n",
        "\n",
        "# translate inp (with untrained model)\n",
        "sampled_out, logp = model.symbolic_translate(inp, greedy=False)\n",
        "print(\"\\nSymbolic_translate output:\\n\", sampled_out, logp)\n",
        "print(\"\\nSample translations:\\n\", s.run(sampled_out))\n",
        "\n",
        "# score logp(out | inp) with untrained input\n",
        "logp = model.symbolic_score(inp, out)\n",
        "print(\"\\nSymbolic_score output:\\n\", logp)\n",
        "print(\"\\nLog-probabilities (clipped):\\n\", s.run(logp)[:, :2, :5])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Symbolic_translate output:\n",
            " Tensor(\"transpose_1:0\", shape=(?, ?), dtype=int32) Tensor(\"LogSoftmax:0\", shape=(?, ?, 49), dtype=float32)\n",
            "\n",
            "Sample translations:\n",
            " [[ 0 34 45  1 21 27  1 41 15  7  7]\n",
            " [ 0  0  2 26 28 21 37  6 15  2 25]\n",
            " [ 0 29  9 19 36 37  1 41 16 33 12]]\n",
            "\n",
            "Symbolic_score output:\n",
            " Tensor(\"LogSoftmax_1:0\", shape=(?, ?, 49), dtype=float32)\n",
            "\n",
            "Log-probabilities (clipped):\n",
            " [[[  0.        -69.07755   -69.07755   -69.07755   -69.07755  ]\n",
            "  [ -3.883887   -3.8835962  -3.891128   -3.9000025  -3.8954918]]\n",
            "\n",
            " [[  0.        -69.07755   -69.07755   -69.07755   -69.07755  ]\n",
            "  [ -3.883887   -3.8835962  -3.891128   -3.9000025  -3.8954918]]\n",
            "\n",
            " [[  0.        -69.07755   -69.07755   -69.07755   -69.07755  ]\n",
            "  [ -3.9003222  -3.890575   -3.8988514  -3.8998508  -3.8977628]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckx5mX165OpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare any operations you want here\n",
        "input_sequence = tf.placeholder('int32', [None, None])\n",
        "greedy_translations, logp = model.symbolic_translate(input_sequence, greedy=True)\n",
        "\n",
        "\n",
        "def translate(lines):\n",
        "    \"\"\"\n",
        "    You are given a list of input lines. \n",
        "    Make your neural network translate them.\n",
        "    :return: a list of output lines\n",
        "    \"\"\"\n",
        "    # Convert lines to a matrix of indices\n",
        "    lines_ix = inp_voc.to_matrix(lines)\n",
        "    print(\"lines_ix shape={}\".format(lines_ix.shape))\n",
        "\n",
        "    # Compute translations in form of indices\n",
        "    trans_ix = s.run(greedy_translations, {input_sequence:lines_ix})\n",
        "\n",
        "    # Convert translations back into strings\n",
        "    return out_voc.to_lines(trans_ix)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLbPm9mG5bDj",
        "colab_type": "code",
        "outputId": "8b396ce0-42c8-44c2-f279-7bc62c1e740e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"Sample inputs:\", all_words[:3])\n",
        "print(\"Dummy translations:\", translate(all_words[:3]))\n",
        "\n",
        "assert isinstance(greedy_translations,\n",
        "                  tf.Tensor) and greedy_translations.dtype.is_integer, \"trans must be a tensor of integers (token ids)\"\n",
        "assert translate(all_words[:3]) == translate(\n",
        "    all_words[:3]), \"make sure translation is deterministic (use greedy=True and disable any noise layers)\"\n",
        "assert type(translate(all_words[:3])) is list and (type(translate(all_words[:1])[0]) is str or type(\n",
        "    translate(all_words[:1])[0]) is unicode), \"translate(lines) must return a sequence of strings!\"\n",
        "print(\"Tests passed!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample inputs: ['אנרכיזם' 'אוטיזם קלאסי' 'אלבדו']\n",
            "lines_ix shape=(3, 14)\n",
            "Dummy translations: ['ææææææææævvvvvv6v6v6axaxacmd', 'ææææææææædvvvvvv6v6v6axaccmd', \"èqqh'r'07h7hllllllallaxaxfax\"]\n",
            "lines_ix shape=(3, 14)\n",
            "lines_ix shape=(3, 14)\n",
            "lines_ix shape=(3, 14)\n",
            "lines_ix shape=(1, 9)\n",
            "Tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmb_E_hdaHTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 打分函数"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U1ATUqRaC-e",
        "colab_type": "code",
        "outputId": "3f920ba3-552e-424e-832c-a815237b0a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install editdistance\n",
        "import editdistance  # !pip install editdistance\n",
        "\n",
        "\n",
        "def get_distance(word, trans):\n",
        "    \"\"\"\n",
        "    A function that takes word and predicted translation\n",
        "    and evaluates (Levenshtein's) edit distance to closest correct translation\n",
        "    \"\"\"\n",
        "    references = word_to_translation[word]\n",
        "    assert len(references) != 0, \"wrong/unknown word\"\n",
        "    return min(editdistance.eval(trans, ref) for ref in references)\n",
        "\n",
        "\n",
        "def score(words, bsize=8):\n",
        "    \"\"\"a function that computes levenshtein distance for bsize random samples\"\"\"\n",
        "    assert isinstance(words, np.ndarray)\n",
        "\n",
        "    batch_words = np.random.choice(words, size=bsize, replace=False)\n",
        "    batch_trans = translate(batch_words)\n",
        "\n",
        "    distances = list(map(get_distance, batch_words, batch_trans))\n",
        "\n",
        "    return np.array(distances, dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (0.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFfU773QaNZ6",
        "colab_type": "code",
        "outputId": "894a9c21-5462-4488-946c-765cb11d2b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "[score(test_words, 10).mean() for _ in range(5)]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines_ix shape=(10, 19)\n",
            "lines_ix shape=(10, 16)\n",
            "lines_ix shape=(10, 21)\n",
            "lines_ix shape=(10, 20)\n",
            "lines_ix shape=(10, 19)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[30.4, 30.6, 37.9, 38.0, 33.6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adu6CLgHaWjO",
        "colab_type": "text"
      },
      "source": [
        "### 开始训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFEKkRyRaVza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import utility functions\n",
        "#from basic_model_tf import initialize_uninitialized, infer_length, infer_mask, select_values_over_last_axis\n",
        "\n",
        "\n",
        "class supervised_training:\n",
        "\n",
        "    # variable for inputs and correct answers\n",
        "    input_sequence = tf.placeholder('int32', [None, None])\n",
        "    reference_answers = tf.placeholder('int32', [None, None])\n",
        "\n",
        "    # Compute log-probabilities of all possible tokens at each step. Use model interface.\n",
        "    logprobs_seq = model.symbolic_score(input_sequence, reference_answers)\n",
        "\n",
        "    # compute mean crossentropy\n",
        "    crossentropy = - select_values_over_last_axis(logprobs_seq, reference_answers)\n",
        "\n",
        "    mask = infer_mask(reference_answers, out_voc.eos_ix)\n",
        "\n",
        "    loss = tf.reduce_sum(crossentropy * mask)/tf.reduce_sum(mask)\n",
        "\n",
        "    # Build weights optimizer. Use model.weights to get all trainable params.\n",
        "    train_step = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "\n",
        "# intialize optimizer params while keeping model intact\n",
        "initialize_uninitialized(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh2njBSnaRM8",
        "colab_type": "code",
        "outputId": "75f56fa5-d601-48bc-a230-7e2b5f283756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def sample_batch(words, word_to_translation, batch_size):\n",
        "    \"\"\"\n",
        "    sample random batch of words and random correct translation for each word\n",
        "    example usage:\n",
        "    batch_x,batch_y = sample_batch(train_words, word_to_translations,10)\n",
        "    \"\"\"\n",
        "    # choose words\n",
        "    batch_words = np.random.choice(words, size=batch_size)\n",
        "\n",
        "    # choose translations\n",
        "    batch_trans_candidates = list(map(word_to_translation.get, batch_words))\n",
        "    batch_trans = list(map(random.choice, batch_trans_candidates))\n",
        "\n",
        "    return inp_voc.to_matrix(batch_words), out_voc.to_matrix(batch_trans)\n",
        "    \n",
        "bx, by = sample_batch(train_words, word_to_translation, batch_size=3)\n",
        "print(\"Source:\")\n",
        "print(bx)\n",
        "print(\"Target:\")\n",
        "print(by)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source:\n",
            "[[ 0  5 35 40 57 19 12  1  1  1]\n",
            " [ 0 44  7  9 46 11  1  1  1  1]\n",
            " [ 0 44 13 11 36 46 14 11 27  1]]\n",
            "Target:\n",
            "[[ 0 16  8 46 26  3 44  2  2  1  1]\n",
            " [ 0 16 46 39  2 33 29  1  1  1  1]\n",
            " [ 0 16 14 14 29 44 33 25 29  6  1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg643JAeksHM",
        "colab_type": "code",
        "outputId": "dea67a0b-f615-467a-9058-a4983d0141c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word_to_translation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "486"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QU7Jp9Yaseu",
        "colab_type": "code",
        "outputId": "f4d4cdad-0410-41e4-f5e4-b202b7bcdd25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm, trange  # or use tqdm_notebook,tnrange\n",
        "\n",
        "loss_history = []\n",
        "editdist_history = []\n",
        "\n",
        "for i in trange(25):\n",
        "    bx, by = sample_batch(train_words, word_to_translation, 16)\n",
        "\n",
        "    feed_dict = {\n",
        "        supervised_training.input_sequence: bx,\n",
        "        supervised_training.reference_answers: by\n",
        "    }\n",
        "\n",
        "    loss, _ = s.run([supervised_training.loss,\n",
        "                     supervised_training.train_step], feed_dict)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    if (i+1) % REPORT_FREQ == 0:\n",
        "        clear_output(True)\n",
        "        current_scores = score(test_words)\n",
        "        editdist_history.append(current_scores.mean())\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(131)\n",
        "        plt.title('train loss / traning time')\n",
        "        plt.plot(loss_history)\n",
        "        plt.grid()\n",
        "        plt.subplot(132)\n",
        "        plt.title('val score distribution')\n",
        "        plt.hist(current_scores, bins=20)\n",
        "        plt.subplot(133)\n",
        "        plt.title('val score / traning time')\n",
        "        plt.plot(editdist_history)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "        print(\"llh=%.3f, mean score=%.3f\" %\n",
        "              (np.mean(loss_history[-10:]), np.mean(editdist_history[-10:])))\n",
        "\n",
        "# Note: it's okay if loss oscillates up and down as long as it gets better on average over long term (e.g. 5k batches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  4%|▍         | 1/25 [00:00<00:05,  4.67it/s]\u001b[A\n",
            " 16%|█▌        | 4/25 [00:00<00:03,  6.17it/s]\u001b[A\n",
            " 28%|██▊       | 7/25 [00:00<00:02,  7.94it/s]\u001b[A\n",
            " 40%|████      | 10/25 [00:00<00:01,  9.93it/s]\u001b[A\n",
            " 52%|█████▏    | 13/25 [00:00<00:00, 12.11it/s]\u001b[A\n",
            " 64%|██████▍   | 16/25 [00:00<00:00, 14.03it/s]\u001b[A\n",
            " 76%|███████▌  | 19/25 [00:00<00:00, 16.12it/s]\u001b[A\n",
            " 88%|████████▊ | 22/25 [00:01<00:00, 17.97it/s]\u001b[A\n",
            "100%|██████████| 25/25 [00:01<00:00, 19.35it/s]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpTUM6v6avXQ",
        "colab_type": "code",
        "outputId": "f39f5d16-1280-4dd2-f9cd-789b9b3eaf65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "for word in train_words[:10]:\n",
        "    print(\"%s -> %s\" % (word, translate([word])[0]))\n",
        "\n",
        "test_scores = []\n",
        "for start_i in trange(0, len(test_words), 32):\n",
        "    batch_words = test_words[start_i:start_i+32]\n",
        "    batch_trans = translate(batch_words)\n",
        "    distances = list(map(get_distance, batch_words, batch_trans))\n",
        "    test_scores.extend(distances)\n",
        "\n",
        "print(\"Supervised test score:\", np.mean(test_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines_ix shape=(1, 12)\n",
            "הצעה צנועה -> aalll\n",
            "lines_ix shape=(1, 12)\n",
            "אפטוזאורוס -> aalll\n",
            "lines_ix shape=(1, 6)\n",
            "אשור -> aalll\n",
            "lines_ix shape=(1, 14)\n",
            "מרוץ מכוניות -> aalll\n",
            "lines_ix shape=(1, 10)\n",
            "אן ארבור -> aalll\n",
            "lines_ix shape=(1, 12)\n",
            "21 באוגוסט -> aalll\n",
            "lines_ix shape=(1, 10)\n",
            "אפרודיטה -> aalll\n",
            "lines_ix shape=(1, 7)\n",
            "אפריל -> aalll\n",
            "lines_ix shape=(1, 11)\n",
            "אל-קאעידה -> aalll\n",
            "lines_ix shape=(1, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00, 26.36it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "אסתטיקה -> aalll\n",
            "lines_ix shape=(32, 19)\n",
            "lines_ix shape=(17, 21)\n",
            "Supervised test score: 8.142857142857142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWG4okZgeu21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQrTTe7njmcY",
        "colab_type": "text"
      },
      "source": [
        "### 自定义损失函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyz9hoHMn7J-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_ix_to_token = dict([(v,k) for k, v in inp_voc.token_to_ix.items()])\n",
        "trans_ix_to_token = dict([(v,k) for k, v in out_voc.token_to_ix.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBBaRAbkkIw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _compute_levenshtein(words_ix, trans_ix):\n",
        "    \"\"\"\n",
        "    A custom tensorflow operation that computes levenshtein loss for predicted trans.\n",
        "\n",
        "    Params:\n",
        "    - words_ix - a matrix of letter indices, shape=[batch_size,word_length]\n",
        "    - words_mask - a matrix of zeros/ones, \n",
        "       1 means \"word is still not finished\"\n",
        "       0 means \"word has already finished and this is padding\"\n",
        "\n",
        "    - trans_mask - a matrix of output letter indices, shape=[batch_size,translation_length]\n",
        "    - trans_mask - a matrix of zeros/ones, similar to words_mask but for trans_ix\n",
        "\n",
        "\n",
        "    Please implement the function and make sure it passes tests from the next cell.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # convert words to strings\n",
        "    # map(word_to_translation.get, batch_words)\n",
        "    words_str = inp_voc.to_lines(words_ix)\n",
        "    words = list(map(word_to_translation.get, words_str))\n",
        "    words = [w[0] for w in words]\n",
        "\n",
        "    assert type(words) is list and type(\n",
        "        words[0]) is str and len(words) == len(words_ix)\n",
        "\n",
        "    translations = out_voc.to_lines(trans_ix)\n",
        "\n",
        "    assert type(translations) is list and type(\n",
        "        translations[0]) is str and len(translations) == len(trans_ix)\n",
        "\n",
        "    # computes levenstein distances. can be arbitrary python code.\n",
        "    distances = [editdistance.eval(w, t) for w, t in zip(words, translations)]\n",
        "\n",
        "    assert type(distances) in (list, tuple, np.ndarray) and len(\n",
        "        distances) == len(words_ix)\n",
        "\n",
        "    distances = np.array(list(distances), dtype='float32')\n",
        "    return distances\n",
        "\n",
        "def compute_levenshtein(words_ix, trans_ix):\n",
        "    out = tf.py_func(_compute_levenshtein, [words_ix, trans_ix, ], tf.float32)\n",
        "    out.set_shape([None])\n",
        "\n",
        "    return tf.stop_gradient(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgF-I-0mp0HW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVNhL7kreB8Y",
        "colab_type": "code",
        "outputId": "d5c79cbc-6278-4699-ea0a-92a9fa958d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(batch_words[0])\n",
        "batch_words_num = inp_voc.to_matrix(batch_words)\n",
        "print(batch_words_num[0])\n",
        "print(list(map(words_ix_to_token.get, batch_words_num[0])))\n",
        "#print(inp_voc.to_lines(batch_words_ix[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "אן ברונטה\n",
            "[ 0 44 27 40  9 36 11 42 24 34  1  1  1  1  1  1  1  1  1  1  1]\n",
            "['_', 'א', 'ן', ' ', 'ב', 'ר', 'ו', 'נ', 'ט', 'ה', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEQIItqBk3DI",
        "colab_type": "code",
        "outputId": "410203e1-44d1-4883-a16f-0ba5962e87d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# 测试上面代码\n",
        "# test suite\n",
        "# sample random batch of (words, correct trans, wrong trans)\n",
        "batch_words = np.random.choice(train_words, size=100)\n",
        "\n",
        "batch_trans = list(map(random.choice, map(\n",
        "    word_to_translation.get, batch_words)))\n",
        "batch_trans_wrong = np.random.choice(all_translations, size=100)\n",
        "\n",
        "batch_words_ix = tf.constant(inp_voc.to_matrix(batch_words))\n",
        "batch_trans_ix = tf.constant(out_voc.to_matrix(batch_trans))\n",
        "batch_trans_wrong_ix = tf.constant(out_voc.to_matrix(batch_trans_wrong))\n",
        "\n",
        "# assert compute_levenshtein is zero for ideal translations\n",
        "correct_answers_score = compute_levenshtein(\n",
        "    batch_words_ix, batch_trans_ix).eval()\n",
        "print(\"score={}\".format(correct_answers_score))\n",
        "assert np.all(correct_answers_score ==0), \"a perfect translation got nonzero levenshtein score!\"\n",
        "\n",
        "print(\"Everything seems alright!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "Everything seems alright!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4iNrSX4r3QL",
        "colab_type": "text"
      },
      "source": [
        "### self-critical policy gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yecBY47tsMkb",
        "colab_type": "text"
      },
      "source": [
        "实现 self-critical sequence training.[论文](https://arxiv.org/abs/1612.00563)\n",
        "\n",
        "policy-based的优化方法，但是b的计算方法有点不一样。\n",
        "\n",
        "$$ \\nabla J = E_{x \\sim p(s)} E_{y \\sim \\pi(y|x)} \\nabla log \\pi(y|x) \\cdot (R(x,y) - b(x)) $$\n",
        "reward R(x,y)为负编辑距离，因为我们是最小化J。 \n",
        "\n",
        "The baseline b(x) represents how well model fares on word x.\n",
        "\n",
        "In practice, this means that we compute baseline as a score of greedy translation, $b(x) = R(x,y_{greedy}(x)) $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKLTJO6MMyDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HHu9aYcr9AB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class trainer:\n",
        "\n",
        "    input_sequence = tf.placeholder('int32', [None, None])\n",
        "\n",
        "    # use model to __sample__ symbolic translations given input_sequence\n",
        "    sample_translations, sample_logp = model.symbolic_translate(input_sequence, greedy=False)\n",
        "    # use model to __greedy__ symbolic translations given input_sequence\n",
        "    greedy_translations, greedy_logp = model.symbolic_translate(input_sequence, greedy=True)\n",
        "\n",
        "    rewards = - compute_levenshtein(input_sequence, sample_translations)\n",
        "\n",
        "    # compute __negative__ levenshtein for greedy mode\n",
        "    baseline = - compute_levenshtein(input_sequence, greedy_translations)\n",
        "\n",
        "    # compute advantage using rewards and baseline\n",
        "    advantage = rewards - baseline\n",
        "    assert advantage.shape.ndims == 1, \"advantage must be of shape [batch_size]\"\n",
        "\n",
        "    # compute log_pi(a_t|s_t), shape = [batch, seq_length]\n",
        "    logprobs_phoneme = select_values_over_last_axis(sample_logp, sample_translations)\n",
        "    # ^-- hint: look at how crossentropy is implemented in supervised learning loss above\n",
        "    # mind the sign - this one should not be multiplied by -1 :)\n",
        "\n",
        "\n",
        "    # Compute policy gradient\n",
        "    # or rather surrogate function who's gradient is policy gradient\n",
        "    J = logprobs_phoneme*advantage[:, None]\n",
        "\n",
        "    mask = infer_mask(sample_translations, out_voc.eos_ix)\n",
        "    loss = - tf.reduce_sum(J*mask) / tf.reduce_sum(mask)\n",
        "\n",
        "    # regularize with negative entropy. Don't forget the sign!\n",
        "    # note: for entropy you need probabilities for all tokens (sample_logp), not just phoneme_logprobs\n",
        "    #entropy = <compute entropy matrix of shape[batch, seq_length], H = -sum(p*log_p), don't forget the sign!>\n",
        "    # hint: you can get sample probabilities from sample_logp using math :)\n",
        "    entropy = tf.reduce_sum(tf.math.pow(2.0, sample_logp) * sample_logp, axis=-1)\n",
        "\n",
        "    assert entropy.shape.ndims == 2, \"please make sure elementwise entropy is of shape [batch,time]\"\n",
        "\n",
        "    loss -= 0.01*tf.reduce_sum(entropy*mask) / tf.reduce_sum(mask)\n",
        "\n",
        "    # compute weight updates, clip by norm\n",
        "    grads = tf.gradients(loss, model.weights)\n",
        "    grads = tf.clip_by_global_norm(grads, 50)[0]\n",
        "\n",
        "    train_step = tf.train.AdamOptimizer(\n",
        "        learning_rate=1e-5).apply_gradients(zip(grads, model.weights,))\n",
        "\n",
        "\n",
        "initialize_uninitialized()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iuxCm1tNc9",
        "colab_type": "code",
        "outputId": "842c9cf9-e749-43ed-b5b4-11906b4a273a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "for i in trange(10000):\n",
        "    bx = sample_batch(train_words, word_to_translation, 32)[0]\n",
        "    pseudo_loss, _ = s.run([trainer.loss, trainer.train_step], {\n",
        "                           trainer.input_sequence: bx})\n",
        "\n",
        "    loss_history.append(\n",
        "        pseudo_loss\n",
        "    )\n",
        "\n",
        "    if (i+1) % REPORT_FREQ == 0:\n",
        "        clear_output(True)\n",
        "        current_scores = score(test_words)\n",
        "        editdist_history.append(current_scores.mean())\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.subplot(121)\n",
        "        plt.title('val score distribution')\n",
        "        plt.hist(current_scores, bins=20)\n",
        "        plt.subplot(122)\n",
        "        plt.title('val score / traning time')\n",
        "        plt.plot(editdist_history)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "        print(\"J=%.3f, mean score=%.3f\" %\n",
        "              (np.mean(loss_history[-10:]), np.mean(editdist_history[-10:])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines_ix shape=(8, 21)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAEICAYAAACd/8f0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 10000/10000 [16:35<00:00,  5.47it/s]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "J=-0.029, mean score=7.975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5xg4sH5tQsN",
        "colab_type": "code",
        "outputId": "865414dd-4702-4eca-d000-448976d09307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "# 测试\n",
        "for word in train_words[:10]:\n",
        "    print(\"%s -> %s\" % (word, translate([word])[0]))\n",
        "\n",
        "test_scores = []\n",
        "for start_i in trange(0, len(test_words), 32):\n",
        "    batch_words = test_words[start_i:start_i+32]\n",
        "    batch_trans = translate(batch_words)\n",
        "    distances = list(map(get_distance, batch_words, batch_trans))\n",
        "    test_scores.extend(distances)\n",
        "print(\"Supervised test score:\", np.mean(test_scores))\n",
        "\n",
        "# ^^ If you get Out Of Memory, please replace this with batched computation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines_ix shape=(1, 12)\n",
            "הצעה צנועה -> aaiiaa\n",
            "lines_ix shape=(1, 12)\n",
            "אפטוזאורוס -> aaiiaa\n",
            "lines_ix shape=(1, 6)\n",
            "אשור -> aaa\n",
            "lines_ix shape=(1, 14)\n",
            "מרוץ מכוניות -> aaiiaa\n",
            "lines_ix shape=(1, 10)\n",
            "אן ארבור -> aaiiaa\n",
            "lines_ix shape=(1, 12)\n",
            "21 באוגוסט -> aaiiaa\n",
            "lines_ix shape=(1, 10)\n",
            "אפרודיטה -> aaiiaa\n",
            "lines_ix shape=(1, 7)\n",
            "אפריל -> aaia\n",
            "lines_ix shape=(1, 11)\n",
            "אל-קאעידה -> aaiiaa\n",
            "lines_ix shape=(1, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "אסתטיקה -> aaiai\n",
            "lines_ix shape=(32, 19)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 2/2 [00:00<00:00, 26.26it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "lines_ix shape=(17, 21)\n",
            "Supervised test score: 7.63265306122449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C66zxpiYtiTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75KII4datjAB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Step 6: Make it actually work (5++ pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmjyGpsmsl1N",
        "colab_type": "text"
      },
      "source": [
        "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/do_something_scst.png width=400>\n",
        "\n",
        "In this section we want you to finally __restart with EASY_MODE=False__ and experiment to find a good model/curriculum for that task.\n",
        "\n",
        "We recommend you to start with the following architecture\n",
        "\n",
        "```\n",
        "encoder---decoder\n",
        "\n",
        "           P(y|h)\n",
        "             ^\n",
        " LSTM  ->   LSTM\n",
        "  ^          ^\n",
        " biLSTM  ->   LSTM\n",
        "  ^          ^\n",
        "input       y_prev\n",
        "```\n",
        "\n",
        "__Note:__ you can fit all 4 state tensors of both LSTMs into a in a single state - just assume that it contains, for example, [h0, c0, h1, c1] - pack it in encode and update in decode.\n",
        "\n",
        "Here are some cool ideas on what you can do then.\n",
        "\n",
        "__General tips & tricks:__\n",
        "* In some tensorflow versions and for some layers, it is required that each rnn/gru/lstm cell gets it's own `tf.variable_scope(unique_name, reuse=False)`.\n",
        "  * Otherwise it will complain about wrong tensor sizes because it tries to reuse weights from one rnn to the other.\n",
        "* You will likely need to adjust pre-training time for such a network.\n",
        "* Supervised pre-training may benefit from clipping gradients somehow.\n",
        "* SCST may indulge a higher learning rate in some cases and changing entropy regularizer over time.\n",
        "* It's often useful to save pre-trained model parameters to not re-train it every time you want new policy gradient parameters. \n",
        "* When leaving training for nighttime, try setting REPORT_FREQ to a larger value (e.g. 500) not to waste time on it.\n",
        "\n",
        "\n",
        "\n",
        "__Formal criteria:__\n",
        "To get 5 points we want you to build an architecture that:\n",
        "* _doesn't consist of single GRU_\n",
        "* _works better_ than single GRU baseline. \n",
        "* We also want you to provide either learning curve or trained model, preferably both\n",
        "* ... and write a brief report or experiment log describing what you did and how it fared.\n",
        "\n",
        "### Attention\n",
        "There's more than one way to connect decoder to encoder\n",
        "  * __Vanilla:__ layer_i of encoder last state goes to layer_i of decoder initial state\n",
        "  * __Every tick:__ feed encoder last state _on every iteration_ of decoder.\n",
        "  * __Attention:__ allow decoder to \"peek\" at one (or several) positions of encoded sequence on every tick.\n",
        "    \n",
        "The most effective (and cool) of those is, of course, attention.\n",
        "You can read more about attention [in this nice blog post](https://distill.pub/2016/augmented-rnns/). The easiest way to begin is to use \"soft\" attention with \"additive\" or \"dot-product\" intermediate layers.\n",
        "\n",
        "__Tips__\n",
        "* Model usually generalizes better if you no longer allow decoder to see final encoder state\n",
        "* Once your model made it through several epochs, it is a good idea to visualize attention maps to understand what your model has actually learned\n",
        "\n",
        "* There's more stuff [here](https://github.com/yandexdataschool/Practical_RL/blob/master/week8_scst/bonus.ipynb)\n",
        "* If you opted for hard attention, we recommend [gumbel-softmax](https://blog.evjang.com/2016/11/tutorial-categorical-variational.html) instead of sampling. Also please make sure soft attention works fine before you switch to hard.\n",
        "\n",
        "\n",
        "### UREX\n",
        "* This is a way to improve exploration in policy-based settings. The main idea is that you find and upweight under-appreciated actions.\n",
        "* Here's [video](https://www.youtube.com/watch?v=fZNyHoXgV7M&feature=youtu.be&t=3444)\n",
        " and an [article](https://arxiv.org/abs/1611.09321).\n",
        "* You may want to reduce batch size 'cuz UREX requires you to sample multiple times per source sentence.\n",
        "* Once you got it working, try using experience replay with importance sampling instead of (in addition to) basic UREX.\n",
        "\n",
        "### Some additional ideas:\n",
        "* (advanced deep learning) It may be a good idea to first train on small phrases and then adapt to larger ones (a.k.a. training curriculum).\n",
        "* (advanced nlp) You may want to switch from raw utf8 to something like unicode or even syllables to make task easier.\n",
        "* (advanced nlp) Since hebrew words are written __with vowels omitted__, you may want to use a small Hebrew vowel markup dataset at `he-pron-wiktionary.txt`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-olFslBuRap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}