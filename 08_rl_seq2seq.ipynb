{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_rl_seq2seq.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/08_rl_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDydjcUQyNI5",
        "colab_type": "text"
      },
      "source": [
        "#### 任务 \n",
        "Hebrew->English machine translation for words and short phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVRYmV1dyEMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If True, only translates phrases shorter than 20 characters (way easier).\n",
        "EASY_MODE = True\n",
        "# Useful for initial coding.\n",
        "# If false, works with all phrases (please switch to this mode for homework assignment)\n",
        "\n",
        "MODE = \"he-to-en\"  # way we translate. Either \"he-to-en\" or \"en-to-he\"\n",
        "# maximal length of _generated_ output, does not affect training\n",
        "MAX_OUTPUT_LENGTH = 50 if not EASY_MODE else 20\n",
        "REPORT_FREQ = 100  # how often to evaluate validation score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5WR2Gde6tfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cHHaS1Qz-no",
        "colab_type": "text"
      },
      "source": [
        "数据预处理\n",
        "\n",
        "\n",
        "数据的保存格式为：{ word1:[translation1,translation2,...], word2:[...],...}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9BiinrTx3kQ",
        "colab_type": "code",
        "outputId": "2318583f-502f-47b9-c621-e2179743c314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "word_to_translation = defaultdict(list)  # our dictionary\n",
        "\n",
        "bos = '_'\n",
        "eos = ';'\n",
        "\n",
        "with open(\"main_dataset_small.txt\") as fin:\n",
        "    for line in fin:\n",
        "        en, he = line[:-1].lower().replace(bos, ' ').replace(eos,                                         ' ').split('\\t')\n",
        "        word, trans = (he, en) if MODE == 'he-to-en' else (en, he)\n",
        "        if len(word) < 3:\n",
        "            continue\n",
        "        if EASY_MODE:\n",
        "            if max(len(word), len(trans)) > 20:\n",
        "                continue\n",
        "        word_to_translation[word].append(trans)\n",
        "print(\"size = \", len(word_to_translation))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size =  486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFSd8pU50OKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get all unique lines in source language\n",
        "all_words = np.array(list(word_to_translation.keys()))\n",
        "# get all unique lines in translation language\n",
        "all_translations = np.array(\n",
        "    [ts for all_ts in word_to_translation.values() for ts in all_ts])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTe_bEzA07zU",
        "colab_type": "text"
      },
      "source": [
        "利用sklearn.model_selection中的工具分隔数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyVQwX1Z00tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_words, test_words = train_test_split(\n",
        "    all_words, test_size=0.1, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbMPM2Rc1DHR",
        "colab_type": "text"
      },
      "source": [
        "### 构造Vocab类。\n",
        "- 把字符转为数字\n",
        "- 把数字转为字符"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tci26jEvpq_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkU65bqC2Nhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokens, bos=\"__BOS__\", eos=\"__EOS__\", sep=''):\n",
        "        \"\"\"\n",
        "        A special class that handles tokenizing and detokenizing\n",
        "        \"\"\"\n",
        "        assert bos in tokens, eos in tokens\n",
        "        self.tokens = tokens\n",
        "        self.token_to_ix = {t: i for i, t in enumerate(tokens)}\n",
        "\n",
        "        self.bos = bos\n",
        "        self.bos_ix = self.token_to_ix[bos]\n",
        "        self.eos = eos\n",
        "        self.eos_ix = self.token_to_ix[eos]\n",
        "        self.sep = sep\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_lines(lines, bos=\"__BOS__\", eos=\"__EOS__\", sep=''):\n",
        "        flat_lines = sep.join(list(lines))\n",
        "        flat_lines = list(flat_lines.split(sep)) if sep else list(flat_lines)\n",
        "        tokens = list(set(sep.join(flat_lines)))\n",
        "        tokens = [t for t in tokens if t not in (bos, eos) and len(t) != 0]\n",
        "        tokens = [bos, eos] + tokens\n",
        "        return Vocab(tokens, bos, eos, sep)\n",
        "\n",
        "    def tokenize(self, string):\n",
        "        \"\"\"converts string to a list of tokens\"\"\"\n",
        "        tokens = list(filter(len, string.split(self.sep))) \\\n",
        "            if self.sep != '' else list(string)\n",
        "        return [self.bos] + tokens + [self.eos]\n",
        "\n",
        "    def to_matrix(self, lines, max_len=None):\n",
        "        \"\"\"\n",
        "        convert variable length token sequences into  fixed size matrix\n",
        "        example usage:\n",
        "        >>>print( as_matrix(words[:3],source_to_ix))\n",
        "        [[15 22 21 28 27 13 -1 -1 -1 -1 -1]\n",
        "         [30 21 15 15 21 14 28 27 13 -1 -1]\n",
        "         [25 37 31 34 21 20 37 21 28 19 13]]\n",
        "        \"\"\"\n",
        "        max_len = max_len or max(map(len, lines)) + 2  # 2 for bos and eos\n",
        "\n",
        "        matrix = np.zeros((len(lines), max_len), dtype='int32') + self.eos_ix\n",
        "        #print(\"matrix[0]={}\".format(matrix[0]))\n",
        "        for i, seq in enumerate(lines):\n",
        "            tokens = self.tokenize(seq)\n",
        "            row_ix = list(map(self.token_to_ix.get, tokens))[:max_len]\n",
        "            matrix[i, :len(row_ix)] = row_ix\n",
        "\n",
        "        return matrix\n",
        "\n",
        "    def to_lines(self, matrix, crop=True):\n",
        "        \"\"\"\n",
        "        Convert matrix of token ids into strings\n",
        "        :param matrix: matrix of tokens of int32, shape=[batch,time]\n",
        "        :param crop: if True, crops BOS and EOS from line\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        lines = []\n",
        "        for line_ix in map(list, matrix):\n",
        "            if crop:\n",
        "                if line_ix[0] == self.bos_ix:\n",
        "                    line_ix = line_ix[1:]\n",
        "                if self.eos_ix in line_ix:\n",
        "                    line_ix = line_ix[:line_ix.index(self.eos_ix)]\n",
        "            line = self.sep.join(self.tokens[i] for i in line_ix)\n",
        "            lines.append(line)\n",
        "        return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahuQvfEO1DqM",
        "colab_type": "code",
        "outputId": "76a2ccc7-4328-4647-cae8-efaa99ec2955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "\n",
        "inp_voc = Vocab.from_lines(''.join(all_words), bos=bos, eos=eos, sep='')\n",
        "out_voc = Vocab.from_lines(''.join(all_translations), bos=bos, eos=eos, sep='')\n",
        "\n",
        "# Here's how you cast lines into ids and backwards.\n",
        "batch_lines = all_words[:5]\n",
        "batch_ids = inp_voc.to_matrix(batch_lines)\n",
        "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
        "\n",
        "print(\"lines\")\n",
        "print(batch_lines)\n",
        "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
        "print(batch_ids)\n",
        "print(\"\\nback to words\")\n",
        "print(batch_lines_restored)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines\n",
            "['אנרכיזם' 'אוטיזם קלאסי' 'אלבדו' 'אלבמה' 'אכילס']\n",
            "\n",
            "words to ids (0 = bos, 1 = eos):\n",
            "[[ 0 44 42 36 10 14 47 59  1  1  1  1  1  1]\n",
            " [ 0 44 11 24 14 47 59 40 13  7 44 30 14  1]\n",
            " [ 0 44  7  9 46 11  1  1  1  1  1  1  1  1]\n",
            " [ 0 44  7  9 16 34  1  1  1  1  1  1  1  1]\n",
            " [ 0 44 10 14  7 30  1  1  1  1  1  1  1  1]]\n",
            "\n",
            "back to words\n",
            "['אנרכיזם', 'אוטיזם קלאסי', 'אלבדו', 'אלבמה', 'אכילס']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBaw-SjEpEr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59u-B03Q2FDR",
        "colab_type": "code",
        "outputId": "8e2a85f0-8b42-42a9-b6bd-b7aed21ada8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=[8, 4])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"words\")\n",
        "plt.hist(list(map(len, all_words)), bins=20)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('translations')\n",
        "plt.hist(list(map(len, all_translations)), bins=20)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 7., 18., 35., 41., 45., 71.,  0., 68., 25., 28., 23., 20., 23.,\n",
              "         0., 17., 14., 17., 16., 13., 10.]),\n",
              " array([ 3.  ,  3.85,  4.7 ,  5.55,  6.4 ,  7.25,  8.1 ,  8.95,  9.8 ,\n",
              "        10.65, 11.5 , 12.35, 13.2 , 14.05, 14.9 , 15.75, 16.6 , 17.45,\n",
              "        18.3 , 19.15, 20.  ]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAEICAYAAACHwyd6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s0a-8ap3KfB",
        "colab_type": "text"
      },
      "source": [
        "### 3.实现Encoder-Decoder模型\n",
        "\n",
        "seq2seq模型model提供以下接口：\n",
        "- model.symbolic_translate(inp, **flags)-> out, logp：输入为：\n",
        "- model.symbolic_score(inp, out, **flags) -> logp\n",
        "- model.weights：所有层的权重。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ402oKX2VJ9",
        "colab_type": "code",
        "outputId": "0ec336b9-d7a6-4571-c0c4-4f21dd8ae404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as L\n",
        "%tensorflow_version 1.x \n",
        "tf.reset_default_graph()\n",
        "s = tf.InteractiveSession()\n",
        "\n",
        "# ^^^ if you get \"variable *** already exists\": re-run this cell again"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j0begV-4587",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 本部分实现单层的GRU encoder-decoder模型。\n",
        "# 注意事项：\n",
        "# 1: when using several recurrent layers TF can mixed up the weights of different recurrent layers.\n",
        "# In that case, make sure you both create AND use each rnn/gru/lstm/custom layer in a unique variable scope\n",
        "# e.g. with tf.variable_scope(\"first_lstm\"): new_cell, new_out = self.lstm_1(...)\n",
        "#      with tf.variable_scope(\"second_lstm\"): new_cell2, new_out2 = self.lstm_2(...)\n",
        "# Note 2: everything you need for decoding should be stored in model state (output list of both encode and decode)\n",
        "# e.g. for attention, you should store all encoder sequence and input mask\n",
        "# there in addition to lstm/gru states.\n",
        "\n",
        "class BasicTranslationModel:\n",
        "    def __init__(self, name, inp_voc, out_voc,\n",
        "                 emb_size, hid_size,):\n",
        "\n",
        "        self.name = name\n",
        "        self.inp_voc = inp_voc\n",
        "        self.out_voc = out_voc\n",
        "\n",
        "        with tf.variable_scope(name):\n",
        "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
        "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
        "            self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
        "            self.dec_start = L.Dense(hid_size)\n",
        "            self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
        "            self.logits = L.Dense(len(out_voc))\n",
        "\n",
        "            # run on dummy output to .build all layers (and therefore create\n",
        "            # weights)\n",
        "            inp = tf.placeholder('int32', [None, None])\n",
        "            out = tf.placeholder('int32', [None, None])\n",
        "            h0 = self.encode(inp)\n",
        "            h1 = self.decode(h0, out[:, 0])\n",
        "            # h2 = self.decode(h1,out[:,1]) etc.\n",
        "\n",
        "        self.weights = tf.get_collection(\n",
        "            tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :return: a list of initial decoder state tensors\n",
        "        \"\"\"\n",
        "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
        "        inp_emb = self.emb_inp(inp)\n",
        "\n",
        "        _, enc_last = tf.nn.dynamic_rnn(\n",
        "            self.enc0, inp_emb,\n",
        "            sequence_length=inp_lengths,\n",
        "            dtype=inp_emb.dtype)\n",
        "\n",
        "        dec_start = self.dec_start(enc_last)\n",
        "        return [dec_start]\n",
        "\n",
        "    def decode(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits\n",
        "        :param prev_state: a list of previous decoder state tensors\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n",
        "        \"\"\"\n",
        "\n",
        "        [prev_dec] = prev_state\n",
        "\n",
        "        prev_emb = self.emb_out(prev_tokens[:, None])[:, 0]\n",
        "\n",
        "        new_dec_out, new_dec_state = self.dec0(prev_emb, prev_dec)\n",
        "\n",
        "        output_logits = self.logits(new_dec_out)\n",
        "\n",
        "        return [new_dec_state], output_logits\n",
        "\n",
        "    def symbolic_score(self, inp, out, eps=1e-30, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic int32 matrices of hebrew words and their english translations.\n",
        "        Computes the log-probabilities of all possible english characters given english prefices and hebrew word.\n",
        "        :param inp: input sequence, int32 matrix of shape [batch,time]\n",
        "        :param out: output sequence, int32 matrix of shape [batch,time]\n",
        "        :return: log-probabilities of all possible english characters of shape [bath,time,n_tokens]\n",
        "        NOTE: log-probabilities time axis  is synchronized with out\n",
        "        In other words, logp are probabilities of __current__ output at each tick, not the next one\n",
        "        therefore you can get likelihood as logprobas * tf.one_hot(out,n_tokens)\n",
        "        \"\"\"\n",
        "        first_state = self.encode(inp, **flags)\n",
        "\n",
        "        batch_size = tf.shape(inp)[0]\n",
        "        bos = tf.fill([batch_size], self.out_voc.bos_ix)\n",
        "        first_logits = tf.log(tf.one_hot(bos, len(self.out_voc)) + eps)\n",
        "\n",
        "        def step(blob, y_prev):\n",
        "            h_prev = blob[:-1]\n",
        "            h_new, logits = self.decode(h_prev, y_prev, **flags)\n",
        "            return list(h_new) + [logits]\n",
        "\n",
        "        results = tf.scan(step, initializer=list(first_state) + [first_logits],\n",
        "                          elems=tf.transpose(out))\n",
        "\n",
        "        # gather state and logits, each of shape [time,batch,...]\n",
        "        states_seq, logits_seq = results[:-1], results[-1]\n",
        "\n",
        "        # add initial state and logits\n",
        "        logits_seq = tf.concat((first_logits[None], logits_seq), axis=0)\n",
        "\n",
        "        # convert from [time,batch,...] to [batch,time,...]\n",
        "        logits_seq = tf.transpose(logits_seq, [1, 0, 2])\n",
        "\n",
        "        return tf.nn.log_softmax(logits_seq)\n",
        "\n",
        "    def symbolic_translate(\n",
        "            self,\n",
        "            inp,\n",
        "            greedy=False,\n",
        "            max_len=None,\n",
        "            eps=1e-30,\n",
        "            **flags):\n",
        "        \"\"\"\n",
        "        takes symbolic int32 matrix of hebrew words, produces output tokens sampled\n",
        "        from the model and output log-probabilities for all possible tokens at each tick.\n",
        "        :param inp: input sequence, int32 matrix of shape [batch,time]\n",
        "        :param greedy: if greedy, takes token with highest probablity at each tick.\n",
        "            Otherwise samples proportionally to probability.\n",
        "        :param max_len: max length of output, defaults to 2 * input length\n",
        "        :return: output tokens int32[batch,time] and\n",
        "                 log-probabilities of all tokens at each tick, [batch,time,n_tokens]\n",
        "        \"\"\"\n",
        "        first_state = self.encode(inp, **flags)\n",
        "\n",
        "        batch_size = tf.shape(inp)[0]\n",
        "        bos = tf.fill([batch_size], self.out_voc.bos_ix)\n",
        "        first_logits = tf.log(tf.one_hot(bos, len(self.out_voc)) + eps)\n",
        "        max_len = tf.reduce_max(tf.shape(inp)[1]) * 2\n",
        "\n",
        "        def step(blob, t):\n",
        "            h_prev, y_prev = blob[:-2], blob[-1]\n",
        "            h_new, logits = self.decode(h_prev, y_prev, **flags)\n",
        "            y_new = (\n",
        "                tf.argmax(logits, axis=-1) if greedy\n",
        "                else tf.multinomial(logits, 1)[:, 0]\n",
        "            )\n",
        "            return list(h_new) + [logits, tf.cast(y_new, y_prev.dtype)]\n",
        "\n",
        "        results = tf.scan(\n",
        "            step,\n",
        "            initializer=list(first_state) + [first_logits, bos],\n",
        "            elems=[tf.range(max_len)],\n",
        "        )\n",
        "\n",
        "        # gather state, logits and outs, each of shape [time,batch,...]\n",
        "        states_seq, logits_seq, out_seq = (\n",
        "            results[:-2], results[-2], results[-1]\n",
        "        )\n",
        "\n",
        "        # add initial state, logits and out\n",
        "        logits_seq = tf.concat((first_logits[None], logits_seq), axis=0)\n",
        "        out_seq = tf.concat((bos[None], out_seq), axis=0)\n",
        "        states_seq = [\n",
        "            tf.concat((init[None], states), axis=0)\n",
        "            for init, states in zip(first_state, states_seq)\n",
        "        ]\n",
        "\n",
        "        # convert from [time,batch,...] to [batch,time,...]\n",
        "        logits_seq = tf.transpose(logits_seq, [1, 0, 2])\n",
        "        out_seq = tf.transpose(out_seq)\n",
        "        states_seq = [\n",
        "            tf.transpose(states, [1, 0] + list(range(2, states.shape.ndims)))\n",
        "            for states in states_seq\n",
        "        ]\n",
        "\n",
        "        return out_seq, tf.nn.log_softmax(logits_seq)\n",
        "\n",
        "\n",
        "### Utility functions ###\n",
        "\n",
        "def initialize_uninitialized(sess=None):\n",
        "    \"\"\"\n",
        "    Initialize unitialized variables, doesn't affect those already initialized\n",
        "    :param sess: in which session to initialize stuff. Defaults to tf.get_default_session()\n",
        "    \"\"\"\n",
        "    sess = sess or tf.get_default_session()\n",
        "    global_vars = tf.global_variables()\n",
        "    is_not_initialized = sess.run(\n",
        "        [tf.is_variable_initialized(var) for var in global_vars]\n",
        "    )\n",
        "    not_initialized_vars = [\n",
        "        v for (v, f)\n",
        "        in zip(global_vars, is_not_initialized)\n",
        "        if not f\n",
        "    ]\n",
        "\n",
        "    if len(not_initialized_vars):\n",
        "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
        "\n",
        "\n",
        "def infer_length(seq, eos_ix, time_major=False, dtype=tf.int32):\n",
        "    \"\"\"\n",
        "    compute length given output indices and eos code\n",
        "    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n",
        "    :param eos_ix: integer index of end-of-sentence token\n",
        "    :returns: lengths, int32 vector of shape [batch]\n",
        "    \"\"\"\n",
        "    axis = 0 if time_major else 1\n",
        "    is_eos = tf.cast(tf.equal(seq, eos_ix), dtype)\n",
        "    count_eos = tf.cumsum(is_eos, axis=axis, exclusive=True)\n",
        "    lengths = tf.reduce_sum(tf.cast(tf.equal(count_eos, 0), dtype), axis=axis)\n",
        "    return lengths\n",
        "\n",
        "\n",
        "def infer_mask(seq, eos_ix, time_major=False, dtype=tf.float32):\n",
        "    \"\"\"\n",
        "    compute mask given output indices and eos code\n",
        "    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n",
        "    :param eos_ix: integer index of end-of-sentence token\n",
        "    :returns: mask, float32 matrix with '0's and '1's of same shape as seq\n",
        "    \"\"\"\n",
        "    axis = 0 if time_major else 1\n",
        "    lengths = infer_length(seq, eos_ix, time_major=time_major)\n",
        "    mask = tf.sequence_mask(lengths, maxlen=tf.shape(seq)[axis], dtype=dtype)\n",
        "    if time_major:\n",
        "        mask = tf.transpose(mask)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def select_values_over_last_axis(values, indices):\n",
        "    \"\"\"\n",
        "    Auxiliary function to select logits corresponding to chosen tokens.\n",
        "    :param values: logits for all actions: float32[batch,tick,action]\n",
        "    :param indices: action ids int32[batch,tick]\n",
        "    :returns: values selected for the given actions: float[batch,tick]\n",
        "    \"\"\"\n",
        "    assert values.shape.ndims == 3 and indices.shape.ndims == 2\n",
        "    batch_size, seq_len = tf.shape(indices)[0], tf.shape(indices)[1]\n",
        "    batch_i = tf.tile(tf.range(0, batch_size)[:, None], [1, seq_len])\n",
        "    time_i = tf.tile(tf.range(0, seq_len)[None, :], [batch_size, 1])\n",
        "    indices_nd = tf.stack([batch_i, time_i, indices], axis=-1)\n",
        "\n",
        "    return tf.gather_nd(values, indices_nd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCK4NAw548gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BasicTranslationModel('model', inp_voc, out_voc,emb_size=64, hid_size=128)\n",
        "\n",
        "s.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxkbBoAO_tu-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6L1DRZu5DEr",
        "colab_type": "code",
        "outputId": "a4d7e2ac-169d-4978-b4ba-ff61e60feaae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "# Play around with symbolic_translate and symbolic_score\n",
        "inp = tf.placeholder_with_default(np.random.randint(\n",
        "    0, 10, [3, 5], dtype='int32'), [None, None])\n",
        "out = tf.placeholder_with_default(np.random.randint(\n",
        "    0, 10, [3, 5], dtype='int32'), [None, None])\n",
        "\n",
        "# translate inp (with untrained model)\n",
        "sampled_out, logp = model.symbolic_translate(inp, greedy=False)\n",
        "print(\"\\nSymbolic_translate output:\\n\", sampled_out, logp)\n",
        "print(\"\\nSample translations:\\n\", s.run(sampled_out))\n",
        "\n",
        "# score logp(out | inp) with untrained input\n",
        "logp = model.symbolic_score(inp, out)\n",
        "print(\"\\nSymbolic_score output:\\n\", logp)\n",
        "print(\"\\nLog-probabilities (clipped):\\n\", s.run(logp)[:, :2, :5])\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Symbolic_translate output:\n",
            " Tensor(\"transpose_1:0\", shape=(?, ?), dtype=int32) Tensor(\"LogSoftmax:0\", shape=(?, ?, 49), dtype=float32)\n",
            "\n",
            "Sample translations:\n",
            " [[ 0 17 19  0 36 17 39 33  2 31  4]\n",
            " [ 0  3 11  3 10 12 24 17 39 33 11]\n",
            " [ 0 10 41 14 12 35 43  1  2 19 32]]\n",
            "\n",
            "Symbolic_score output:\n",
            " Tensor(\"LogSoftmax_1:0\", shape=(?, ?, 49), dtype=float32)\n",
            "\n",
            "Log-probabilities (clipped):\n",
            " [[[  0.        -69.07755   -69.07755   -69.07755   -69.07755  ]\n",
            "  [ -3.8931136  -3.8981245  -3.895202   -3.8944452  -3.8811853]]\n",
            "\n",
            " [[  0.        -69.07755   -69.07755   -69.07755   -69.07755  ]\n",
            "  [ -3.8858316  -3.9041443  -3.8990376  -3.8946145  -3.89851  ]]\n",
            "\n",
            " [[  0.        -69.07755   -69.07755   -69.07755   -69.07755  ]\n",
            "  [ -3.88896    -3.9152758  -3.8899417  -3.8944757  -3.9005055]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckx5mX165OpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare any operations you want here\n",
        "input_sequence = tf.placeholder('int32', [None, None])\n",
        "greedy_translations, logp = model.symbolic_translate(input_sequence, greedy=True)\n",
        "\n",
        "\n",
        "def translate(lines):\n",
        "    \"\"\"\n",
        "    You are given a list of input lines. \n",
        "    Make your neural network translate them.\n",
        "    :return: a list of output lines\n",
        "    \"\"\"\n",
        "    # Convert lines to a matrix of indices\n",
        "    lines_ix = inp_voc.to_matrix(lines)\n",
        "    print(\"lines_ix shape={}\".format(lines_ix.shape))\n",
        "\n",
        "    # Compute translations in form of indices\n",
        "    trans_ix = s.run(greedy_translations, {input_sequence:lines_ix})\n",
        "\n",
        "    # Convert translations back into strings\n",
        "    return out_voc.to_lines(trans_ix)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLbPm9mG5bDj",
        "colab_type": "code",
        "outputId": "e3531183-21b4-4186-b542-0aa12d435b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"Sample inputs:\", all_words[:3])\n",
        "print(\"Dummy translations:\", translate(all_words[:3]))\n",
        "\n",
        "assert isinstance(greedy_translations,\n",
        "                  tf.Tensor) and greedy_translations.dtype.is_integer, \"trans must be a tensor of integers (token ids)\"\n",
        "assert translate(all_words[:3]) == translate(\n",
        "    all_words[:3]), \"make sure translation is deterministic (use greedy=True and disable any noise layers)\"\n",
        "assert type(translate(all_words[:3])) is list and (type(translate(all_words[:1])[0]) is str or type(\n",
        "    translate(all_words[:1])[0]) is unicode), \"translate(lines) must return a sequence of strings!\"\n",
        "print(\"Tests passed!\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample inputs: ['אנרכיזם' 'אוטיזם קלאסי' 'אלבדו']\n",
            "lines_ix shape=(3, 14)\n",
            "Dummy translations: ['óóóóóóóóóóóóóóóóóóóóóóóóóóóó', 'óóóóóóóóóóóóóóóóóóóóóóóóóóóó', 'óóóóóóóóóóóóóóóóóóóóóóóóóóóó']\n",
            "lines_ix shape=(3, 14)\n",
            "lines_ix shape=(3, 14)\n",
            "lines_ix shape=(3, 14)\n",
            "lines_ix shape=(1, 9)\n",
            "Tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmb_E_hdaHTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 打分函数"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U1ATUqRaC-e",
        "colab_type": "code",
        "outputId": "3ae7cd4a-7b91-458a-83ba-aa01037d5ff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install editdistance\n",
        "import editdistance  # !pip install editdistance\n",
        "\n",
        "\n",
        "def get_distance(word, trans):\n",
        "    \"\"\"\n",
        "    A function that takes word and predicted translation\n",
        "    and evaluates (Levenshtein's) edit distance to closest correct translation\n",
        "    \"\"\"\n",
        "    references = word_to_translation[word]\n",
        "    assert len(references) != 0, \"wrong/unknown word\"\n",
        "    return min(editdistance.eval(trans, ref) for ref in references)\n",
        "\n",
        "\n",
        "def score(words, bsize=8):\n",
        "    \"\"\"a function that computes levenshtein distance for bsize random samples\"\"\"\n",
        "    assert isinstance(words, np.ndarray)\n",
        "\n",
        "    batch_words = np.random.choice(words, size=bsize, replace=False)\n",
        "    batch_trans = translate(batch_words)\n",
        "\n",
        "    distances = list(map(get_distance, batch_words, batch_trans))\n",
        "\n",
        "    return np.array(distances, dtype='float32')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (0.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFfU773QaNZ6",
        "colab_type": "code",
        "outputId": "56e6e351-16a5-483d-ef0a-73a33bff8e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "[score(test_words, 10).mean() for _ in range(5)]\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines_ix shape=(10, 19)\n",
            "lines_ix shape=(10, 21)\n",
            "lines_ix shape=(10, 20)\n",
            "lines_ix shape=(10, 17)\n",
            "lines_ix shape=(10, 19)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[37.4, 41.6, 39.5, 33.6, 37.3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adu6CLgHaWjO",
        "colab_type": "text"
      },
      "source": [
        "### 开始训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFEKkRyRaVza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import utility functions\n",
        "#from basic_model_tf import initialize_uninitialized, infer_length, infer_mask, select_values_over_last_axis\n",
        "\n",
        "\n",
        "class supervised_training:\n",
        "\n",
        "    # variable for inputs and correct answers\n",
        "    input_sequence = tf.placeholder('int32', [None, None])\n",
        "    reference_answers = tf.placeholder('int32', [None, None])\n",
        "\n",
        "    # Compute log-probabilities of all possible tokens at each step. Use model interface.\n",
        "    logprobs_seq = model.symbolic_score(input_sequence, reference_answers)\n",
        "\n",
        "    # compute mean crossentropy\n",
        "    crossentropy = - select_values_over_last_axis(logprobs_seq, reference_answers)\n",
        "\n",
        "    mask = infer_mask(reference_answers, out_voc.eos_ix)\n",
        "\n",
        "    loss = tf.reduce_sum(crossentropy * mask)/tf.reduce_sum(mask)\n",
        "\n",
        "    # Build weights optimizer. Use model.weights to get all trainable params.\n",
        "    train_step = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "\n",
        "# intialize optimizer params while keeping model intact\n",
        "initialize_uninitialized(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh2njBSnaRM8",
        "colab_type": "code",
        "outputId": "fe6db46a-ae1c-4b24-c14e-84f0d6fb0a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def sample_batch(words, word_to_translation, batch_size):\n",
        "    \"\"\"\n",
        "    sample random batch of words and random correct translation for each word\n",
        "    example usage:\n",
        "    batch_x,batch_y = sample_batch(train_words, word_to_translations,10)\n",
        "    \"\"\"\n",
        "    # choose words\n",
        "    batch_words = np.random.choice(words, size=batch_size)\n",
        "\n",
        "    # choose translations\n",
        "    batch_trans_candidates = list(map(word_to_translation.get, batch_words))\n",
        "    batch_trans = list(map(random.choice, batch_trans_candidates))\n",
        "\n",
        "    return inp_voc.to_matrix(batch_words), out_voc.to_matrix(batch_trans)\n",
        "    \n",
        "bx, by = sample_batch(train_words, word_to_translation, batch_size=3)\n",
        "print(\"Source:\")\n",
        "print(bx)\n",
        "print(\"Target:\")\n",
        "print(by)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source:\n",
            "[[ 0 15 14 47 14 13 34 40 44 24 11 16 14 23  1  1  1  1  1]\n",
            " [ 0 34 44 11 13 14 14 42 11 30 40 34 44 24  7 42 24 14  1]\n",
            " [ 0 44  9 20  7 11 59  1  1  1  1  1  1  1  1  1  1  1  1]]\n",
            "Target:\n",
            "[[ 0 16  3 29  7 25 14 26 45 43 30 23 25 14 23  1]\n",
            " [ 0 16  3 46 16  6  3 25 14 26 29 14  2 16  6  1]\n",
            " [ 0 16 39 23 16 46 29  7  1  1  1  1  1  1  1  1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg643JAeksHM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9baaec7b-6f50-4039-8f1f-daeca9683349"
      },
      "source": [
        "len(word_to_translation)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "486"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QU7Jp9Yaseu",
        "colab_type": "code",
        "outputId": "2338419c-0a6a-44cf-f683-ad21ce994ed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm, trange  # or use tqdm_notebook,tnrange\n",
        "\n",
        "loss_history = []\n",
        "editdist_history = []\n",
        "\n",
        "for i in trange(25):\n",
        "    bx, by = sample_batch(train_words, word_to_translation, 16)\n",
        "\n",
        "    feed_dict = {\n",
        "        supervised_training.input_sequence: bx,\n",
        "        supervised_training.reference_answers: by\n",
        "    }\n",
        "\n",
        "    loss, _ = s.run([supervised_training.loss,\n",
        "                     supervised_training.train_step], feed_dict)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    if (i+1) % REPORT_FREQ == 0:\n",
        "        clear_output(True)\n",
        "        current_scores = score(test_words)\n",
        "        editdist_history.append(current_scores.mean())\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(131)\n",
        "        plt.title('train loss / traning time')\n",
        "        plt.plot(loss_history)\n",
        "        plt.grid()\n",
        "        plt.subplot(132)\n",
        "        plt.title('val score distribution')\n",
        "        plt.hist(current_scores, bins=20)\n",
        "        plt.subplot(133)\n",
        "        plt.title('val score / traning time')\n",
        "        plt.plot(editdist_history)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "        print(\"llh=%.3f, mean score=%.3f\" %\n",
        "              (np.mean(loss_history[-10:]), np.mean(editdist_history[-10:])))\n",
        "\n",
        "# Note: it's okay if loss oscillates up and down as long as it gets better on average over long term (e.g. 5k batches)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25/25 [00:01<00:00, 21.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpTUM6v6avXQ",
        "colab_type": "code",
        "outputId": "d0f50a1a-5528-4d0e-fabb-6995b82a63d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "for word in train_words[:10]:\n",
        "    print(\"%s -> %s\" % (word, translate([word])[0]))\n",
        "\n",
        "test_scores = []\n",
        "for start_i in trange(0, len(test_words), 32):\n",
        "    batch_words = test_words[start_i:start_i+32]\n",
        "    batch_trans = translate(batch_words)\n",
        "    distances = list(map(get_distance, batch_words, batch_trans))\n",
        "    test_scores.extend(distances)\n",
        "\n",
        "print(\"Supervised test score:\", np.mean(test_scores))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "lines_ix shape=(1, 12)\n",
            "הצעה צנועה -> aaassss\n",
            "lines_ix shape=(1, 12)\n",
            "אפטוזאורוס -> aaaasss\n",
            "lines_ix shape=(1, 6)\n",
            "אשור -> aaass\n",
            "lines_ix shape=(1, 14)\n",
            "מרוץ מכוניות -> aaaasss\n",
            "lines_ix shape=(1, 10)\n",
            "אן ארבור -> aaasss\n",
            "lines_ix shape=(1, 12)\n",
            "21 באוגוסט -> aaassss\n",
            "lines_ix shape=(1, 10)\n",
            "אפרודיטה -> aaasss\n",
            "lines_ix shape=(1, 7)\n",
            "אפריל -> aaasss\n",
            "lines_ix shape=(1, 11)\n",
            "אל-קאעידה -> aaassss\n",
            "lines_ix shape=(1, 9)\n",
            "אסתטיקה -> aaasss\n",
            "lines_ix shape=(32, 19)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 2/2 [00:00<00:00, 22.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "lines_ix shape=(17, 21)\n",
            "Supervised test score: 8.122448979591837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWG4okZgeu21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQrTTe7njmcY",
        "colab_type": "text"
      },
      "source": [
        "### 自定义损失函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyz9hoHMn7J-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_ix_to_token = dict([(v,k) for k, v in inp_voc.token_to_ix.items()])\n",
        "trans_ix_to_token = dict([(v,k) for k, v in out_voc.token_to_ix.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBBaRAbkkIw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _compute_levenshtein(words_ix, trans_ix):\n",
        "    \"\"\"\n",
        "    A custom tensorflow operation that computes levenshtein loss for predicted trans.\n",
        "\n",
        "    Params:\n",
        "    - words_ix - a matrix of letter indices, shape=[batch_size,word_length]\n",
        "    - words_mask - a matrix of zeros/ones, \n",
        "       1 means \"word is still not finished\"\n",
        "       0 means \"word has already finished and this is padding\"\n",
        "\n",
        "    - trans_mask - a matrix of output letter indices, shape=[batch_size,translation_length]\n",
        "    - trans_mask - a matrix of zeros/ones, similar to words_mask but for trans_ix\n",
        "\n",
        "\n",
        "    Please implement the function and make sure it passes tests from the next cell.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # convert words to strings\n",
        "    # map(word_to_translation.get, batch_words)\n",
        "    words_str = inp_voc.to_lines(words_ix)\n",
        "    words = list(map(word_to_translation.get, words_str))\n",
        "    words = [w[0] for w in words]\n",
        "\n",
        "    assert type(words) is list and type(\n",
        "        words[0]) is str and len(words) == len(words_ix)\n",
        "\n",
        "    translations = out_voc.to_lines(trans_ix)\n",
        "\n",
        "    assert type(translations) is list and type(\n",
        "        translations[0]) is str and len(translations) == len(trans_ix)\n",
        "\n",
        "    # computes levenstein distances. can be arbitrary python code.\n",
        "    distances = [editdistance.eval(w, t) for w, t in zip(words, translations)]\n",
        "\n",
        "    assert type(distances) in (list, tuple, np.ndarray) and len(\n",
        "        distances) == len(words_ix)\n",
        "\n",
        "    distances = np.array(list(distances), dtype='float32')\n",
        "    return distances\n",
        "\n",
        "def compute_levenshtein(words_ix, trans_ix):\n",
        "    out = tf.py_func(_compute_levenshtein, [words_ix, trans_ix, ], tf.float32)\n",
        "    out.set_shape([None])\n",
        "\n",
        "    return tf.stop_gradient(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgF-I-0mp0HW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVNhL7kreB8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "aeab76d3-fe16-4221-d8c8-bb8bcaab90f3"
      },
      "source": [
        "print(batch_words[0])\n",
        "batch_words_num = inp_voc.to_matrix(batch_words)\n",
        "print(batch_words_num[0])\n",
        "print(list(map(words_ix_to_token.get, batch_words_num[0])))\n",
        "#print(inp_voc.to_lines(batch_words_ix[0]))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "אן ברונטה\n",
            "[ 0 44 27 40  9 36 11 42 24 34  1  1  1  1  1  1  1  1  1  1  1]\n",
            "['_', 'א', 'ן', ' ', 'ב', 'ר', 'ו', 'נ', 'ט', 'ה', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEQIItqBk3DI",
        "colab_type": "code",
        "outputId": "b1268197-dcc1-4f3c-adc2-224441e2f5b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# 测试上面代码\n",
        "# test suite\n",
        "# sample random batch of (words, correct trans, wrong trans)\n",
        "batch_words = np.random.choice(train_words, size=100)\n",
        "\n",
        "batch_trans = list(map(random.choice, map(\n",
        "    word_to_translation.get, batch_words)))\n",
        "batch_trans_wrong = np.random.choice(all_translations, size=100)\n",
        "\n",
        "batch_words_ix = tf.constant(inp_voc.to_matrix(batch_words))\n",
        "batch_trans_ix = tf.constant(out_voc.to_matrix(batch_trans))\n",
        "batch_trans_wrong_ix = tf.constant(out_voc.to_matrix(batch_trans_wrong))\n",
        "\n",
        "# assert compute_levenshtein is zero for ideal translations\n",
        "correct_answers_score = compute_levenshtein(\n",
        "    batch_words_ix, batch_trans_ix).eval()\n",
        "print(\"score={}\".format(correct_answers_score))\n",
        "assert np.all(correct_answers_score ==0), \"a perfect translation got nonzero levenshtein score!\"\n",
        "\n",
        "print(\"Everything seems alright!\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score=[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n",
            "Everything seems alright!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4iNrSX4r3QL",
        "colab_type": "text"
      },
      "source": [
        "### self-critical policy gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yecBY47tsMkb",
        "colab_type": "text"
      },
      "source": [
        "实现 self-critical sequence training.[论文](https://arxiv.org/abs/1612.00563)\n",
        "\n",
        "policy-based的优化方法，但是b的计算方法有点不一样。\n",
        "\n",
        "$$ \\nabla J = E_{x \\sim p(s)} E_{y \\sim \\pi(y|x)} \\nabla log \\pi(y|x) \\cdot (R(x,y) - b(x)) $$\n",
        "reward R(x,y)为负编辑距离，因为我们是最小化J。 \n",
        "\n",
        "The baseline b(x) represents how well model fares on word x.\n",
        "\n",
        "In practice, this means that we compute baseline as a score of greedy translation, $b(x) = R(x,y_{greedy}(x)) $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKLTJO6MMyDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HHu9aYcr9AB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class trainer:\n",
        "\n",
        "    input_sequence = tf.placeholder('int32', [None, None])\n",
        "\n",
        "    # use model to __sample__ symbolic translations given input_sequence\n",
        "    sample_translations, sample_logp = model.symbolic_translate(input_sequence, greedy=False)\n",
        "    # use model to __greedy__ symbolic translations given input_sequence\n",
        "    greedy_translations, greedy_logp = model.symbolic_translate(input_sequence, greedy=True)\n",
        "\n",
        "    rewards = - compute_levenshtein(input_sequence, sample_translations)\n",
        "\n",
        "    # compute __negative__ levenshtein for greedy mode\n",
        "    baseline = - compute_levenshtein(input_sequence, greedy_translations)\n",
        "\n",
        "    # compute advantage using rewards and baseline\n",
        "    advantage = rewards - baseline\n",
        "    assert advantage.shape.ndims == 1, \"advantage must be of shape [batch_size]\"\n",
        "\n",
        "    # compute log_pi(a_t|s_t), shape = [batch, seq_length]\n",
        "    logprobs_phoneme = select_values_over_last_axis(sample_logp, reference_answers)\n",
        "    # ^-- hint: look at how crossentropy is implemented in supervised learning loss above\n",
        "    # mind the sign - this one should not be multiplied by -1 :)\n",
        "\n",
        "\n",
        "    # Compute policy gradient\n",
        "    # or rather surrogate function who's gradient is policy gradient\n",
        "    J = logprobs_phoneme*advantage[:, None]\n",
        "\n",
        "    mask = infer_mask(sample_translations, out_voc.eos_ix)\n",
        "    loss = - tf.reduce_sum(J*mask) / tf.reduce_sum(mask)\n",
        "\n",
        "    # regularize with negative entropy. Don't forget the sign!\n",
        "    # note: for entropy you need probabilities for all tokens (sample_logp), not just phoneme_logprobs\n",
        "    entropy = <compute entropy matrix of shape[batch, seq_length], H = -sum(p*log_p), don't forget the sign!>\n",
        "    # hint: you can get sample probabilities from sample_logp using math :)\n",
        "\n",
        "\n",
        "    assert entropy.shape.ndims == 2, \"please make sure elementwise entropy is of shape [batch,time]\"\n",
        "\n",
        "    loss -= 0.01*tf.reduce_sum(entropy*mask) / tf.reduce_sum(mask)\n",
        "\n",
        "    # compute weight updates, clip by norm\n",
        "    grads = tf.gradients(loss, model.weights)\n",
        "    grads = tf.clip_by_global_norm(grads, 50)[0]\n",
        "\n",
        "    train_step = tf.train.AdamOptimizer(\n",
        "        learning_rate=1e-5).apply_gradients(zip(grads, model.weights,))\n",
        "\n",
        "\n",
        "initialize_uninitialized()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iuxCm1tNc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in trange(100000):\n",
        "    bx = sample_batch(train_words, word_to_translation, 32)[0]\n",
        "    pseudo_loss, _ = s.run([trainer.loss, trainer.train_step], {\n",
        "                           trainer.input_sequence: bx})\n",
        "\n",
        "    loss_history.append(\n",
        "        pseudo_loss\n",
        "    )\n",
        "\n",
        "    if (i+1) % REPORT_FREQ == 0:\n",
        "        clear_output(True)\n",
        "        current_scores = score(test_words)\n",
        "        editdist_history.append(current_scores.mean())\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.subplot(121)\n",
        "        plt.title('val score distribution')\n",
        "        plt.hist(current_scores, bins=20)\n",
        "        plt.subplot(122)\n",
        "        plt.title('val score / traning time')\n",
        "        plt.plot(editdist_history)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "        print(\"J=%.3f, mean score=%.3f\" %\n",
        "              (np.mean(loss_history[-10:]), np.mean(editdist_history[-10:])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5xg4sH5tQsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 测试\n",
        "for word in train_words[:10]:\n",
        "    print(\"%s -> %s\" % (word, translate([word])[0]))\n",
        "\n",
        "test_scores = []\n",
        "for start_i in trange(0, len(test_words), 32):\n",
        "    batch_words = test_words[start_i:start_i+32]\n",
        "    batch_trans = translate(batch_words)\n",
        "    distances = list(map(get_distance, batch_words, batch_trans))\n",
        "    test_scores.extend(distances)\n",
        "print(\"Supervised test score:\", np.mean(test_scores))\n",
        "\n",
        "# ^^ If you get Out Of Memory, please replace this with batched computation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C66zxpiYtiTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75KII4datjAB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Step 6: Make it actually work (5++ pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmjyGpsmsl1N",
        "colab_type": "text"
      },
      "source": [
        "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/do_something_scst.png width=400>\n",
        "\n",
        "In this section we want you to finally __restart with EASY_MODE=False__ and experiment to find a good model/curriculum for that task.\n",
        "\n",
        "We recommend you to start with the following architecture\n",
        "\n",
        "```\n",
        "encoder---decoder\n",
        "\n",
        "           P(y|h)\n",
        "             ^\n",
        " LSTM  ->   LSTM\n",
        "  ^          ^\n",
        " biLSTM  ->   LSTM\n",
        "  ^          ^\n",
        "input       y_prev\n",
        "```\n",
        "\n",
        "__Note:__ you can fit all 4 state tensors of both LSTMs into a in a single state - just assume that it contains, for example, [h0, c0, h1, c1] - pack it in encode and update in decode.\n",
        "\n",
        "Here are some cool ideas on what you can do then.\n",
        "\n",
        "__General tips & tricks:__\n",
        "* In some tensorflow versions and for some layers, it is required that each rnn/gru/lstm cell gets it's own `tf.variable_scope(unique_name, reuse=False)`.\n",
        "  * Otherwise it will complain about wrong tensor sizes because it tries to reuse weights from one rnn to the other.\n",
        "* You will likely need to adjust pre-training time for such a network.\n",
        "* Supervised pre-training may benefit from clipping gradients somehow.\n",
        "* SCST may indulge a higher learning rate in some cases and changing entropy regularizer over time.\n",
        "* It's often useful to save pre-trained model parameters to not re-train it every time you want new policy gradient parameters. \n",
        "* When leaving training for nighttime, try setting REPORT_FREQ to a larger value (e.g. 500) not to waste time on it.\n",
        "\n",
        "\n",
        "\n",
        "__Formal criteria:__\n",
        "To get 5 points we want you to build an architecture that:\n",
        "* _doesn't consist of single GRU_\n",
        "* _works better_ than single GRU baseline. \n",
        "* We also want you to provide either learning curve or trained model, preferably both\n",
        "* ... and write a brief report or experiment log describing what you did and how it fared.\n",
        "\n",
        "### Attention\n",
        "There's more than one way to connect decoder to encoder\n",
        "  * __Vanilla:__ layer_i of encoder last state goes to layer_i of decoder initial state\n",
        "  * __Every tick:__ feed encoder last state _on every iteration_ of decoder.\n",
        "  * __Attention:__ allow decoder to \"peek\" at one (or several) positions of encoded sequence on every tick.\n",
        "    \n",
        "The most effective (and cool) of those is, of course, attention.\n",
        "You can read more about attention [in this nice blog post](https://distill.pub/2016/augmented-rnns/). The easiest way to begin is to use \"soft\" attention with \"additive\" or \"dot-product\" intermediate layers.\n",
        "\n",
        "__Tips__\n",
        "* Model usually generalizes better if you no longer allow decoder to see final encoder state\n",
        "* Once your model made it through several epochs, it is a good idea to visualize attention maps to understand what your model has actually learned\n",
        "\n",
        "* There's more stuff [here](https://github.com/yandexdataschool/Practical_RL/blob/master/week8_scst/bonus.ipynb)\n",
        "* If you opted for hard attention, we recommend [gumbel-softmax](https://blog.evjang.com/2016/11/tutorial-categorical-variational.html) instead of sampling. Also please make sure soft attention works fine before you switch to hard.\n",
        "\n",
        "\n",
        "### UREX\n",
        "* This is a way to improve exploration in policy-based settings. The main idea is that you find and upweight under-appreciated actions.\n",
        "* Here's [video](https://www.youtube.com/watch?v=fZNyHoXgV7M&feature=youtu.be&t=3444)\n",
        " and an [article](https://arxiv.org/abs/1611.09321).\n",
        "* You may want to reduce batch size 'cuz UREX requires you to sample multiple times per source sentence.\n",
        "* Once you got it working, try using experience replay with importance sampling instead of (in addition to) basic UREX.\n",
        "\n",
        "### Some additional ideas:\n",
        "* (advanced deep learning) It may be a good idea to first train on small phrases and then adapt to larger ones (a.k.a. training curriculum).\n",
        "* (advanced nlp) You may want to switch from raw utf8 to something like unicode or even syllables to make task easier.\n",
        "* (advanced nlp) Since hebrew words are written __with vowels omitted__, you may want to use a small Hebrew vowel markup dataset at `he-pron-wiktionary.txt`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-olFslBuRap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}