{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_rl_seq2seq.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/08_rl_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDydjcUQyNI5",
        "colab_type": "text"
      },
      "source": [
        "#### 任务 \n",
        "Hebrew->English machine translation for words and short phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVRYmV1dyEMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If True, only translates phrases shorter than 20 characters (way easier).\n",
        "EASY_MODE = True\n",
        "# Useful for initial coding.\n",
        "# If false, works with all phrases (please switch to this mode for homework assignment)\n",
        "\n",
        "MODE = \"he-to-en\"  # way we translate. Either \"he-to-en\" or \"en-to-he\"\n",
        "# maximal length of _generated_ output, does not affect training\n",
        "MAX_OUTPUT_LENGTH = 50 if not EASY_MODE else 20\n",
        "REPORT_FREQ = 100  # how often to evaluate validation score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5WR2Gde6tfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cHHaS1Qz-no",
        "colab_type": "text"
      },
      "source": [
        "数据预处理\n",
        "\n",
        "\n",
        "数据的保存格式为：{ word1:[translation1,translation2,...], word2:[...],...}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9BiinrTx3kQ",
        "colab_type": "code",
        "outputId": "2c077b13-6da5-461e-ef21-aa453d9b441c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "word_to_translation = defaultdict(list)  # our dictionary\n",
        "\n",
        "bos = '_'\n",
        "eos = ';'\n",
        "\n",
        "with open(\"main_dataset.txt\") as fin:\n",
        "    for line in fin:\n",
        "        en, he = line[:-1].lower().replace(bos, ' ').replace(eos,                                         ' ').split('\\t')\n",
        "        word, trans = (he, en) if MODE == 'he-to-en' else (en, he)\n",
        "        if len(word) < 3:\n",
        "            continue\n",
        "        if EASY_MODE:\n",
        "            if max(len(word), len(trans)) > 20:\n",
        "                continue\n",
        "        word_to_translation[word].append(trans)\n",
        "print(\"size = \", len(word_to_translation))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size =  130113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEZGKLt00pun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK8aTf1B1fSx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFSd8pU50OKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get all unique lines in source language\n",
        "all_words = np.array(list(word_to_translation.keys()))\n",
        "# get all unique lines in translation language\n",
        "all_translations = np.array(\n",
        "    [ts for all_ts in word_to_translation.values() for ts in all_ts])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTe_bEzA07zU",
        "colab_type": "text"
      },
      "source": [
        "split the dataset\n",
        "\n",
        "\n",
        "We hold out 10% of all words to be used for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyVQwX1Z00tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_words, test_words = train_test_split(\n",
        "    all_words, test_size=0.1, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngQwTAuS00VJ",
        "colab_type": "code",
        "outputId": "042f6805-322a-42fb-e27b-ce585a141914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "130113\n",
            "156326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbMPM2Rc1DHR",
        "colab_type": "text"
      },
      "source": [
        "### Building vocabularies\n",
        "\n",
        "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into english words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkU65bqC2Nhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokens, bos=\"__BOS__\", eos=\"__EOS__\", sep=''):\n",
        "        \"\"\"\n",
        "        A special class that handles tokenizing and detokenizing\n",
        "        \"\"\"\n",
        "        assert bos in tokens, eos in tokens\n",
        "        self.tokens = tokens\n",
        "        self.token_to_ix = {t: i for i, t in enumerate(tokens)}\n",
        "\n",
        "        self.bos = bos\n",
        "        self.bos_ix = self.token_to_ix[bos]\n",
        "        self.eos = eos\n",
        "        self.eos_ix = self.token_to_ix[eos]\n",
        "        self.sep = sep\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_lines(lines, bos=\"__BOS__\", eos=\"__EOS__\", sep=''):\n",
        "        flat_lines = sep.join(list(lines))\n",
        "        flat_lines = list(flat_lines.split(sep)) if sep else list(flat_lines)\n",
        "        tokens = list(set(sep.join(flat_lines)))\n",
        "        tokens = [t for t in tokens if t not in (bos, eos) and len(t) != 0]\n",
        "        tokens = [bos, eos] + tokens\n",
        "        return Vocab(tokens, bos, eos, sep)\n",
        "\n",
        "    def tokenize(self, string):\n",
        "        \"\"\"converts string to a list of tokens\"\"\"\n",
        "        tokens = list(filter(len, string.split(self.sep))) \\\n",
        "            if self.sep != '' else list(string)\n",
        "        return [self.bos] + tokens + [self.eos]\n",
        "\n",
        "    def to_matrix(self, lines, max_len=None):\n",
        "        \"\"\"\n",
        "        convert variable length token sequences into  fixed size matrix\n",
        "        example usage:\n",
        "        >>>print( as_matrix(words[:3],source_to_ix))\n",
        "        [[15 22 21 28 27 13 -1 -1 -1 -1 -1]\n",
        "         [30 21 15 15 21 14 28 27 13 -1 -1]\n",
        "         [25 37 31 34 21 20 37 21 28 19 13]]\n",
        "        \"\"\"\n",
        "        max_len = max_len or max(map(len, lines)) + 2  # 2 for bos and eos\n",
        "\n",
        "        matrix = np.zeros((len(lines), max_len), dtype='int32') + self.eos_ix\n",
        "        for i, seq in enumerate(lines):\n",
        "            tokens = self.tokenize(seq)\n",
        "            row_ix = list(map(self.token_to_ix.get, tokens))[:max_len]\n",
        "            matrix[i, :len(row_ix)] = row_ix\n",
        "\n",
        "        return matrix\n",
        "\n",
        "    def to_lines(self, matrix, crop=True):\n",
        "        \"\"\"\n",
        "        Convert matrix of token ids into strings\n",
        "        :param matrix: matrix of tokens of int32, shape=[batch,time]\n",
        "        :param crop: if True, crops BOS and EOS from line\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        lines = []\n",
        "        for line_ix in map(list, matrix):\n",
        "            if crop:\n",
        "                if line_ix[0] == self.bos_ix:\n",
        "                    line_ix = line_ix[1:]\n",
        "                if self.eos_ix in line_ix:\n",
        "                    line_ix = line_ix[:line_ix.index(self.eos_ix)]\n",
        "            line = self.sep.join(self.tokens[i] for i in line_ix)\n",
        "            lines.append(line)\n",
        "        return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahuQvfEO1DqM",
        "colab_type": "code",
        "outputId": "e708db3c-013f-4c0f-ce5a-6d6ba89582bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "\n",
        "inp_voc = Vocab.from_lines(''.join(all_words), bos=bos, eos=eos, sep='')\n",
        "out_voc = Vocab.from_lines(''.join(all_translations), bos=bos, eos=eos, sep='')\n",
        "\n",
        "# Here's how you cast lines into ids and backwards.\n",
        "batch_lines = all_words[:5]\n",
        "batch_ids = inp_voc.to_matrix(batch_lines)\n",
        "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
        "\n",
        "print(\"lines\")\n",
        "print(batch_lines)\n",
        "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
        "print(batch_ids)\n",
        "print(\"\\nback to words\")\n",
        "print(batch_lines_restored)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines\n",
            "['אנרכיזם' 'אוטיזם קלאסי' 'אלבדו' 'אלבמה' 'אכילס']\n",
            "\n",
            "words to ids (0 = bos, 1 = eos):\n",
            "[[  0  54  28  67 167   7   5  45   1   1   1   1   1   1]\n",
            " [  0  54  72  86   7   5  45  88 184 135  54  36   7   1]\n",
            " [  0  54 135 169   2  72   1   1   1   1   1   1   1   1]\n",
            " [  0  54 135 169 151  41   1   1   1   1   1   1   1   1]\n",
            " [  0  54 167   7 135  36   1   1   1   1   1   1   1   1]]\n",
            "\n",
            "back to words\n",
            "['אנרכיזם', 'אוטיזם קלאסי', 'אלבדו', 'אלבמה', 'אכילס']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59u-B03Q2FDR",
        "colab_type": "code",
        "outputId": "e2327cf4-6066-4748-84dc-9af27978038a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=[8, 4])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"words\")\n",
        "plt.hist(list(map(len, all_words)), bins=20)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('translations')\n",
        "plt.hist(list(map(len, all_translations)), bins=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([   21.,   112.,  3098.,  8157., 11482., 12556., 11430.,  9568.,\n",
              "         9254.,  9755., 10299., 11123., 11203., 10840.,  9316.,  7873.,\n",
              "         6527.,  5523.,  4505.,  3684.]),\n",
              " array([ 1.  ,  1.95,  2.9 ,  3.85,  4.8 ,  5.75,  6.7 ,  7.65,  8.6 ,\n",
              "         9.55, 10.5 , 11.45, 12.4 , 13.35, 14.3 , 15.25, 16.2 , 17.15,\n",
              "        18.1 , 19.05, 20.  ]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAEICAYAAABLWh2RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s0a-8ap3KfB",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: deploy encoder-decoder (1 point)\n",
        "\n",
        "seq2seq模型model提供以下接口：\n",
        "- model.symbolic_translate(inp, **flags)-> out, logp：输入为：\n",
        "- model.symbolic_score(inp, out, **flags) -> logp\n",
        "- model.weights：所有层的权重。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ402oKX2VJ9",
        "colab_type": "code",
        "outputId": "6fabb577-abc5-41b5-f9b4-17967ee21d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "s = tf.InteractiveSession()\n",
        "\n",
        "# ^^^ if you get \"variable *** already exists\": re-run this cell again"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j0begV-4587",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras.layers as L\n",
        "\n",
        "# This code implements a single-GRU seq2seq model. You will have to improve it later in the assignment.\n",
        "# Note 1: when using several recurrent layers TF can mixed up the weights of different recurrent layers.\n",
        "# In that case, make sure you both create AND use each rnn/gru/lstm/custom layer in a unique variable scope\n",
        "# e.g. with tf.variable_scope(\"first_lstm\"): new_cell, new_out = self.lstm_1(...)\n",
        "#      with tf.variable_scope(\"second_lstm\"): new_cell2, new_out2 = self.lstm_2(...)\n",
        "# Note 2: everything you need for decoding should be stored in model state (output list of both encode and decode)\n",
        "# e.g. for attention, you should store all encoder sequence and input mask\n",
        "# there in addition to lstm/gru states.\n",
        "\n",
        "\n",
        "class BasicTranslationModel:\n",
        "    def __init__(self, name, inp_voc, out_voc,\n",
        "                 emb_size, hid_size,):\n",
        "\n",
        "        self.name = name\n",
        "        self.inp_voc = inp_voc\n",
        "        self.out_voc = out_voc\n",
        "\n",
        "        with tf.variable_scope(name):\n",
        "            self.emb_inp = L.Embedding(len(inp_voc), emb_size)\n",
        "            self.emb_out = L.Embedding(len(out_voc), emb_size)\n",
        "            self.enc0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
        "            self.dec_start = L.Dense(hid_size)\n",
        "            self.dec0 = tf.nn.rnn_cell.GRUCell(hid_size)\n",
        "            self.logits = L.Dense(len(out_voc))\n",
        "\n",
        "            # run on dummy output to .build all layers (and therefore create\n",
        "            # weights)\n",
        "            inp = tf.placeholder('int32', [None, None])\n",
        "            out = tf.placeholder('int32', [None, None])\n",
        "            h0 = self.encode(inp)\n",
        "            h1 = self.decode(h0, out[:, 0])\n",
        "            # h2 = self.decode(h1,out[:,1]) etc.\n",
        "\n",
        "        self.weights = tf.get_collection(\n",
        "            tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :return: a list of initial decoder state tensors\n",
        "        \"\"\"\n",
        "        inp_lengths = infer_length(inp, self.inp_voc.eos_ix)\n",
        "        inp_emb = self.emb_inp(inp)\n",
        "\n",
        "        _, enc_last = tf.nn.dynamic_rnn(\n",
        "            self.enc0, inp_emb,\n",
        "            sequence_length=inp_lengths,\n",
        "            dtype=inp_emb.dtype)\n",
        "\n",
        "        dec_start = self.dec_start(enc_last)\n",
        "        return [dec_start]\n",
        "\n",
        "    def decode(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits\n",
        "        :param prev_state: a list of previous decoder state tensors\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n",
        "        \"\"\"\n",
        "\n",
        "        [prev_dec] = prev_state\n",
        "\n",
        "        prev_emb = self.emb_out(prev_tokens[:, None])[:, 0]\n",
        "\n",
        "        new_dec_out, new_dec_state = self.dec0(prev_emb, prev_dec)\n",
        "\n",
        "        output_logits = self.logits(new_dec_out)\n",
        "\n",
        "        return [new_dec_state], output_logits\n",
        "\n",
        "    def symbolic_score(self, inp, out, eps=1e-30, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic int32 matrices of hebrew words and their english translations.\n",
        "        Computes the log-probabilities of all possible english characters given english prefices and hebrew word.\n",
        "        :param inp: input sequence, int32 matrix of shape [batch,time]\n",
        "        :param out: output sequence, int32 matrix of shape [batch,time]\n",
        "        :return: log-probabilities of all possible english characters of shape [bath,time,n_tokens]\n",
        "        NOTE: log-probabilities time axis  is synchronized with out\n",
        "        In other words, logp are probabilities of __current__ output at each tick, not the next one\n",
        "        therefore you can get likelihood as logprobas * tf.one_hot(out,n_tokens)\n",
        "        \"\"\"\n",
        "        first_state = self.encode(inp, **flags)\n",
        "\n",
        "        batch_size = tf.shape(inp)[0]\n",
        "        bos = tf.fill([batch_size], self.out_voc.bos_ix)\n",
        "        first_logits = tf.log(tf.one_hot(bos, len(self.out_voc)) + eps)\n",
        "\n",
        "        def step(blob, y_prev):\n",
        "            h_prev = blob[:-1]\n",
        "            h_new, logits = self.decode(h_prev, y_prev, **flags)\n",
        "            return list(h_new) + [logits]\n",
        "\n",
        "        results = tf.scan(step, initializer=list(first_state) + [first_logits],\n",
        "                          elems=tf.transpose(out))\n",
        "\n",
        "        # gather state and logits, each of shape [time,batch,...]\n",
        "        states_seq, logits_seq = results[:-1], results[-1]\n",
        "\n",
        "        # add initial state and logits\n",
        "        logits_seq = tf.concat((first_logits[None], logits_seq), axis=0)\n",
        "\n",
        "        # convert from [time,batch,...] to [batch,time,...]\n",
        "        logits_seq = tf.transpose(logits_seq, [1, 0, 2])\n",
        "\n",
        "        return tf.nn.log_softmax(logits_seq)\n",
        "\n",
        "    def symbolic_translate(\n",
        "            self,\n",
        "            inp,\n",
        "            greedy=False,\n",
        "            max_len=None,\n",
        "            eps=1e-30,\n",
        "            **flags):\n",
        "        \"\"\"\n",
        "        takes symbolic int32 matrix of hebrew words, produces output tokens sampled\n",
        "        from the model and output log-probabilities for all possible tokens at each tick.\n",
        "        :param inp: input sequence, int32 matrix of shape [batch,time]\n",
        "        :param greedy: if greedy, takes token with highest probablity at each tick.\n",
        "            Otherwise samples proportionally to probability.\n",
        "        :param max_len: max length of output, defaults to 2 * input length\n",
        "        :return: output tokens int32[batch,time] and\n",
        "                 log-probabilities of all tokens at each tick, [batch,time,n_tokens]\n",
        "        \"\"\"\n",
        "        first_state = self.encode(inp, **flags)\n",
        "\n",
        "        batch_size = tf.shape(inp)[0]\n",
        "        bos = tf.fill([batch_size], self.out_voc.bos_ix)\n",
        "        first_logits = tf.log(tf.one_hot(bos, len(self.out_voc)) + eps)\n",
        "        max_len = tf.reduce_max(tf.shape(inp)[1]) * 2\n",
        "\n",
        "        def step(blob, t):\n",
        "            h_prev, y_prev = blob[:-2], blob[-1]\n",
        "            h_new, logits = self.decode(h_prev, y_prev, **flags)\n",
        "            y_new = (\n",
        "                tf.argmax(logits, axis=-1) if greedy\n",
        "                else tf.multinomial(logits, 1)[:, 0]\n",
        "            )\n",
        "            return list(h_new) + [logits, tf.cast(y_new, y_prev.dtype)]\n",
        "\n",
        "        results = tf.scan(\n",
        "            step,\n",
        "            initializer=list(first_state) + [first_logits, bos],\n",
        "            elems=[tf.range(max_len)],\n",
        "        )\n",
        "\n",
        "        # gather state, logits and outs, each of shape [time,batch,...]\n",
        "        states_seq, logits_seq, out_seq = (\n",
        "            results[:-2], results[-2], results[-1]\n",
        "        )\n",
        "\n",
        "        # add initial state, logits and out\n",
        "        logits_seq = tf.concat((first_logits[None], logits_seq), axis=0)\n",
        "        out_seq = tf.concat((bos[None], out_seq), axis=0)\n",
        "        states_seq = [\n",
        "            tf.concat((init[None], states), axis=0)\n",
        "            for init, states in zip(first_state, states_seq)\n",
        "        ]\n",
        "\n",
        "        # convert from [time,batch,...] to [batch,time,...]\n",
        "        logits_seq = tf.transpose(logits_seq, [1, 0, 2])\n",
        "        out_seq = tf.transpose(out_seq)\n",
        "        states_seq = [\n",
        "            tf.transpose(states, [1, 0] + list(range(2, states.shape.ndims)))\n",
        "            for states in states_seq\n",
        "        ]\n",
        "\n",
        "        return out_seq, tf.nn.log_softmax(logits_seq)\n",
        "\n",
        "\n",
        "### Utility functions ###\n",
        "\n",
        "def initialize_uninitialized(sess=None):\n",
        "    \"\"\"\n",
        "    Initialize unitialized variables, doesn't affect those already initialized\n",
        "    :param sess: in which session to initialize stuff. Defaults to tf.get_default_session()\n",
        "    \"\"\"\n",
        "    sess = sess or tf.get_default_session()\n",
        "    global_vars = tf.global_variables()\n",
        "    is_not_initialized = sess.run(\n",
        "        [tf.is_variable_initialized(var) for var in global_vars]\n",
        "    )\n",
        "    not_initialized_vars = [\n",
        "        v for (v, f)\n",
        "        in zip(global_vars, is_not_initialized)\n",
        "        if not f\n",
        "    ]\n",
        "\n",
        "    if len(not_initialized_vars):\n",
        "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
        "\n",
        "\n",
        "def infer_length(seq, eos_ix, time_major=False, dtype=tf.int32):\n",
        "    \"\"\"\n",
        "    compute length given output indices and eos code\n",
        "    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n",
        "    :param eos_ix: integer index of end-of-sentence token\n",
        "    :returns: lengths, int32 vector of shape [batch]\n",
        "    \"\"\"\n",
        "    axis = 0 if time_major else 1\n",
        "    is_eos = tf.cast(tf.equal(seq, eos_ix), dtype)\n",
        "    count_eos = tf.cumsum(is_eos, axis=axis, exclusive=True)\n",
        "    lengths = tf.reduce_sum(tf.cast(tf.equal(count_eos, 0), dtype), axis=axis)\n",
        "    return lengths\n",
        "\n",
        "\n",
        "def infer_mask(seq, eos_ix, time_major=False, dtype=tf.float32):\n",
        "    \"\"\"\n",
        "    compute mask given output indices and eos code\n",
        "    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n",
        "    :param eos_ix: integer index of end-of-sentence token\n",
        "    :returns: mask, float32 matrix with '0's and '1's of same shape as seq\n",
        "    \"\"\"\n",
        "    axis = 0 if time_major else 1\n",
        "    lengths = infer_length(seq, eos_ix, time_major=time_major)\n",
        "    mask = tf.sequence_mask(lengths, maxlen=tf.shape(seq)[axis], dtype=dtype)\n",
        "    if time_major:\n",
        "        mask = tf.transpose(mask)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def select_values_over_last_axis(values, indices):\n",
        "    \"\"\"\n",
        "    Auxiliary function to select logits corresponding to chosen tokens.\n",
        "    :param values: logits for all actions: float32[batch,tick,action]\n",
        "    :param indices: action ids int32[batch,tick]\n",
        "    :returns: values selected for the given actions: float[batch,tick]\n",
        "    \"\"\"\n",
        "    assert values.shape.ndims == 3 and indices.shape.ndims == 2\n",
        "    batch_size, seq_len = tf.shape(indices)[0], tf.shape(indices)[1]\n",
        "    batch_i = tf.tile(tf.range(0, batch_size)[:, None], [1, seq_len])\n",
        "    time_i = tf.tile(tf.range(0, seq_len)[None, :], [batch_size, 1])\n",
        "    indices_nd = tf.stack([batch_i, time_i, indices], axis=-1)\n",
        "\n",
        "    return tf.gather_nd(values, indices_nd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCK4NAw548gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BasicTranslationModel('model', inp_voc, out_voc,emb_size=64, hid_size=128)\n",
        "\n",
        "s.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6L1DRZu5DEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Play around with symbolic_translate and symbolic_score\n",
        "inp = tf.placeholder_with_default(np.random.randint(\n",
        "    0, 10, [3, 5], dtype='int32'), [None, None])\n",
        "out = tf.placeholder_with_default(np.random.randint(\n",
        "    0, 10, [3, 5], dtype='int32'), [None, None])\n",
        "\n",
        "# translate inp (with untrained model)\n",
        "sampled_out, logp = model.symbolic_translate(inp, greedy=False)\n",
        "print(\"\\nSymbolic_translate output:\\n\", sampled_out, logp)\n",
        "print(\"\\nSample translations:\\n\", s.run(sampled_out))\n",
        "\n",
        "# score logp(out | inp) with untrained input\n",
        "logp = model.symbolic_score(inp, out)\n",
        "print(\"\\nSymbolic_score output:\\n\", logp)\n",
        "print(\"\\nLog-probabilities (clipped):\\n\", s.run(logp)[:, :2, :5])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckx5mX165OpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare any operations you want here\n",
        "input_sequence = tf.placeholder('int32', [None, None])\n",
        "greedy_translations, logp = <build symbolic translations with greedy = True >\n",
        "\n",
        "\n",
        "def translate(lines):\n",
        "    \"\"\"\n",
        "    You are given a list of input lines. \n",
        "    Make your neural network translate them.\n",
        "    :return: a list of output lines\n",
        "    \"\"\"\n",
        "    # Convert lines to a matrix of indices\n",
        "    lines_ix = <YOUR CODE >\n",
        "\n",
        "    # Compute translations in form of indices\n",
        "    trans_ix = s.run(greedy_translations, { < YOUR CODE - feed dict > })\n",
        "\n",
        "    # Convert translations back into strings\n",
        "    return out_voc.to_lines(trans_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLbPm9mG5bDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Sample inputs:\", all_words[:3])\n",
        "print(\"Dummy translations:\", translate(all_words[:3]))\n",
        "\n",
        "assert isinstance(greedy_translations,\n",
        "                  tf.Tensor) and greedy_translations.dtype.is_integer, \"trans must be a tensor of integers (token ids)\"\n",
        "assert translate(all_words[:3]) == translate(\n",
        "    all_words[:3]), \"make sure translation is deterministic (use greedy=True and disable any noise layers)\"\n",
        "assert type(translate(all_words[:3])) is list and (type(translate(all_words[:1])[0]) is str or type(\n",
        "    translate(all_words[:1])[0]) is unicode), \"translate(lines) must return a sequence of strings!\"\n",
        "print(\"Tests passed!\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}