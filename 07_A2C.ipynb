{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_A2C.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/07_A2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LHL_ua138iZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f6490458-ceec-44fe-b657-a6c3b1804520"
      },
      "source": [
        "# # in google colab uncomment this\n",
        "\n",
        "import os\n",
        "\n",
        "os.system('apt-get install -y xvfb')\n",
        "os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "os.system('apt-get install -y python-opengl ffmpeg')\n",
        "os.system('pip install pyglet==1.2.4')\n",
        "\n",
        "os.system('python -m pip install -U pygame --user')\n",
        "\n",
        "print('setup complete')\n",
        "\n",
        "# XVFB will be launched if you run on a server\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setup complete\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wye7HMPw4QoP",
        "colab_type": "text"
      },
      "source": [
        "### 实现A2C(Adavantage-Actor Critic)算法\n",
        "\n",
        "利用多个Atari 2600环境并行训练agent\n",
        "\n",
        "atari_wrappers.py 提供了observations的预处理操作,包括：resize,grayscal,take max between frames, skip frames and stack frames。\n",
        "\n",
        "env_batch.py 提供了并行运行多个环境的功能。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvCe8jBA7jjO",
        "colab_type": "text"
      },
      "source": [
        "### 并行运行多个环境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fG-sUaA7ElL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pylint: skip-file\n",
        "from multiprocessing import Process, Pipe\n",
        "\n",
        "from gym import Env, Wrapper, Space\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SpaceBatch(Space):\n",
        "    def __init__(self, spaces):\n",
        "        first_type = type(spaces[0])\n",
        "        first_shape = spaces[0].shape\n",
        "        first_dtype = spaces[0].dtype\n",
        "        for space in spaces:\n",
        "            if not isinstance(space, first_type):\n",
        "                raise TypeError(\"spaces have different types: {}, {}\"\n",
        "                                .format(first_type, type(space)))\n",
        "            if first_shape != space.shape:\n",
        "                raise ValueError(\"spaces have different shapes: {}, {}\"\n",
        "                                 .format(first_shape, space.shape))\n",
        "            if first_dtype != space.dtype:\n",
        "                raise ValueError(\"spaces have different data types: {}, {}\"\n",
        "                                 .format(first_dtype, space.dtype))\n",
        "\n",
        "        self.spaces = spaces\n",
        "        super(SpaceBatch, self).__init__(shape=self.spaces[0].shape,\n",
        "                                         dtype=self.spaces[0].dtype)\n",
        "\n",
        "    def sample(self):\n",
        "        return np.stack([space.sample() for space in self.spaces])\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        return getattr(self.spaces[0], attr)\n",
        "\n",
        "\n",
        "class EnvBatch(Env):\n",
        "    def __init__(self, make_env, nenvs=None):\n",
        "        make_env_functions = self._get_make_env_functions(make_env, nenvs)\n",
        "        self._envs = [make_env() for make_env in make_env_functions]\n",
        "        self._nenvs = len(self.envs)\n",
        "        # self.observation_space = SpaceBatch([env.observation_space\n",
        "        #                                      for env in self._envs])\n",
        "        self.action_space = SpaceBatch([env.action_space\n",
        "                                        for env in self._envs])\n",
        "\n",
        "    def _get_make_env_functions(self, make_env, nenvs):\n",
        "        if nenvs is None and not isinstance(make_env, list):\n",
        "            raise ValueError(\"When nenvs is None make_env\"\n",
        "                             \" must be a list of callables\")\n",
        "        if nenvs is not None and not callable(make_env):\n",
        "            raise ValueError(\n",
        "                \"When nenvs is not None make_env must be callable\")\n",
        "\n",
        "        if nenvs is not None:\n",
        "            make_env = [make_env for _ in range(nenvs)]\n",
        "        return make_env\n",
        "\n",
        "    @property\n",
        "    def nenvs(self):\n",
        "        return self._nenvs\n",
        "\n",
        "    @property\n",
        "    def envs(self):\n",
        "        return self._envs\n",
        "\n",
        "    def _check_actions(self, actions):\n",
        "        if not len(actions) == self.nenvs:\n",
        "            raise ValueError(\n",
        "                \"number of actions is not equal to number of envs: \"\n",
        "                \"len(actions) = {}, nenvs = {}\"\n",
        "                .format(len(actions), self.nenvs))\n",
        "\n",
        "    def step(self, actions):\n",
        "        self._check_actions(actions)\n",
        "        obs, rews, resets, infos = [], [], [], []\n",
        "        for env, action in zip(self._envs, actions):\n",
        "            ob, rew, done, info = env.step(action)\n",
        "            if done:\n",
        "                ob = env.reset()\n",
        "            obs.append(ob)\n",
        "            rews.append(rew)\n",
        "            resets.append(done)\n",
        "            infos.append(info)\n",
        "        return np.stack(obs), np.stack(rews), np.stack(resets), infos\n",
        "\n",
        "    def reset(self):\n",
        "        return np.stack([env.reset() for env in self.envs])\n",
        "\n",
        "\n",
        "class SingleEnvBatch(Wrapper, EnvBatch):\n",
        "    def __init__(self, env):\n",
        "        super(SingleEnvBatch, self).__init__(env)\n",
        "        self.observation_space = SpaceBatch([self.env.observation_space])\n",
        "        self.action_space = SpaceBatch([self.env.action_space])\n",
        "\n",
        "    @property\n",
        "    def nenvs(self):\n",
        "        return 1\n",
        "\n",
        "    @property\n",
        "    def envs(self):\n",
        "        return [self.env]\n",
        "\n",
        "    def step(self, actions):\n",
        "        self._check_actions(actions)\n",
        "        ob, rew, done, info = self.env.step(actions[0])\n",
        "        if done:\n",
        "            ob = self.env.reset()\n",
        "        return (\n",
        "            ob[None],\n",
        "            np.expand_dims(rew, 0),\n",
        "            np.expand_dims(done, 0),\n",
        "            [info],\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()[None]\n",
        "\n",
        "\n",
        "def worker(parent_connection, worker_connection, make_env_function,\n",
        "           send_spaces=True):\n",
        "    # Adapted from SubprocVecEnv github.com/openai/baselines\n",
        "    parent_connection.close()\n",
        "    env = make_env_function()\n",
        "    if send_spaces:\n",
        "        worker_connection.send((env.observation_space, env.action_space))\n",
        "    while True:\n",
        "        cmd, action = worker_connection.recv()\n",
        "        if cmd == \"step\":\n",
        "            ob, rew, done, info = env.step(action)\n",
        "            if done:\n",
        "                ob = env.reset()\n",
        "            worker_connection.send((ob, rew, done, info))\n",
        "        elif cmd == \"reset\":\n",
        "            ob = env.reset()\n",
        "            worker_connection.send(ob)\n",
        "        elif cmd == \"close\":\n",
        "            env.close()\n",
        "            worker_connection.close()\n",
        "            break\n",
        "        else:\n",
        "            raise NotImplementedError(\"Unknown command %s\" % cmd)\n",
        "\n",
        "\n",
        "class ParallelEnvBatch(EnvBatch):\n",
        "    \"\"\"\n",
        "    An abstract batch of environments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, make_env, nenvs=None):\n",
        "        make_env_functions = self._get_make_env_functions(make_env, nenvs)\n",
        "        self._nenvs = len(make_env_functions)\n",
        "        self._parent_connections, self._worker_connections = zip(*[\n",
        "            Pipe() for _ in range(self._nenvs)\n",
        "        ])\n",
        "        self._processes = [\n",
        "            Process(\n",
        "                target=worker,\n",
        "                args=(parent_connection, worker_connection, make_env),\n",
        "                daemon=True\n",
        "            )\n",
        "            for i, (parent_connection, worker_connection, make_env)\n",
        "            in enumerate(zip(self._parent_connections,\n",
        "                             self._worker_connections,\n",
        "                             make_env_functions))\n",
        "        ]\n",
        "        for p in self._processes:\n",
        "            p.start()\n",
        "        self._closed = False\n",
        "\n",
        "        for conn in self._worker_connections:\n",
        "            conn.close()\n",
        "\n",
        "        observation_spaces, action_spaces = [], []\n",
        "        for conn in self._parent_connections:\n",
        "            ob_space, ac_space = conn.recv()\n",
        "            observation_spaces.append(ob_space)\n",
        "            action_spaces.append(ac_space)\n",
        "        self.observation_space = SpaceBatch(observation_spaces)\n",
        "        self.action_space = SpaceBatch(action_spaces)\n",
        "\n",
        "    @property\n",
        "    def nenvs(self):\n",
        "        return self._nenvs\n",
        "\n",
        "    def step(self, actions):\n",
        "        self._check_actions(actions)\n",
        "        for conn, a in zip(self._parent_connections, actions):\n",
        "            conn.send((\"step\", a))\n",
        "        results = [conn.recv() for conn in self._parent_connections]\n",
        "        obs, rews, dones, infos = zip(*results)\n",
        "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
        "\n",
        "    def reset(self):\n",
        "        for conn in self._parent_connections:\n",
        "            conn.send((\"reset\", None))\n",
        "        return np.stack([conn.recv() for conn in self._parent_connections])\n",
        "\n",
        "    def close(self):\n",
        "        if self._closed:\n",
        "            return\n",
        "        for conn in self._parent_connections:\n",
        "            conn.send((\"close\", None))\n",
        "        for p in self._processes:\n",
        "            p.join()\n",
        "        self._closed = True\n",
        "\n",
        "    def render(self):\n",
        "        raise ValueError(\"render not defined for %s\" % self)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLAoy8Re7olb",
        "colab_type": "text"
      },
      "source": [
        "### 预处理observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l26uqs361b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Environment wrappers. \"\"\"\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import gym\n",
        "import gym.spaces as spaces\n",
        "from gym.envs import atari\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "\n",
        "class EpisodicLife(gym.Wrapper):\n",
        "    \"\"\" Sets done flag to true when agent dies. \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(EpisodicLife, self).__init__(env)\n",
        "        self.lives = 0\n",
        "        self.real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rew, done, info = self.env.step(action)\n",
        "        self.real_done = done\n",
        "        info[\"real_done\"] = done\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if 0 < lives < self.lives:\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, rew, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        if self.real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class FireReset(gym.Wrapper):\n",
        "    \"\"\" Makes fire action when reseting environment.\n",
        "    Some environments are fixed until the agent makes the fire action,\n",
        "    this wrapper makes this action so that the epsiode starts automatically.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(FireReset, self).__init__(env)\n",
        "        action_meanings = env.unwrapped.get_action_meanings()\n",
        "        if len(action_meanings) < 3:\n",
        "            raise ValueError(\n",
        "                \"env.unwrapped.get_action_meanings() must be of length >= 3\"\n",
        "                f\"but is of length {len(action_meanings)}\")\n",
        "        if env.unwrapped.get_action_meanings()[1] != \"FIRE\":\n",
        "            raise ValueError(\n",
        "                \"env.unwrapped.get_action_meanings() must have 'FIRE' \"\n",
        "                f\"under index 1, but is {action_meanings}\")\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class StartWithRandomActions(gym.Wrapper):\n",
        "    \"\"\" Makes random number of random actions at the beginning of each\n",
        "    episode. \"\"\"\n",
        "\n",
        "    def __init__(self, env, max_random_actions=30):\n",
        "        super(StartWithRandomActions, self).__init__(env)\n",
        "        self.max_random_actions = max_random_actions\n",
        "        self.real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rew, done, info = self.env.step(action)\n",
        "        self.real_done = info.get(\"real_done\", True)\n",
        "        return obs, rew, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs = self.env.reset()\n",
        "        if self.real_done:\n",
        "            num_random_actions = np.random.randint(self.max_random_actions + 1)\n",
        "            for _ in range(num_random_actions):\n",
        "                obs, _, _, _ = self.env.step(self.env.action_space.sample())\n",
        "            self.real_done = False\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ImagePreprocessing(gym.ObservationWrapper):\n",
        "    \"\"\" Preprocesses image-observations by possibly grayscaling and resizing. \"\"\"\n",
        "\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True):\n",
        "        super(ImagePreprocessing, self).__init__(env)\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.grayscale = grayscale\n",
        "        ospace = self.env.observation_space\n",
        "        low, high, dtype = ospace.low.min(), ospace.high.max(), ospace.dtype\n",
        "        if self.grayscale:\n",
        "            self.observation_space = spaces.Box(\n",
        "                low=low,\n",
        "                high=high,\n",
        "                shape=(width, height),\n",
        "                dtype=dtype,\n",
        "            )\n",
        "        else:\n",
        "            obs_shape = (width, height) + self.observation_space.shape[2:]\n",
        "            self.observation_space = spaces.Box(low=low, high=high,\n",
        "                                                shape=obs_shape, dtype=dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        \"\"\" Performs image preprocessing. \"\"\"\n",
        "        if self.grayscale:\n",
        "            observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        observation = cv2.resize(observation, (self.width, self.height),\n",
        "                                 cv2.INTER_AREA)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class MaxBetweenFrames(gym.ObservationWrapper):\n",
        "    \"\"\" Takes maximum between two subsequent frames. \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        if (isinstance(env.unwrapped, atari.AtariEnv) and\n",
        "                \"NoFrameskip\" not in env.spec.id):\n",
        "            raise ValueError(\n",
        "                \"MaxBetweenFrames requires NoFrameskip in atari env id\")\n",
        "        super(MaxBetweenFrames, self).__init__(env)\n",
        "        self.last_obs = None\n",
        "\n",
        "    def observation(self, observation):\n",
        "        obs = np.maximum(observation, self.last_obs)\n",
        "        self.last_obs = observation\n",
        "        return obs\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.last_obs = self.env.reset()\n",
        "        return self.last_obs\n",
        "\n",
        "\n",
        "class QueueFrames(gym.ObservationWrapper):\n",
        "    \"\"\" Queues specified number of frames together along new dimension. \"\"\"\n",
        "\n",
        "    def __init__(self, env, nframes, concat=False):\n",
        "        super(QueueFrames, self).__init__(env)\n",
        "        self.obs_queue = deque([], maxlen=nframes)\n",
        "        self.concat = concat\n",
        "        ospace = self.observation_space\n",
        "        if self.concat:\n",
        "            oshape = ospace.shape[:-1] + (ospace.shape[-1] * nframes,)\n",
        "        else:\n",
        "            oshape = ospace.shape + (nframes,)\n",
        "        self.observation_space = spaces.Box(\n",
        "            ospace.low.min(), ospace.high.max(), oshape, ospace.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.obs_queue.append(observation)\n",
        "        return (np.concatenate(self.obs_queue, -1) if self.concat\n",
        "                else np.dstack(self.obs_queue))\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs = self.env.reset()\n",
        "        for _ in range(self.obs_queue.maxlen - 1):\n",
        "            self.obs_queue.append(obs)\n",
        "        return self.observation(obs)\n",
        "\n",
        "\n",
        "class SkipFrames(gym.Wrapper):\n",
        "    \"\"\" Performs the same action for several steps and returns the final result.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, nskip=4):\n",
        "        super(SkipFrames, self).__init__(env)\n",
        "        if (isinstance(env.unwrapped, atari.AtariEnv) and\n",
        "                \"NoFrameskip\" not in env.spec.id):\n",
        "            raise ValueError(\"SkipFrames requires NoFrameskip in atari env id\")\n",
        "        self.nskip = nskip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        for _ in range(self.nskip):\n",
        "            obs, rew, done, info = self.env.step(action)\n",
        "            total_reward += rew\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class ClipReward(gym.RewardWrapper):\n",
        "    \"\"\" Modifes reward to be in {-1, 0, 1} by taking sign of it. \"\"\"\n",
        "\n",
        "    def reward(self, reward):\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class TFSummaries(gym.Wrapper):\n",
        "    \"\"\" Writes env summaries.\"\"\"\n",
        "\n",
        "    def __init__(self, env, prefix=None, running_mean_size=100, step_var=None):\n",
        "        super(TFSummaries, self).__init__(env)\n",
        "        self.episode_counter = 0\n",
        "        self.prefix = prefix or self.env.spec.id\n",
        "        self.step_var = (step_var if step_var is not None\n",
        "                         else tf.train.get_global_step())\n",
        "\n",
        "        nenvs = getattr(self.env.unwrapped, \"nenvs\", 1)\n",
        "        self.rewards = np.zeros(nenvs)\n",
        "        self.had_ended_episodes = np.zeros(nenvs, dtype=np.bool)\n",
        "        self.episode_lengths = np.zeros(nenvs)\n",
        "        self.reward_queues = [deque([], maxlen=running_mean_size)\n",
        "                              for _ in range(nenvs)]\n",
        "\n",
        "    def should_write_summaries(self):\n",
        "        \"\"\" Returns true if it's time to write summaries. \"\"\"\n",
        "        return np.all(self.had_ended_episodes)\n",
        "\n",
        "    def add_summaries(self):\n",
        "        \"\"\" Writes summaries. \"\"\"\n",
        "        tf.contrib.summary.scalar(\n",
        "            f\"{self.prefix}/total_reward\",\n",
        "            tf.reduce_mean([q[-1] for q in self.reward_queues]),\n",
        "            step=self.step_var)\n",
        "        tf.contrib.summary.scalar(\n",
        "            f\"{self.prefix}/reward_mean_{self.reward_queues[0].maxlen}\",\n",
        "            tf.reduce_mean([np.mean(q) for q in self.reward_queues]),\n",
        "            step=self.step_var)\n",
        "        tf.contrib.summary.scalar(\n",
        "            f\"{self.prefix}/episode_length\",\n",
        "            tf.reduce_mean(self.episode_lengths),\n",
        "            step=self.step_var)\n",
        "        if self.had_ended_episodes.size > 1:\n",
        "            tf.contrib.summary.scalar(\n",
        "                f\"{self.prefix}/min_reward\",\n",
        "                min(q[-1] for q in self.reward_queues),\n",
        "                step=self.step_var)\n",
        "            tf.contrib.summary.scalar(\n",
        "                f\"{self.prefix}/max_reward\",\n",
        "                max(q[-1] for q in self.reward_queues),\n",
        "                step=self.step_var)\n",
        "        self.episode_lengths.fill(0)\n",
        "        self.had_ended_episodes.fill(False)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rew, done, info = self.env.step(action)\n",
        "        self.rewards += rew\n",
        "        self.episode_lengths[~self.had_ended_episodes] += 1\n",
        "\n",
        "        info_collection = [info] if isinstance(info, dict) else info\n",
        "        done_collection = [done] if isinstance(done, bool) else done\n",
        "        done_indices = [i for i, info in enumerate(info_collection)\n",
        "                        if info.get(\"real_done\", done_collection[i])]\n",
        "        for i in done_indices:\n",
        "            if not self.had_ended_episodes[i]:\n",
        "                self.had_ended_episodes[i] = True\n",
        "            self.reward_queues[i].append(self.rewards[i])\n",
        "            self.rewards[i] = 0\n",
        "\n",
        "        if self.should_write_summaries():\n",
        "            self.add_summaries()\n",
        "        return obs, rew, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.rewards.fill(0)\n",
        "        self.episode_lengths.fill(0)\n",
        "        self.had_ended_episodes.fill(False)\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "def nature_dqn_env(env_id, nenvs=None, seed=None,\n",
        "                   summaries=True, clip_reward=True):\n",
        "    \"\"\" Wraps env as in Nature DQN paper. \"\"\"\n",
        "    if \"NoFrameskip\" not in env_id:\n",
        "        raise ValueError(f\"env_id must have 'NoFrameskip' but is {env_id}\")\n",
        "    if nenvs is not None:\n",
        "        if seed is None:\n",
        "            seed = list(range(nenvs))\n",
        "        if isinstance(seed, int):\n",
        "            seed = [seed] * nenvs\n",
        "        if len(seed) != nenvs:\n",
        "            raise ValueError(f\"seed has length {len(seed)} but must have \"\n",
        "                             f\"length equal to nenvs which is {nenvs}\")\n",
        "\n",
        "        env = ParallelEnvBatch([\n",
        "            lambda i=i, env_seed=env_seed: nature_dqn_env(\n",
        "                env_id, seed=env_seed, summaries=False, clip_reward=False)\n",
        "            for i, env_seed in enumerate(seed)\n",
        "        ])\n",
        "        if summaries:\n",
        "            env = TFSummaries(env, prefix=env_id)\n",
        "        if clip_reward:\n",
        "            env = ClipReward(env)\n",
        "        return env\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    env.seed(seed)\n",
        "    if summaries:\n",
        "        env = TFSummaries(env)\n",
        "    env = EpisodicLife(env)\n",
        "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
        "        env = FireReset(env)\n",
        "    env = StartWithRandomActions(env, max_random_actions=30)\n",
        "    env = MaxBetweenFrames(env)\n",
        "    env = SkipFrames(env, 4)\n",
        "    env = ImagePreprocessing(env, width=84, height=84, grayscale=True)\n",
        "    env = QueueFrames(env, 4)\n",
        "    if clip_reward:\n",
        "        env = ClipReward(env)\n",
        "    return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5IMyd524MCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "#from atari_wrappers import nature_dqn_env\n",
        "\n",
        "\n",
        "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=8)\n",
        "obs = env.reset()\n",
        "assert obs.shape == (8, 84, 84, 4)\n",
        "assert obs.dtype == np.uint8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DQTKGeV7L1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}