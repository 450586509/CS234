{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_A2C.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/07_A2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LHL_ua138iZ",
        "colab_type": "code",
        "outputId": "32dce07a-34d6-41cb-fe90-dec9555c2bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# # in google colab uncomment this\n",
        "\n",
        "import os\n",
        "\n",
        "os.system('apt-get install -y xvfb')\n",
        "os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "os.system('apt-get install -y python-opengl ffmpeg')\n",
        "os.system('pip install pyglet==1.2.4')\n",
        "\n",
        "os.system('python -m pip install -U pygame --user')\n",
        "\n",
        "print('setup complete')\n",
        "\n",
        "# XVFB will be launched if you run on a server\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setup complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wye7HMPw4QoP",
        "colab_type": "text"
      },
      "source": [
        "### 实现A2C(Adavantage-Actor Critic)算法\n",
        "\n",
        "利用多个Atari 2600环境并行训练agent\n",
        "\n",
        "atari_wrappers.py 提供了observations的预处理操作,包括：resize,grayscal,take max between frames, skip frames and stack frames。\n",
        "\n",
        "env_batch.py 提供了并行运行多个环境的功能。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvCe8jBA7jjO",
        "colab_type": "text"
      },
      "source": [
        "### 并行运行多个环境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fG-sUaA7ElL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pylint: skip-file\n",
        "from multiprocessing import Process, Pipe\n",
        "\n",
        "from gym import Env, Wrapper, Space\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SpaceBatch(Space):\n",
        "    def __init__(self, spaces):\n",
        "        first_type = type(spaces[0])\n",
        "        first_shape = spaces[0].shape\n",
        "        first_dtype = spaces[0].dtype\n",
        "        for space in spaces:\n",
        "            if not isinstance(space, first_type):\n",
        "                raise TypeError(\"spaces have different types: {}, {}\"\n",
        "                                .format(first_type, type(space)))\n",
        "            if first_shape != space.shape:\n",
        "                raise ValueError(\"spaces have different shapes: {}, {}\"\n",
        "                                 .format(first_shape, space.shape))\n",
        "            if first_dtype != space.dtype:\n",
        "                raise ValueError(\"spaces have different data types: {}, {}\"\n",
        "                                 .format(first_dtype, space.dtype))\n",
        "\n",
        "        self.spaces = spaces\n",
        "        super(SpaceBatch, self).__init__(shape=self.spaces[0].shape,\n",
        "                                         dtype=self.spaces[0].dtype)\n",
        "\n",
        "    def sample(self):\n",
        "        return np.stack([space.sample() for space in self.spaces])\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        return getattr(self.spaces[0], attr)\n",
        "\n",
        "\n",
        "class EnvBatch(Env):\n",
        "    def __init__(self, make_env, nenvs=None):\n",
        "        make_env_functions = self._get_make_env_functions(make_env, nenvs)\n",
        "        self._envs = [make_env() for make_env in make_env_functions]\n",
        "        self._nenvs = len(self.envs)\n",
        "        # self.observation_space = SpaceBatch([env.observation_space\n",
        "        #                                      for env in self._envs])\n",
        "        self.action_space = SpaceBatch([env.action_space\n",
        "                                        for env in self._envs])\n",
        "\n",
        "    def _get_make_env_functions(self, make_env, nenvs):\n",
        "        if nenvs is None and not isinstance(make_env, list):\n",
        "            raise ValueError(\"When nenvs is None make_env\"\n",
        "                             \" must be a list of callables\")\n",
        "        if nenvs is not None and not callable(make_env):\n",
        "            raise ValueError(\n",
        "                \"When nenvs is not None make_env must be callable\")\n",
        "\n",
        "        if nenvs is not None:\n",
        "            make_env = [make_env for _ in range(nenvs)]\n",
        "        return make_env\n",
        "\n",
        "    @property\n",
        "    def nenvs(self):\n",
        "        return self._nenvs\n",
        "\n",
        "    @property\n",
        "    def envs(self):\n",
        "        return self._envs\n",
        "\n",
        "    def _check_actions(self, actions):\n",
        "        if not len(actions) == self.nenvs:\n",
        "            raise ValueError(\n",
        "                \"number of actions is not equal to number of envs: \"\n",
        "                \"len(actions) = {}, nenvs = {}\"\n",
        "                .format(len(actions), self.nenvs))\n",
        "\n",
        "    def step(self, actions):\n",
        "        self._check_actions(actions)\n",
        "        obs, rews, resets, infos = [], [], [], []\n",
        "        for env, action in zip(self._envs, actions):\n",
        "            ob, rew, done, info = env.step(action)\n",
        "            if done:\n",
        "                ob = env.reset()\n",
        "            obs.append(ob)\n",
        "            rews.append(rew)\n",
        "            resets.append(done)\n",
        "            infos.append(info)\n",
        "        return np.stack(obs), np.stack(rews), np.stack(resets), infos\n",
        "\n",
        "    def reset(self):\n",
        "        return np.stack([env.reset() for env in self.envs])\n",
        "\n",
        "\n",
        "class SingleEnvBatch(Wrapper, EnvBatch):\n",
        "    def __init__(self, env):\n",
        "        super(SingleEnvBatch, self).__init__(env)\n",
        "        self.observation_space = SpaceBatch([self.env.observation_space])\n",
        "        self.action_space = SpaceBatch([self.env.action_space])\n",
        "\n",
        "    @property\n",
        "    def nenvs(self):\n",
        "        return 1\n",
        "\n",
        "    @property\n",
        "    def envs(self):\n",
        "        return [self.env]\n",
        "\n",
        "    def step(self, actions):\n",
        "        self._check_actions(actions)\n",
        "        ob, rew, done, info = self.env.step(actions[0])\n",
        "        if done:\n",
        "            ob = self.env.reset()\n",
        "        return (\n",
        "            ob[None],\n",
        "            np.expand_dims(rew, 0),\n",
        "            np.expand_dims(done, 0),\n",
        "            [info],\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()[None]\n",
        "\n",
        "\n",
        "def worker(parent_connection, worker_connection, make_env_function,\n",
        "           send_spaces=True):\n",
        "    # Adapted from SubprocVecEnv github.com/openai/baselines\n",
        "    parent_connection.close()\n",
        "    env = make_env_function()\n",
        "    if send_spaces:\n",
        "        worker_connection.send((env.observation_space, env.action_space))\n",
        "    while True:\n",
        "        cmd, action = worker_connection.recv()\n",
        "        if cmd == \"step\":\n",
        "            ob, rew, done, info = env.step(action)\n",
        "            if done:\n",
        "                ob = env.reset()\n",
        "            worker_connection.send((ob, rew, done, info))\n",
        "        elif cmd == \"reset\":\n",
        "            ob = env.reset()\n",
        "            worker_connection.send(ob)\n",
        "        elif cmd == \"close\":\n",
        "            env.close()\n",
        "            worker_connection.close()\n",
        "            break\n",
        "        else:\n",
        "            raise NotImplementedError(\"Unknown command %s\" % cmd)\n",
        "\n",
        "\n",
        "class ParallelEnvBatch(EnvBatch):\n",
        "    \"\"\"\n",
        "    An abstract batch of environments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, make_env, nenvs=None):\n",
        "        make_env_functions = self._get_make_env_functions(make_env, nenvs)\n",
        "        self._nenvs = len(make_env_functions)\n",
        "        self._parent_connections, self._worker_connections = zip(*[\n",
        "            Pipe() for _ in range(self._nenvs)\n",
        "        ])\n",
        "        self._processes = [\n",
        "            Process(\n",
        "                target=worker,\n",
        "                args=(parent_connection, worker_connection, make_env),\n",
        "                daemon=True\n",
        "            )\n",
        "            for i, (parent_connection, worker_connection, make_env)\n",
        "            in enumerate(zip(self._parent_connections,\n",
        "                             self._worker_connections,\n",
        "                             make_env_functions))\n",
        "        ]\n",
        "        for p in self._processes:\n",
        "            p.start()\n",
        "        self._closed = False\n",
        "\n",
        "        for conn in self._worker_connections:\n",
        "            conn.close()\n",
        "\n",
        "        observation_spaces, action_spaces = [], []\n",
        "        for conn in self._parent_connections:\n",
        "            ob_space, ac_space = conn.recv()\n",
        "            observation_spaces.append(ob_space)\n",
        "            action_spaces.append(ac_space)\n",
        "        self.observation_space = SpaceBatch(observation_spaces)\n",
        "        self.action_space = SpaceBatch(action_spaces)\n",
        "\n",
        "    @property\n",
        "    def nenvs(self):\n",
        "        return self._nenvs\n",
        "\n",
        "    def step(self, actions):\n",
        "        self._check_actions(actions)\n",
        "        for conn, a in zip(self._parent_connections, actions):\n",
        "            conn.send((\"step\", a))\n",
        "        results = [conn.recv() for conn in self._parent_connections]\n",
        "        obs, rews, dones, infos = zip(*results)\n",
        "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
        "\n",
        "    def reset(self):\n",
        "        for conn in self._parent_connections:\n",
        "            conn.send((\"reset\", None))\n",
        "        return np.stack([conn.recv() for conn in self._parent_connections])\n",
        "\n",
        "    def close(self):\n",
        "        if self._closed:\n",
        "            return\n",
        "        for conn in self._parent_connections:\n",
        "            conn.send((\"close\", None))\n",
        "        for p in self._processes:\n",
        "            p.join()\n",
        "        self._closed = True\n",
        "\n",
        "    def render(self):\n",
        "        raise ValueError(\"render not defined for %s\" % self)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLAoy8Re7olb",
        "colab_type": "text"
      },
      "source": [
        "### 预处理observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l26uqs361b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Environment wrappers. \"\"\"\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import gym\n",
        "import gym.spaces as spaces\n",
        "from gym.envs import atari\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "\n",
        "class EpisodicLife(gym.Wrapper):\n",
        "    \"\"\" Sets done flag to true when agent dies. \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(EpisodicLife, self).__init__(env)\n",
        "        self.lives = 0\n",
        "        self.real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rew, done, info = self.env.step(action)\n",
        "        self.real_done = done\n",
        "        info[\"real_done\"] = done\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if 0 < lives < self.lives:\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, rew, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        if self.real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class FireReset(gym.Wrapper):\n",
        "    \"\"\" Makes fire action when reseting environment.\n",
        "    Some environments are fixed until the agent makes the fire action,\n",
        "    this wrapper makes this action so that the epsiode starts automatically.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(FireReset, self).__init__(env)\n",
        "        action_meanings = env.unwrapped.get_action_meanings()\n",
        "        if len(action_meanings) < 3:\n",
        "            raise ValueError(\n",
        "                \"env.unwrapped.get_action_meanings() must be of length >= 3\"\n",
        "                f\"but is of length {len(action_meanings)}\")\n",
        "        if env.unwrapped.get_action_meanings()[1] != \"FIRE\":\n",
        "            raise ValueError(\n",
        "                \"env.unwrapped.get_action_meanings() must have 'FIRE' \"\n",
        "                f\"under index 1, but is {action_meanings}\")\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class StartWithRandomActions(gym.Wrapper):\n",
        "    \"\"\" Makes random number of random actions at the beginning of each\n",
        "    episode. \"\"\"\n",
        "\n",
        "    def __init__(self, env, max_random_actions=30):\n",
        "        super(StartWithRandomActions, self).__init__(env)\n",
        "        self.max_random_actions = max_random_actions\n",
        "        self.real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rew, done, info = self.env.step(action)\n",
        "        self.real_done = info.get(\"real_done\", True)\n",
        "        return obs, rew, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs = self.env.reset()\n",
        "        if self.real_done:\n",
        "            num_random_actions = np.random.randint(self.max_random_actions + 1)\n",
        "            for _ in range(num_random_actions):\n",
        "                obs, _, _, _ = self.env.step(self.env.action_space.sample())\n",
        "            self.real_done = False\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ImagePreprocessing(gym.ObservationWrapper):\n",
        "    \"\"\" Preprocesses image-observations by possibly grayscaling and resizing. \"\"\"\n",
        "\n",
        "    def __init__(self, env, width=84, height=84, grayscale=True):\n",
        "        super(ImagePreprocessing, self).__init__(env)\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.grayscale = grayscale\n",
        "        ospace = self.env.observation_space\n",
        "        low, high, dtype = ospace.low.min(), ospace.high.max(), ospace.dtype\n",
        "        if self.grayscale:\n",
        "            self.observation_space = spaces.Box(\n",
        "                low=low,\n",
        "                high=high,\n",
        "                shape=(width, height),\n",
        "                dtype=dtype,\n",
        "            )\n",
        "        else:\n",
        "            obs_shape = (width, height) + self.observation_space.shape[2:]\n",
        "            self.observation_space = spaces.Box(low=low, high=high,\n",
        "                                                shape=obs_shape, dtype=dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        \"\"\" Performs image preprocessing. \"\"\"\n",
        "        if self.grayscale:\n",
        "            observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "        observation = cv2.resize(observation, (self.width, self.height),\n",
        "                                 cv2.INTER_AREA)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class MaxBetweenFrames(gym.ObservationWrapper):\n",
        "    \"\"\" Takes maximum between two subsequent frames. \"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        if (isinstance(env.unwrapped, atari.AtariEnv) and\n",
        "                \"NoFrameskip\" not in env.spec.id):\n",
        "            raise ValueError(\n",
        "                \"MaxBetweenFrames requires NoFrameskip in atari env id\")\n",
        "        super(MaxBetweenFrames, self).__init__(env)\n",
        "        self.last_obs = None\n",
        "\n",
        "    def observation(self, observation):\n",
        "        obs = np.maximum(observation, self.last_obs)\n",
        "        self.last_obs = observation\n",
        "        return obs\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.last_obs = self.env.reset()\n",
        "        return self.last_obs\n",
        "\n",
        "\n",
        "class QueueFrames(gym.ObservationWrapper):\n",
        "    \"\"\" Queues specified number of frames together along new dimension. \"\"\"\n",
        "\n",
        "    def __init__(self, env, nframes, concat=False):\n",
        "        super(QueueFrames, self).__init__(env)\n",
        "        self.obs_queue = deque([], maxlen=nframes)\n",
        "        self.concat = concat\n",
        "        ospace = self.observation_space\n",
        "        if self.concat:\n",
        "            oshape = ospace.shape[:-1] + (ospace.shape[-1] * nframes,)\n",
        "        else:\n",
        "            oshape = ospace.shape + (nframes,)\n",
        "        self.observation_space = spaces.Box(\n",
        "            ospace.low.min(), ospace.high.max(), oshape, ospace.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.obs_queue.append(observation)\n",
        "        return (np.concatenate(self.obs_queue, -1) if self.concat\n",
        "                else np.dstack(self.obs_queue))\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs = self.env.reset()\n",
        "        for _ in range(self.obs_queue.maxlen - 1):\n",
        "            self.obs_queue.append(obs)\n",
        "        return self.observation(obs)\n",
        "\n",
        "\n",
        "class SkipFrames(gym.Wrapper):\n",
        "    \"\"\" Performs the same action for several steps and returns the final result.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, nskip=4):\n",
        "        super(SkipFrames, self).__init__(env)\n",
        "        if (isinstance(env.unwrapped, atari.AtariEnv) and\n",
        "                \"NoFrameskip\" not in env.spec.id):\n",
        "            raise ValueError(\"SkipFrames requires NoFrameskip in atari env id\")\n",
        "        self.nskip = nskip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        for _ in range(self.nskip):\n",
        "            obs, rew, done, info = self.env.step(action)\n",
        "            total_reward += rew\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class ClipReward(gym.RewardWrapper):\n",
        "    \"\"\" Modifes reward to be in {-1, 0, 1} by taking sign of it. \"\"\"\n",
        "\n",
        "    def reward(self, reward):\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class TFSummaries(gym.Wrapper):\n",
        "    \"\"\" Writes env summaries.\"\"\"\n",
        "\n",
        "    def __init__(self, env, prefix=None, running_mean_size=100, step_var=None):\n",
        "        super(TFSummaries, self).__init__(env)\n",
        "        self.episode_counter = 0\n",
        "        self.prefix = prefix or self.env.spec.id\n",
        "        self.step_var = (step_var if step_var is not None\n",
        "                         else tf.train.get_global_step())\n",
        "\n",
        "        nenvs = getattr(self.env.unwrapped, \"nenvs\", 1)\n",
        "        self.rewards = np.zeros(nenvs)\n",
        "        self.had_ended_episodes = np.zeros(nenvs, dtype=np.bool)\n",
        "        self.episode_lengths = np.zeros(nenvs)\n",
        "        self.reward_queues = [deque([], maxlen=running_mean_size)\n",
        "                              for _ in range(nenvs)]\n",
        "\n",
        "    def should_write_summaries(self):\n",
        "        \"\"\" Returns true if it's time to write summaries. \"\"\"\n",
        "        return np.all(self.had_ended_episodes)\n",
        "\n",
        "    def add_summaries(self):\n",
        "        \"\"\" Writes summaries. \"\"\"\n",
        "        tf.contrib.summary.scalar(\n",
        "            f\"{self.prefix}/total_reward\",\n",
        "            tf.reduce_mean([q[-1] for q in self.reward_queues]),\n",
        "            step=self.step_var)\n",
        "        tf.contrib.summary.scalar(\n",
        "            f\"{self.prefix}/reward_mean_{self.reward_queues[0].maxlen}\",\n",
        "            tf.reduce_mean([np.mean(q) for q in self.reward_queues]),\n",
        "            step=self.step_var)\n",
        "        tf.contrib.summary.scalar(\n",
        "            f\"{self.prefix}/episode_length\",\n",
        "            tf.reduce_mean(self.episode_lengths),\n",
        "            step=self.step_var)\n",
        "        if self.had_ended_episodes.size > 1:\n",
        "            tf.contrib.summary.scalar(\n",
        "                f\"{self.prefix}/min_reward\",\n",
        "                min(q[-1] for q in self.reward_queues),\n",
        "                step=self.step_var)\n",
        "            tf.contrib.summary.scalar(\n",
        "                f\"{self.prefix}/max_reward\",\n",
        "                max(q[-1] for q in self.reward_queues),\n",
        "                step=self.step_var)\n",
        "        self.episode_lengths.fill(0)\n",
        "        self.had_ended_episodes.fill(False)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rew, done, info = self.env.step(action)\n",
        "        self.rewards += rew\n",
        "        self.episode_lengths[~self.had_ended_episodes] += 1\n",
        "\n",
        "        info_collection = [info] if isinstance(info, dict) else info\n",
        "        done_collection = [done] if isinstance(done, bool) else done\n",
        "        done_indices = [i for i, info in enumerate(info_collection)\n",
        "                        if info.get(\"real_done\", done_collection[i])]\n",
        "        for i in done_indices:\n",
        "            if not self.had_ended_episodes[i]:\n",
        "                self.had_ended_episodes[i] = True\n",
        "            self.reward_queues[i].append(self.rewards[i])\n",
        "            self.rewards[i] = 0\n",
        "\n",
        "        if self.should_write_summaries():\n",
        "            self.add_summaries()\n",
        "        return obs, rew, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.rewards.fill(0)\n",
        "        self.episode_lengths.fill(0)\n",
        "        self.had_ended_episodes.fill(False)\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "def nature_dqn_env(env_id, nenvs=None, seed=None,\n",
        "                   summaries=True, clip_reward=True):\n",
        "    \"\"\" Wraps env as in Nature DQN paper. \"\"\"\n",
        "    if \"NoFrameskip\" not in env_id:\n",
        "        raise ValueError(f\"env_id must have 'NoFrameskip' but is {env_id}\")\n",
        "    if nenvs is not None:\n",
        "        if seed is None:\n",
        "            seed = list(range(nenvs))\n",
        "        if isinstance(seed, int):\n",
        "            seed = [seed] * nenvs\n",
        "        if len(seed) != nenvs:\n",
        "            raise ValueError(f\"seed has length {len(seed)} but must have \"\n",
        "                             f\"length equal to nenvs which is {nenvs}\")\n",
        "\n",
        "        env = ParallelEnvBatch([\n",
        "            lambda i=i, env_seed=env_seed: nature_dqn_env(\n",
        "                env_id, seed=env_seed, summaries=False, clip_reward=False)\n",
        "            for i, env_seed in enumerate(seed)\n",
        "        ])\n",
        "        if summaries:\n",
        "            env = TFSummaries(env, prefix=env_id)\n",
        "        if clip_reward:\n",
        "            env = ClipReward(env)\n",
        "        return env\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    env.seed(seed)\n",
        "    if summaries:\n",
        "        env = TFSummaries(env)\n",
        "    env = EpisodicLife(env)\n",
        "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
        "        env = FireReset(env)\n",
        "    env = StartWithRandomActions(env, max_random_actions=30)\n",
        "    env = MaxBetweenFrames(env)\n",
        "    env = SkipFrames(env, 4)\n",
        "    env = ImagePreprocessing(env, width=84, height=84, grayscale=True)\n",
        "    env = QueueFrames(env, 4)\n",
        "    if clip_reward:\n",
        "        env = ClipReward(env)\n",
        "    return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5IMyd524MCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=8)\n",
        "obs = env.reset()\n",
        "assert obs.shape == (8, 84, 84, 4)\n",
        "assert obs.dtype == np.uint8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrrpc6DA0fhq",
        "colab_type": "text"
      },
      "source": [
        "实现一个模型，模型输入当前状态，输出各个action的概率（action的分布）。\n",
        "\n",
        "模型和nature上DQN论文一样，但是有两个输出层。\n",
        "\n",
        "利用参数为根号2的orthogonal初始化方法初始kernels，并且将bias初始值设为0。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DQTKGeV7L1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as torch\n",
        "# import torch as tf\n",
        "import keras\n",
        "import math\n",
        "from keras.layers import Conv2D, Dense, Flatten, InputLayer, MaxPooling2D, Dense\n",
        "class Model:\n",
        "  def __init__(self, n_action, state_dim):\n",
        "    self.n_action = n_action\n",
        "    self.state_dim = state_dim\n",
        "    self.states_ph = tf.placeholder(tf.float32, (None,)+state_dim, name=\"states\")\n",
        "    self.actions_ph = tf.placeholder(tf.int32, name=\"action_ids\")\n",
        "    self.cumulative_rewards_ph = tf.placeholder(tf.float32, name=\"cumulative_returns\")\n",
        "    self.build_graph()\n",
        "\n",
        "  def build_graph(self):\n",
        "    orthogonal_init = keras.initializers.Orthogonal(gain=2 ** 0.5)\n",
        "    conv_layer_1 = Conv2D(filters=32, kernel_size=3, strides=2, activation='relu',kernel_initializer=orthogonal_init, bias_initializer='zeros')(self.states_ph)\n",
        "    conv_layer_2 = Conv2D(filters=32, kernel_size=3, strides=2, activation='relu',kernel_initializer=orthogonal_init, bias_initializer='zeros')(conv_layer_1)\n",
        "    conv_layer_3 = Conv2D(filters=64, kernel_size=3, strides=2, activation='relu',kernel_initializer=orthogonal_init, bias_initializer='zeros')(conv_layer_2)\n",
        "    pooling_layer = MaxPooling2D(pool_size=(2,2))(conv_layer_3)\n",
        "    flatten_layer = Flatten()(pooling_layer)\n",
        "    dense_layer = Dense(units=256, activation=\"relu\",)(flatten_layer)\n",
        "    logits = Dense(units=self.n_action)(dense_layer)\n",
        "    self.action_prob = tf.nn.softmax(logits)\n",
        "    log_action_prob = tf.nn.log_softmax(logits)\n",
        "    indices = tf.stack([tf.range(tf.shape(logits)[0]), self.actions_ph], axis=-1)\n",
        "    self.log_policy_for_actions = tf.gather_nd(log_action_prob, indices)\n",
        "    action_qvalue_layer = Dense(units=self.n_action)(dense_layer)\n",
        "    self.action_qvalue = tf.gather_nd(action_qvalue_layer, indices)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxL07o42uv0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DmPFbYw28AK",
        "colab_type": "text"
      },
      "source": [
        "你需要定义一个policy，并且wrap这个模型。\n",
        "\n",
        "模型计算每个状态下action的概率分布，并且提供依据action概率分布进行采样的方法\n",
        "\n",
        "policy.act should return a dictionary of all the arrays that are needed to interact with an environment and train the model.\n",
        "\n",
        "actions的类型时np.ndarray。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TyoRdn64O6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "class Policy:\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "    self.actions = []\n",
        "\n",
        "  def sample_actions(self, states):\n",
        "    # sess 是全局变量\n",
        "    action_probs = sess.run(self.model.action_prob, feed_dict={self.model.states_ph:states})\n",
        "    actions = [np.random.sample(self.actions, p=action_prob) for action_prob in action_probs]\n",
        "    #x_index = range(len(action_qvalues))\n",
        "    #y_index = actions\n",
        "    #action_qvalues_list = action_qvalues[x_index, y_index]\n",
        "    return actions\n",
        "\n",
        "  def get_action_qvalue(self, states, actions):\n",
        "    action_qvalue = sess.run(self.model.action_qvalue, feed_dict={self.model.states_ph:states, self.model.actions_ph:actions})\n",
        "    return action_qvalue\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  def act(self, inputs):\n",
        "    # inputs 是什么？\n",
        "    #<Implement policy by calling model, sampling actions and computing their log probs>\n",
        "    # Should return a dict containing keys ['actions', 'logits', 'log_probs', 'values'].\n",
        "    result = {}\n",
        "    actions = self.sample_actions(states)\n",
        "    values = self.get_action_qvalue(states, actions)\n",
        "    result[\"actions\"] = actions\n",
        "    result[\"values\"] = values\n",
        "    logits = self.model.logits\n",
        "    result[\"logits\"] = logits\n",
        "    log_probs = self.model.log_policy_for_actions\n",
        "    result[\"log_probs\"] = log_probs\n",
        "    return result\n",
        "    \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh4Ni4uI4Xxt",
        "colab_type": "text"
      },
      "source": [
        "把environment 和 policy传给runner，runner 会返回部分轨迹。这个方法已经为你实现了。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snuORZU942bJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from runners import EnvRunner\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlgsRKJRSlQ5",
        "colab_type": "text"
      },
      "source": [
        "runner和environment交互指定step后，返回一个dict，有如下key:\n",
        "\n",
        "- observations\n",
        "- rewards\n",
        "- resets\n",
        "- actions\n",
        "还有你自己再policy中定义的其它的key。\n",
        "\n",
        "key对应的value是一个长度为T的列表。对应着轨迹的值。\n",
        "\n",
        "为了训练状态价值网络，需要提供目标，计算目标价值的公式如下：\n",
        "\n",
        "$$\n",
        "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
        "$$\n",
        "trajectory['reset']判断某个状态是否为最终状态，因为影响计算target值。\n",
        "\n",
        "\n",
        " You can access trajectory['state']['latest_observation'] to get last observations in partial trajectory — $s_{t+T}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S45KyI2fVx5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "7cb53cff-d07c-4710-ac0c-bab2ffda1f9d"
      },
      "source": [
        "class ComputeValueTargets:\n",
        "  def __init__(self, policy, gamma=0.99):\n",
        "    self.policy = policy\n",
        "    \n",
        "  def __call__(self, trajectory):\n",
        "    # trajectory 结构？？\n",
        "    # This method should modify trajectory inplace by adding \n",
        "    # an item with key 'value_targets' to it. \n",
        "    #<Compute value targets for a given partial trajectory>\n",
        "    states = trajectory.get(states)\n",
        "    actions = trajectory.get(actions)\n",
        "    action_qvalue = self.policy.get_action_qvalue(states, actions)\n",
        "    return action_qvalue\n",
        "    \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-fca51ce03af1>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    <Compute value targets for a given partial trajectory>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtLwDNghV0Vr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "After computing value targets we will transform lists of interactions into tensors with the first dimension batch_size which is equal to T * nenvs, i.e. you essentially need to flatten the first two dimensions.\n",
        "\n",
        "计算value target之后，把它变成batch_size = T*nenvs，通过合并前两个维度。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp9dwJ_8V-cs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MergeTimeBatch:\n",
        "  \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
        "  def __call__(self, trajectory):\n",
        "    # Modify trajectory inplace. \n",
        "    <TODO: implement>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N20g3AUhWBKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = <Create your model here>\n",
        "policy = Policy(model)\n",
        "runner = EnvRunner(env, policy, nsteps=5,\n",
        "                   transforms=[ComputeValueTargets(),\n",
        "                               MergeTimeBatch()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D95gUt5nWCzS",
        "colab_type": "text"
      },
      "source": [
        "现在开始实现A2C算法，可以参考Mnih et al. 2016 paper和Sergey Levine 的lecture 。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIyd-BRJWIK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class A2C:\n",
        "  def __init__(self,\n",
        "               policy,\n",
        "               optimizer,\n",
        "               value_loss_coef=0.25,\n",
        "               entropy_coef=0.01,\n",
        "               max_grad_norm=0.5):\n",
        "    self.policy = policy\n",
        "    self.optimizer = optimizer\n",
        "    self.value_loss_coef = value_loss_coef\n",
        "    self.entropy_coef = entropy_coef\n",
        "    self.max_grad_norm = max_grad_norm\n",
        "  # trajectory的结构？？\n",
        "  def policy_loss(self, trajectory):\n",
        "    # You will need to compute advantages here. \n",
        "    <TODO: implement>\n",
        "  # trajectory的结构\n",
        "  def value_loss(self, trajectory):\n",
        "    <TODO: implement>\n",
        "    \n",
        "  def loss(self, trajectory):\n",
        "    <TODO: implement>\n",
        "      \n",
        "  def step(self, trajectory):\n",
        "    <TODO: implement>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twe-31beWesH",
        "colab_type": "text"
      },
      "source": [
        "然后可以开始训练上面模型。\n",
        "在合理的权重初始化情况下，在GTX1080经过1000万个batch训练后，最近100个episode的reward平均值应该有600。在训练过程中，可以绘制该值随step变化的图像。\n",
        "\n",
        "还可以绘制下列指标：\n",
        "\n",
        "- Coefficient of Determination between value targets and value predictions\n",
        "- Entropy of the policy $\\pi$\n",
        "- Value loss\n",
        "- Policy loss\n",
        "- Value targets\n",
        "- Value predictions\n",
        "- Gradient norm\n",
        "- Advantages\n",
        "- A2C loss\n",
        "\n",
        "推荐使用RMSProp 优化算法，learning_rate初始值为7e-4，并线性降低到0，smoothing 常数设置为0.99，epsilon=le-5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QpMG_uFWZvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a2c = <Create instance of the algorithm> \n",
        "\n",
        "<Write your training loop>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZjvygnI2X21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}