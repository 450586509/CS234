{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experience_replay.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/experience_replay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zadjpWN-rSMi",
        "colab_type": "text"
      },
      "source": [
        "### Training with experience replay\n",
        "- Play game, sample <s,a,r,s'>.\n",
        "\n",
        "- Update q-values based on <s,a,r,s'>.\n",
        "\n",
        "- Store <s,a,r,s'> transition in a buffer.\n",
        "\n",
        "- If buffer is full, delete earliest data.\n",
        "\n",
        "Sample K such transitions from that buffer and update q-values based on them.\n",
        "\n",
        "To enable such training, first we must implement a memory structure that would act like such a buffer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N52eDs-krGuh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "36b59c83-5f7f-477f-b04b-87de7216cc61"
      },
      "source": [
        "# In google collab, uncomment this:\n",
        "!wget https://bit.ly/2FMJP5K -q -O setup.py\n",
        "!bash setup.py 2>&1 1>stdout.log | tee stderr.log\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-16 15:27:06--  https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 640 [text/plain]\n",
            "Saving to: ‘../xvfb’\n",
            "\n",
            "     0K                                                       100%  111M=0s\n",
            "\n",
            "2019-09-16 15:27:06 (111 MB/s) - ‘../xvfb’ saved [640/640]\n",
            "\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCU1Pppzrpvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import queue\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "\n",
        "        Note: for this assignment you can pick any data structure you want.\n",
        "              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n",
        "              However you may find out there are faster and/or more memory-efficient ways to do so.\n",
        "        \"\"\"\n",
        "        self._storage = queue.Queue()\n",
        "        self._maxsize = size\n",
        "\n",
        "        # OPTIONAL: YOUR CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        '''\n",
        "        Make sure, _storage will not exceed _maxsize. \n",
        "        Make sure, FIFO rule is being followed: the oldest examples has to be removed earlier\n",
        "        '''\n",
        "        data = (obs_t, action, reward, obs_tp1, done)\n",
        "\n",
        "        # add data to storage\n",
        "        if not self._storage.qsize == self._maxsize:\n",
        "          self._storage.put(data)\n",
        "        else:\n",
        "          self._storage.get()\n",
        "          self._storage.put(data)\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "        #idxes = <randomly generate batch_size integers to be used as indexes of samples >\n",
        "\n",
        "        # collect <s,a,r,s',done> for each index\n",
        "        samples = random.sample(self._storage.queue, batch_size)\n",
        "        states = [i[0] for i in samples]\n",
        "        actions = [i[1] for i in samples]\n",
        "        rewards = [i[2] for i in samples]\n",
        "        next_states = [i[3] for i in samples]\n",
        "        is_dones = [i[4] for i in samples]\n",
        "\n",
        "\n",
        "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(is_dones)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCZ4pTpVyPIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}