{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experience_replay.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/experience_replay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zadjpWN-rSMi",
        "colab_type": "text"
      },
      "source": [
        "### Training with experience replay\n",
        "- Play game, sample <s,a,r,s'>.\n",
        "\n",
        "- Update q-values based on <s,a,r,s'>.\n",
        "\n",
        "- Store <s,a,r,s'> transition in a buffer.\n",
        "\n",
        "- If buffer is full, delete earliest data.\n",
        "\n",
        "Sample K such transitions from that buffer and update q-values based on them.\n",
        "\n",
        "To enable such training, first we must implement a memory structure that would act like such a buffer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N52eDs-krGuh",
        "colab_type": "code",
        "outputId": "515b9a3c-1f64-4885-c482-86cf3b3554d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "# In google collab, uncomment this:\n",
        "!wget https://bit.ly/2FMJP5K -q -O setup.py\n",
        "!bash setup.py 2>&1 1>stdout.log | tee stderr.log\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-18 16:07:10--  https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 640 [text/plain]\n",
            "Saving to: ‘../xvfb’\n",
            "\n",
            "     0K                                                       100%  118M=0s\n",
            "\n",
            "2019-09-18 16:07:10 (118 MB/s) - ‘../xvfb’ saved [640/640]\n",
            "\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCU1Pppzrpvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import queue\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "\n",
        "        Note: for this assignment you can pick any data structure you want.\n",
        "              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n",
        "              However you may find out there are faster and/or more memory-efficient ways to do so.\n",
        "        \"\"\"\n",
        "        self._storage = queue.Queue()\n",
        "        self._maxsize = size\n",
        "\n",
        "        # OPTIONAL: YOUR CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._storage.qsize()\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        '''\n",
        "        Make sure, _storage will not exceed _maxsize. \n",
        "        Make sure, FIFO rule is being followed: the oldest examples has to be removed earlier\n",
        "        '''\n",
        "        data = (obs_t, action, reward, obs_tp1, done)\n",
        "\n",
        "        # add data to storage\n",
        "        if self._storage.qsize() < self._maxsize:\n",
        "          self._storage.put(data)\n",
        "        else:\n",
        "          self._storage.get()\n",
        "          self._storage.put(data)\n",
        "          #print(\"size={0}\".format(self._storage.qsize()))\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "        #idxes = <randomly generate batch_size integers to be used as indexes of samples >\n",
        "\n",
        "        # collect <s,a,r,s',done> for each index\n",
        "        if batch_size > self._storage.qsize():\n",
        "          batch_size = self._storage.qsize()\n",
        "        samples = random.sample(self._storage.queue, batch_size)\n",
        "        states = [i[0] for i in samples]\n",
        "        actions = [i[1] for i in samples]\n",
        "        rewards = [i[2] for i in samples]\n",
        "        next_states = [i[3] for i in samples]\n",
        "        is_dones = [i[4] for i in samples]\n",
        "\n",
        "\n",
        "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(is_dones)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCZ4pTpVyPIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8b5a36c1-558e-46eb-f14f-0b8882860de6"
      },
      "source": [
        "def obj2arrays(obj):\n",
        "    for x in obj:\n",
        "        yield np.array([x])\n",
        "\n",
        "def obj2sampled(obj):\n",
        "    return tuple(obj2arrays(obj))\n",
        "\n",
        "replay = ReplayBuffer(2)\n",
        "obj1 = (0, 1, 2, 3, True)\n",
        "obj2 = (4, 5, 6, 7, False)\n",
        "replay.add(*obj1)\n",
        "assert replay.sample(\n",
        "    1) == obj2sampled(obj1), \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n",
        "replay.add(*obj2)\n",
        "assert len(replay) == 2, \"Please make sure __len__ methods works as intended.\"\n",
        "replay.add(*obj2)\n",
        "assert len(replay) == 2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n",
        "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2sampled(obj2)\n",
        "replay.add(*obj1)\n",
        "assert max(len(np.unique(a)) for a in replay.sample(100)) == 2\n",
        "replay.add(*obj1)\n",
        "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2sampled(obj1)\n",
        "print(\"Success!\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg48tw9rClVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RkX1Yp4_pyK",
        "colab_type": "text"
      },
      "source": [
        "*利用experience_replay的方法，改善训练*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyjopJWO87__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "##上传qlearning.py到colab的文件中\n",
        "from qlearning import QLearningAgent\n",
        "\n",
        "env = gym.make(\"Taxi-v2\")\n",
        "n_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNe_1unF_zSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_and_train_with_replay(env, agent, replay=None, t_max=10**4, replay_batch_size=32):\n",
        "    \"\"\"\n",
        "    This function should \n",
        "    - run a full game, actions given by agent.getAction(s)\n",
        "    - train agent using agent.update(...) whenever possible\n",
        "    - return total reward\n",
        "    :param replay: ReplayBuffer where agent can store and sample (s,a,r,s',done) tuples.\n",
        "        If None, do not use experience replay\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "\n",
        "        # update agent on current transition. Use agent.update\n",
        "        agent.update(s, a, r, next_s)\n",
        "\n",
        "        if replay is not None:\n",
        "            # store current <s,a,r,s'> transition in buffer\n",
        "            replay.add(s, a, r, next_s, done)\n",
        "\n",
        "            # sample replay_batch_size random transitions from replay,\n",
        "            # then update agent on each of them in a loop\n",
        "            s_, a_, r_, next_s_, done_ = replay.sample(replay_batch_size)\n",
        "            for i in range(min(replay_batch_size,len(s_))):\n",
        "              if not done_[i]:\n",
        "                agent.update(s_[i], a_[i], r_[i], next_s_[i])\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fTLNQzdGE3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create two agents: first will use experience replay, second will not.\n",
        "\n",
        "agent_baseline = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "                                get_legal_actions=lambda s: range(n_actions))\n",
        "\n",
        "agent_replay = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "                              get_legal_actions=lambda s: range(n_actions))\n",
        "\n",
        "replay = ReplayBuffer(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2tPKEm7GLhN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "ab23f5cd-fd14-419f-d1e3-66d1f155bee8"
      },
      "source": [
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "rewards_replay, rewards_baseline = [], []\n",
        "\n",
        "for i in range(1000):\n",
        "    rewards_replay.append(\n",
        "        play_and_train_with_replay(env, agent_replay, replay))\n",
        "    rewards_baseline.append(play_and_train_with_replay(\n",
        "        env, agent_baseline, replay=None))\n",
        "\n",
        "    agent_replay.epsilon *= 0.99\n",
        "    agent_baseline.epsilon *= 0.99\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('Baseline : eps =', agent_replay.epsilon,\n",
        "              'mean reward =', np.mean(rewards_baseline[-10:]))\n",
        "        print('ExpReplay: eps =', agent_baseline.epsilon,\n",
        "              'mean reward =', np.mean(rewards_replay[-10:]))\n",
        "        plt.plot(moving_average(rewards_replay), label='exp. replay')\n",
        "        plt.plot(moving_average(rewards_baseline), label='baseline')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-099cb1cb8b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     rewards_replay.append(\n\u001b[0;32m----> 8\u001b[0;31m         play_and_train_with_replay(env, agent_replay, replay))\n\u001b[0m\u001b[1;32m      9\u001b[0m     rewards_baseline.append(play_and_train_with_replay(\n\u001b[1;32m     10\u001b[0m         env, agent_baseline, replay=None))\n",
            "\u001b[0;32m<ipython-input-62-1791d39baaf9>\u001b[0m in \u001b[0;36mplay_and_train_with_replay\u001b[0;34m(env, agent, replay, t_max, replay_batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0ms_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m               \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gcz_3q7GRUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a5f3d91b-1fa0-4bc9-a8ce-29ee23dfb07e"
      },
      "source": [
        "s_"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-b7011e41b781>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 's_' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWeurRVxIi3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}