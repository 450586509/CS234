{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/CS234/blob/master/Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elsuRZIJGDxG",
        "colab_type": "text"
      },
      "source": [
        "## Q-learning (3 points)\n",
        "\n",
        "This notebook will guide you through implementation of vanilla Q-learning algorithm.\n",
        "\n",
        "You need to implement QLearningAgent (follow instructions for each method) and use it on a number of tests below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JszMaFzsGDxK",
        "colab_type": "code",
        "outputId": "dcc6a125-57a1-459b-9076-425af62f2d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# In google collab, uncomment this:\n",
        "!wget https://bit.ly/2FMJP5K -q -O setup.py\n",
        "!bash setup.py 2>&1 1>stdout.log | tee stderr.log\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-11 12:40:20--  https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 640 [text/plain]\n",
            "Saving to: ‘../xvfb’\n",
            "\n",
            "     0K                                                       100%  136M=0s\n",
            "\n",
            "2019-09-11 12:40:20 (136 MB/s) - ‘../xvfb’ saved [640/640]\n",
            "\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF34kLzQGDxQ",
        "colab_type": "code",
        "outputId": "ef940460-9bce-4a8b-ec1e-a15d3363944e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile qlearning.py\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "        \"\"\"\n",
        "        Q-Learning Agent\n",
        "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
        "        Instance variables you have access to\n",
        "          - self.epsilon (exploration prob)\n",
        "          - self.alpha (learning rate)\n",
        "          - self.discount (discount rate aka gamma)\n",
        "\n",
        "        Functions you should use\n",
        "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
        "            which returns legal actions for a state\n",
        "          - self.get_qvalue(state,action)\n",
        "            which returns Q(state,action)\n",
        "          - self.set_qvalue(state,action,value)\n",
        "            which sets Q(state,action) := value\n",
        "        !!!Important!!!\n",
        "        Note: please avoid using self._qValues directly. \n",
        "            There's a special self.get_qvalue/set_qvalue for that.\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self, state, action, value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "    #---------------------START OF YOUR CODE---------------------#\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Compute your agent's estimate of V(s) using current q-values\n",
        "        V(s) = max_over_action Q(state,action) over possible actions.\n",
        "        Note: please take into account that q-values can be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        action_value_dict = self._qvalues[state]\n",
        "        max_value = -100\n",
        "        #for action, value in action_value_dict.items():\n",
        "        #    if value > max_value:\n",
        "        #        max_value = value\n",
        "        for action in possible_actions:\n",
        "            if self._qvalues[state][action] >= max_value:\n",
        "                max_value = self._qvalues[state][action]\n",
        "\n",
        "        return max_value\n",
        "\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "        \"\"\"\n",
        "\n",
        "        # agent parameters\n",
        "        nextState = next_state\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        \n",
        "\n",
        "        qsa = self._qvalues[state][action]\n",
        "        #print(\"nextState={0}\".format(nextState))\n",
        "        # all nextState\n",
        "        state_value = self.get_value(next_state)\n",
        "        updated_qvalue = (1-learning_rate)*qsa + learning_rate*(reward + gamma*state_value)\n",
        "        # (state, action, value)\n",
        "        self.set_qvalue(state, action, updated_qvalue)\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values). \n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "        best_action = None\n",
        "        action_value_dict = self._qvalues[state]\n",
        "        #print(\"action_value_dict={0}\".format(action_value_dict))\n",
        "        max_value = -100\n",
        "        #for action, value in action_value_dict.items():\n",
        "        for action in possible_actions:\n",
        "            #print(\"action={0}\\tvalue={0}\".format(action, value))\n",
        "            if self._qvalues[state][action] > max_value:\n",
        "                max_value = value\n",
        "                best_action = action\n",
        "        #print(\"get policy best action = {0}\".format(best_action))\n",
        "        return best_action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.  \n",
        "        With probability self.epsilon, we should take a random action.\n",
        "            otherwise - the best policy action (self.get_best_action).\n",
        "\n",
        "        Note: To pick randomly from a list, use random.choice(list). \n",
        "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
        "              and compare it with your probability\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        possibleActions = possible_actions\n",
        "        action = None\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # agent parameters:\n",
        "        epsilon = self.epsilon\n",
        "\n",
        "        best_action = self.getPolicy(state=state)\n",
        "        vals = random.choices([1, 2], k=1, weights=[epsilon, 1-epsilon])\n",
        "        #print(\"epsilon={0}\\tval={1}\".format(epsilon, vals[0]))\n",
        "        if vals[0] == 1:\n",
        "            action = random.choice(possibleActions)\n",
        "        else:\n",
        "            #print(\"get action by policy\")\n",
        "            action = best_action\n",
        "        #print(\"chosen action={0}\".format(action))\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def getPolicy(self, state):\n",
        "        \"\"\"\n",
        "          Compute the best action to take in a state. \n",
        "\n",
        "        \"\"\"\n",
        "        possibleActions = self.get_legal_actions(state)\n",
        "        #print(\"possibleActions={0}\".format(possibleActions))\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possibleActions) == 0:\n",
        "            return None\n",
        "\n",
        "        best_action = None\n",
        "\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        max_value = -100\n",
        "        best_action = None\n",
        "        for action in possibleActions:\n",
        "            if self._qvalues[state][action] >= max_value:\n",
        "                max_value = self._qvalues[state][action]\n",
        "                best_action = action\n",
        "                \n",
        "        return best_action\n",
        "      \n",
        "        \n",
        "      "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting qlearning.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0NjfBz-GDxY",
        "colab_type": "text"
      },
      "source": [
        "### Try it on taxi\n",
        "\n",
        "Here we use the qlearning agent on taxi env from openai gym.\n",
        "You will need to insert a few agent functions here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YZEIiSjGDxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"Taxi-v2\")\n",
        "\n",
        "n_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsNMhrg9GDxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from qlearning import QLearningAgent\n",
        "\n",
        "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "                       get_legal_actions=lambda s: range(n_actions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH4mxWHgGDxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_and_train(env, agent, t_max=10**4):\n",
        "    \"\"\"\n",
        "    This function should \n",
        "    - run a full game, actions given by agent's e-greedy policy\n",
        "    - train agent using agent.update(...) whenever it is possible\n",
        "    - return total reward\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s.\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "\n",
        "        # train (update) agent for state s\n",
        "        agent.update(s, a, r, next_s)\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWFd-YJMGDxm",
        "colab_type": "code",
        "outputId": "5f929d81-13b6-4388-bc4b-53a9252c5b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "rewards = []\n",
        "for i in range(1000):\n",
        "    #print(\"---------------\")\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "    agent.epsilon *= 0.99\n",
        "    #print(agent._qvalues)\n",
        "    #for k,v in agent._qvalues.items():\n",
        "    #  print(\"k={0}\\nv={1}\".format(k,v))\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eps = 2.9191091959171894e-05 mean reward = 8.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2kWeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_UDKLdeGDxs",
        "colab_type": "text"
      },
      "source": [
        "# Binarized state spaces\n",
        "\n",
        "Use agent to train efficiently on CartPole-v0.\n",
        "This environment has a continuous set of possible states, so you will have to group them into bins somehow.\n",
        "\n",
        "The simplest way is to use `round(x,n_digits)` (or numpy round) to round real number to a given amount of digits.\n",
        "\n",
        "The tricky part is to get the n_digits right for each state to train effectively.\n",
        "\n",
        "Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79qoEBI8GDxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(\"first state:%s\" % (env.reset()))\n",
        "plt.imshow(env.render('rgb_array'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfW8DHChGDxy",
        "colab_type": "text"
      },
      "source": [
        "### Play a few games\n",
        "\n",
        "We need to estimate observation distributions. To do so, we'll play a few games and record all states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAEAcIkcGDxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_states = []\n",
        "for _ in range(1000):\n",
        "    all_states.append(env.reset())\n",
        "    done = False\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(env.action_space.sample())\n",
        "        all_states.append(s)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "all_states = np.array(all_states)\n",
        "\n",
        "for obs_i in range(env.observation_space.shape[0]):\n",
        "    plt.hist(all_states[:, obs_i], bins=20)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VL4VaRSGDx5",
        "colab_type": "text"
      },
      "source": [
        "## Binarize environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLQRqIi7GDx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.core import ObservationWrapper\n",
        "\n",
        "\n",
        "class Binarizer(ObservationWrapper):\n",
        "\n",
        "    def observation(self, state):\n",
        "\n",
        "        # state = <round state to some amount digits.>\n",
        "        # hint: you can do that with round(x,n_digits)\n",
        "        # you will need to pick a different n_digits for each dimension\n",
        "\n",
        "        return tuple(state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJnXxmq5GDyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Binarizer(gym.make(\"CartPole-v0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWywRC-NGDyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_states = []\n",
        "for _ in range(1000):\n",
        "    all_states.append(env.reset())\n",
        "    done = False\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(env.action_space.sample())\n",
        "        all_states.append(s)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "all_states = np.array(all_states)\n",
        "\n",
        "for obs_i in range(env.observation_space.shape[0]):\n",
        "\n",
        "    plt.hist(all_states[:, obs_i], bins=20)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BFQH4VVGDyK",
        "colab_type": "text"
      },
      "source": [
        "## Learn binarized policy\n",
        "\n",
        "Now let's train a policy that uses binarized state space.\n",
        "\n",
        "__Tips:__ \n",
        "* If your binarization is too coarse, your agent may fail to find optimal policy. In that case, change binarization. \n",
        "* If your binarization is too fine-grained, your agent will take much longer than 1000 steps to converge. You can either increase number of iterations and decrease epsilon decay or change binarization.\n",
        "* Having 10^3 ~ 10^4 distinct states is recommended (`len(QLearningAgent._qvalues)`), but not required.\n",
        "* A reasonable agent should get to an average reward of >=50."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um31lj94GDyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "                       get_legal_actions=lambda s: range(n_actions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBBSr2ddGDyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards = []\n",
        "for i in range(1000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "\n",
        "    # OPTIONAL YOUR CODE: adjust epsilon\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}