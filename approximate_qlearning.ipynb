{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "approximate_qlearning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/approximate_qlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1e8BuGfR7Cl",
        "colab_type": "text"
      },
      "source": [
        "利用tensorflow 实现qlearning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMIi5JZqRo1D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0559a93d-4206-4396-cb9a-1df9f5552d07"
      },
      "source": [
        "import os\n",
        "os.system('apt-get install -y xvfb')\n",
        "os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "os.system('apt-get install -y python-opengl ffmpeg')\n",
        "os.system('pip install pyglet==1.2.4')\n",
        "\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXrxmlq5R21W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSXUozx1SbxX",
        "colab_type": "code",
        "outputId": "b8d3f1aa-a9a9-4476-8452-673371972e56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "env = gym.make(\"CartPole-v0\").env\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f22cb14e518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nydywf4kAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQXMq0jfPz2j",
        "colab_type": "text"
      },
      "source": [
        "够着一个简单的网络表示q-table\n",
        "- 网络的输入为状态s，输出为q(s,a),输出神经元的数量等于action的数量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgpBDKdUQk1X",
        "colab_type": "text"
      },
      "source": [
        "todo:\n",
        "- 基于keras实现，两层的全连接网络，hidden_size，分别是[100,100,action_num]\n",
        "- 不用激励函数。\n",
        "- 实现get_action(state, network, epsilon=0)\n",
        "- network如何训练？训练数据？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z29Za8wBQb57",
        "colab_type": "code",
        "outputId": "4984c1c9-3bc6-4923-e6cb-d5c05732f78c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "state_dim"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSVtjKX9uPie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "network = keras.models.Sequential()\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "keras.backend.set_session(sess)\n",
        "network.add(layers.Dense(units=128, activation='relu', input_dim=state_dim))\n",
        "network.add(layers.Dense(units=128, activation='relu'))\n",
        "network.add(layers.Dense(units=n_actions))\n",
        "network.compile(loss='mean_squared_error',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMs3HF6FQb2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bS52weFuUAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "def get_action(state, epsilon=0):\n",
        "    \"\"\"\n",
        "    sample actions with epsilon-greedy policy\n",
        "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
        "    \"\"\"\n",
        "    s = np.array([state])\n",
        "    q_values = network.predict(s)[0]\n",
        "    #print(\"q_values={0}\".format(q_values))\n",
        "    \n",
        "    ###YOUR CODE\n",
        "    best_action = np.argmax(q_values)\n",
        "    random_actions = random.choice(list(range(len(q_values))),1)\n",
        "    random_action=random_actions[0]\n",
        "    flag = random.choice([True, False],1,p=[epsilon, 1-epsilon])\n",
        "    if flag[0]:\n",
        "      return random_action\n",
        "    return best_action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQK7w4L2ufWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert network.output_shape == (None, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
        "assert network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
        "s = env.reset()\n",
        "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtJY0yadQvqT",
        "colab_type": "code",
        "outputId": "679e4e1e-4074-4fae-fe68-8dce207b8c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "for eps in [0., 0.1, 0.5, 1.0]:\n",
        "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
        "    best_action = state_frequencies.argmax()\n",
        "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
        "    for other_action in range(n_actions):\n",
        "        if other_action != best_action:\n",
        "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
        "    print('e=%.1f tests passed'%eps)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e=0.0 tests passed\n",
            "e=0.1 tests passed\n",
            "e=0.5 tests passed\n",
            "e=1.0 tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyCKmaGQS62F",
        "colab_type": "text"
      },
      "source": [
        "### 利用gradient descent 求解qlearning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvxb3uXMTPYE",
        "colab_type": "text"
      },
      "source": [
        "We shall now train our agent's Q-function by minimizing the TD loss:$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
        "\n",
        "Where"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBC2mjtPW3DB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_tUKXu4Ncfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
        "states_ph = keras.backend.placeholder(dtype='float32', shape=(None,state_dim))\n",
        "actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n",
        "rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n",
        "next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,state_dim))\n",
        "is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnA6US_ZS6kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_qvalues = network(states_ph)\n",
        "#计算action对应的qvalue\n",
        "import keras.backend as K\n",
        "predicted_qvalues_for_actions = tf.reduce_sum(K.one_hot(actions_ph, n_actions) * predicted_qvalues, axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idz65NUWXMwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnDlS6LdN5ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 在更新值函数的时候用到，平衡reward和下一个状态的值函数\n",
        "gamma = 0.99\n",
        "# compute q-values for all actions in next states\n",
        "predicted_next_qvalues = network(next_states_ph)\n",
        "\n",
        "next_state_values = K.max(predicted_next_qvalues, axis=-1)\n",
        "# compute \"target q-values\" for loss\n",
        "\n",
        "target_qvalues_for_actions = rewards_ph + (K.max(next_state_values) * gamma)\n",
        "# tf.where相当于if。当is_done_ph is True时，返回rewards_ph, else：返回tart_qvalues_for_actions。\n",
        "# tensorflow中实现分支的方法。\n",
        "import tensorflow as tf\n",
        "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPdhyIDZTT7",
        "colab_type": "text"
      },
      "source": [
        "### 定义Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZOaPAF2WqWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 嵌套loss的实现方法。\n",
        "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
        "loss = tf.reduce_mean(loss)\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhRN1v5xaoWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"make sure you update q-values for chosen actions and not just all actions\"\n",
        "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"make sure you don't propagate gradient w.r.t. Q_(s',a')\"\n",
        "assert predicted_next_qvalues.shape.ndims == 2, \"make sure you predicted q-values for all actions in next state\"\n",
        "assert next_state_values.shape.ndims == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "assert target_qvalues_for_actions.shape.ndims == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxGcxKeRa0Gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(t_max=1000, epsilon=0, train=False):\n",
        "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
        "    total_reward = 0\n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        a = get_action(s, epsilon=epsilon)       \n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        \n",
        "        if train:\n",
        "            sess.run(train_step,{\n",
        "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
        "                next_states_ph: [next_s], is_done_ph: [done]\n",
        "            })\n",
        "\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if done: break\n",
        "            \n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7kwTLBZdGUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "epsilon = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2omJ2W0dYTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "36fb0414-33c7-4a51-8598-28e34e854585"
      },
      "source": [
        "for i in range(1000):\n",
        "    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
        "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
        "    \n",
        "    epsilon *= 0.99\n",
        "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
        "    \n",
        "    if np.mean(session_rewards) > 300:\n",
        "        print (\"You Win!\")\n",
        "        break"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #0\tmean reward = 12.670\tepsilon = 0.500\n",
            "epoch #1\tmean reward = 13.650\tepsilon = 0.495\n",
            "epoch #2\tmean reward = 15.430\tepsilon = 0.490\n",
            "epoch #3\tmean reward = 14.340\tepsilon = 0.485\n",
            "epoch #4\tmean reward = 16.080\tepsilon = 0.480\n",
            "epoch #5\tmean reward = 15.030\tepsilon = 0.475\n",
            "epoch #6\tmean reward = 15.610\tepsilon = 0.471\n",
            "epoch #7\tmean reward = 19.990\tepsilon = 0.466\n",
            "epoch #8\tmean reward = 15.820\tepsilon = 0.461\n",
            "epoch #9\tmean reward = 30.610\tepsilon = 0.457\n",
            "epoch #10\tmean reward = 35.680\tepsilon = 0.452\n",
            "epoch #11\tmean reward = 48.810\tepsilon = 0.448\n",
            "epoch #12\tmean reward = 47.100\tepsilon = 0.443\n",
            "epoch #13\tmean reward = 49.840\tepsilon = 0.439\n",
            "epoch #14\tmean reward = 71.970\tepsilon = 0.434\n",
            "epoch #15\tmean reward = 105.700\tepsilon = 0.430\n",
            "epoch #16\tmean reward = 121.320\tepsilon = 0.426\n",
            "epoch #17\tmean reward = 118.760\tepsilon = 0.421\n",
            "epoch #18\tmean reward = 169.340\tepsilon = 0.417\n",
            "epoch #19\tmean reward = 186.710\tepsilon = 0.413\n",
            "epoch #20\tmean reward = 175.290\tepsilon = 0.409\n",
            "epoch #21\tmean reward = 202.770\tepsilon = 0.405\n",
            "epoch #22\tmean reward = 259.430\tepsilon = 0.401\n",
            "epoch #23\tmean reward = 227.230\tepsilon = 0.397\n",
            "epoch #24\tmean reward = 248.860\tepsilon = 0.393\n",
            "epoch #25\tmean reward = 236.180\tepsilon = 0.389\n",
            "epoch #26\tmean reward = 272.780\tepsilon = 0.385\n",
            "epoch #27\tmean reward = 375.290\tepsilon = 0.381\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g03PxAX4eB2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}