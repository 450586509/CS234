{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "approximate_qlearning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/approximate_qlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1e8BuGfR7Cl",
        "colab_type": "text"
      },
      "source": [
        "利用tensorflow 实现qlearning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMIi5JZqRo1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.system('apt-get install -y xvfb')\n",
        "os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "os.system('apt-get install -y python-opengl ffmpeg')\n",
        "os.system('pip install pyglet==1.2.4')\n",
        "\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXrxmlq5R21W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSXUozx1SbxX",
        "colab_type": "code",
        "outputId": "c0b9ff54-a797-4ff8-ddd6-406f6de77e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "env = gym.make(\"CartPole-v0\").env\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f229d8e2f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG0Gdg2IQh71hAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQXMq0jfPz2j",
        "colab_type": "text"
      },
      "source": [
        "够着一个简单的网络表示q-table\n",
        "- 网络的输入为状态s，输出为q(s,a),输出神经元的数量等于action的数量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgpBDKdUQk1X",
        "colab_type": "text"
      },
      "source": [
        "todo:\n",
        "- 基于keras实现，两层的全连接网络，hidden_size，分别是[100,100,action_num]\n",
        "- 不用激励函数。\n",
        "- 实现get_action(state, network, epsilon=0)\n",
        "- network如何训练？训练数据？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z29Za8wBQb57",
        "colab_type": "code",
        "outputId": "d0daba18-9a8c-445d-8295-7f2c384e2ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "state_dim"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSVtjKX9uPie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "716e1b26-3cc9-4d27-8844-e4d25d79ed49"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "network = keras.models.Sequential()\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "keras.backend.set_session(sess)\n",
        "network.add(layers.Dense(units=128, activation='relu', input_dim=state_dim))\n",
        "network.add(layers.Dense(units=128, activation='relu'))\n",
        "network.add(layers.Dense(units=n_actions))\n",
        "network.compile(loss='mean_squared_error',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMs3HF6FQb2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bS52weFuUAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "def get_action(state, epsilon=0):\n",
        "    \"\"\"\n",
        "    sample actions with epsilon-greedy policy\n",
        "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
        "    \"\"\"\n",
        "    s = np.array([state])\n",
        "    q_values = network.predict(s)[0]\n",
        "    #print(\"q_values={0}\".format(q_values))\n",
        "    \n",
        "    ###YOUR CODE\n",
        "    best_action = np.argmax(q_values)\n",
        "    random_actions = random.choice(list(range(len(q_values))),1)\n",
        "    random_action=random_actions[0]\n",
        "    flag = random.choice([True, False],1,p=[epsilon, 1-epsilon])\n",
        "    if flag[0]:\n",
        "      return random_action\n",
        "    return best_action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQK7w4L2ufWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert network.output_shape == (None, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
        "assert network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
        "s = env.reset()\n",
        "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtJY0yadQvqT",
        "colab_type": "code",
        "outputId": "e3ec718e-c87d-4cbc-c250-40b6ee4e7944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "for eps in [0., 0.1, 0.5, 1.0]:\n",
        "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
        "    best_action = state_frequencies.argmax()\n",
        "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
        "    for other_action in range(n_actions):\n",
        "        if other_action != best_action:\n",
        "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
        "    print('e=%.1f tests passed'%eps)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e=0.0 tests passed\n",
            "e=0.1 tests passed\n",
            "e=0.5 tests passed\n",
            "e=1.0 tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyCKmaGQS62F",
        "colab_type": "text"
      },
      "source": [
        "### 利用gradient descent 求解qlearning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvxb3uXMTPYE",
        "colab_type": "text"
      },
      "source": [
        "We shall now train our agent's Q-function by minimizing the TD loss:$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$\n",
        "\n",
        "Where"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBC2mjtPW3DB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_tUKXu4Ncfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
        "states_ph = keras.backend.placeholder(dtype='float32', shape=(None,state_dim))\n",
        "actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n",
        "rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n",
        "next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,state_dim))\n",
        "is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnA6US_ZS6kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_qvalues = network(states_ph)\n",
        "#计算action对应的qvalue\n",
        "import keras.backend as K\n",
        "predicted_qvalues_for_actions = tf.reduce_sum(K.one_hot(actions_ph, n_actions) * predicted_qvalues, axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idz65NUWXMwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnDlS6LdN5ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 在更新值函数的时候用到，平衡reward和下一个状态的值函数\n",
        "gamma = 0.99\n",
        "# compute q-values for all actions in next states\n",
        "predicted_next_qvalues = network(next_states_ph)\n",
        "\n",
        "next_state_values = K.max(predicted_next_qvalues, axis=-1)\n",
        "# compute \"target q-values\" for loss\n",
        "\n",
        "target_qvalues_for_actions = rewards_ph + (K.max(next_state_values) * gamma)\n",
        "# tf.where相当于if。当is_done_ph is True时，返回rewards_ph, else：返回tart_qvalues_for_actions。\n",
        "# tensorflow中实现分支的方法。\n",
        "import tensorflow as tf\n",
        "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPdhyIDZTT7",
        "colab_type": "text"
      },
      "source": [
        "### 定义Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZOaPAF2WqWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 嵌套loss的实现方法。\n",
        "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
        "loss = tf.reduce_mean(loss)\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhRN1v5xaoWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"make sure you update q-values for chosen actions and not just all actions\"\n",
        "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"make sure you don't propagate gradient w.r.t. Q_(s',a')\"\n",
        "assert predicted_next_qvalues.shape.ndims == 2, \"make sure you predicted q-values for all actions in next state\"\n",
        "assert next_state_values.shape.ndims == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "assert target_qvalues_for_actions.shape.ndims == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxGcxKeRa0Gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(t_max=1000, epsilon=0, train=False):\n",
        "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
        "    total_reward = 0\n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        a = get_action(s, epsilon=epsilon)       \n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        \n",
        "        if train:\n",
        "            sess.run(train_step,{\n",
        "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
        "                next_states_ph: [next_s], is_done_ph: [done]\n",
        "            })\n",
        "\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if done: break\n",
        "            \n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7kwTLBZdGUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "epsilon = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2omJ2W0dYTM",
        "colab_type": "code",
        "outputId": "196a2c73-13a1-404e-9ec6-a7341d54e641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "for i in range(1000):\n",
        "    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
        "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
        "    \n",
        "    epsilon *= 0.99\n",
        "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
        "    \n",
        "    if np.mean(session_rewards) > 300:\n",
        "        print (\"You Win!\")\n",
        "        break"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #0\tmean reward = 19.930\tepsilon = 0.500\n",
            "epoch #1\tmean reward = 13.620\tepsilon = 0.495\n",
            "epoch #2\tmean reward = 13.570\tepsilon = 0.490\n",
            "epoch #3\tmean reward = 13.820\tepsilon = 0.485\n",
            "epoch #4\tmean reward = 15.120\tepsilon = 0.480\n",
            "epoch #5\tmean reward = 21.170\tepsilon = 0.475\n",
            "epoch #6\tmean reward = 22.240\tepsilon = 0.471\n",
            "epoch #7\tmean reward = 21.480\tepsilon = 0.466\n",
            "epoch #8\tmean reward = 35.360\tepsilon = 0.461\n",
            "epoch #9\tmean reward = 42.030\tepsilon = 0.457\n",
            "epoch #10\tmean reward = 43.270\tepsilon = 0.452\n",
            "epoch #11\tmean reward = 45.780\tepsilon = 0.448\n",
            "epoch #12\tmean reward = 61.500\tepsilon = 0.443\n",
            "epoch #13\tmean reward = 80.420\tepsilon = 0.439\n",
            "epoch #14\tmean reward = 114.210\tepsilon = 0.434\n",
            "epoch #15\tmean reward = 116.440\tepsilon = 0.430\n",
            "epoch #16\tmean reward = 147.430\tepsilon = 0.426\n",
            "epoch #17\tmean reward = 148.290\tepsilon = 0.421\n",
            "epoch #18\tmean reward = 165.780\tepsilon = 0.417\n",
            "epoch #19\tmean reward = 181.860\tepsilon = 0.413\n",
            "epoch #20\tmean reward = 190.290\tepsilon = 0.409\n",
            "epoch #21\tmean reward = 203.640\tepsilon = 0.405\n",
            "epoch #22\tmean reward = 199.210\tepsilon = 0.401\n",
            "epoch #23\tmean reward = 202.510\tepsilon = 0.397\n",
            "epoch #24\tmean reward = 199.730\tepsilon = 0.393\n",
            "epoch #25\tmean reward = 252.500\tepsilon = 0.389\n",
            "epoch #26\tmean reward = 288.810\tepsilon = 0.385\n",
            "epoch #27\tmean reward = 421.900\tepsilon = 0.381\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDEm-hpFtXTn",
        "colab_type": "text"
      },
      "source": [
        "### 结果展示\n",
        "利用grm.wrappers.Monitor录制游戏，方便展示效果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g03PxAX4eB2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#record sessions\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
        "sessions = [generate_session(epsilon=0, train=False) for _ in range(100)]\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e184rrnWtdI7",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/videos/openaigym.video.0.118.video000064.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "c7f8b786-2215-475a-af56-4fdbbce2f694"
      },
      "source": [
        "#show video\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
        "\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"./videos/openaigym.video.0.118.video000064.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPdGDpVqtn24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35099126-2882-49a9-dbf4-7040cd97ee0d"
      },
      "source": [
        "video_names[-1]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'openaigym.video.0.118.video000064.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cP5Kwart5xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}