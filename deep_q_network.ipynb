{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep q-network.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/deep_q_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgfYtKBKycFp",
        "colab_type": "text"
      },
      "source": [
        "### Deep Q-Network implementation\n",
        "包括三个组成部分：\n",
        "\n",
        "- approximate q-learning\n",
        "- experience replay\n",
        "- target networks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtCHMKLgyIQ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90232db3-a797-4a59-daeb-f9f5e3e2d41e"
      },
      "source": [
        "import os\n",
        "os.system('apt-get install -y xvfb')\n",
        "os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "os.system('apt-get install -y python-opengl ffmpeg')\n",
        "os.system('pip install pyglet==1.2.4')\n",
        "\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuyF1ZSCzcuO",
        "colab_type": "text"
      },
      "source": [
        "### 目标: 训练一个玩atari的agent\n",
        "游戏数据信息：\n",
        "- atari 游戏画面图像默认大小是210x160x3\n",
        "\n",
        "为了节省时间，可以对图像进行resize操作，变成64x64的黑白图像。并且去除顶部和底部无用的图像区域。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVxjCaa5y4VO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.core import ObservationWrapper\n",
        "from gym.spaces import Box\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class PreprocessAtari(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and optionally grayscales it.\"\"\"\n",
        "        ObservationWrapper.__init__(self, env)\n",
        "\n",
        "        self.img_size = (64, 64)\n",
        "        self.observation_space = Box(0.0, 1.0, self.img_size)\n",
        "        print(\"self.observation_space={0}\".format(self.observation_space.shape))\n",
        "\n",
        "    def observation(self, img):\n",
        "        \"\"\"what happens to each observation\"\"\"\n",
        "\n",
        "        # Here's what you need to do:\n",
        "        #  * crop image, remove irrelevant parts\n",
        "        #  * resize image to self.img_size\n",
        "        #     (use imresize imported above or any library you want,\n",
        "        #      e.g. opencv, skimage, PIL, keras)\n",
        "        #  * cast image to grayscale\n",
        "        #  * convert image pixels to (0,1) range, float32 type\n",
        "\n",
        "        # crop image\n",
        "        img=img[20:,:,:]\n",
        "        # resize\n",
        "        img = Image.fromarray(img.astype(np.uint8))\n",
        "        img = img.resize(self.img_size)\n",
        "        # 灰度\n",
        "        img = img.convert('L')\n",
        "\n",
        "        #归一化：\n",
        "        imgdata=np.array(img.getdata(),dtype='float32')\n",
        "        min_val = np.min(imgdata)\n",
        "        max_val = np.max(imgdata)\n",
        "        imgdata = (imgdata-min_val)/max_val\n",
        "        imgdata = imgdata.reshape((1, *self.img_size))\n",
        "        #print(\"imgdata[0,:]\".format(imgdata[0,:]))\n",
        "        print(\"imgdata shape={0}\".format(imgdata.shape))\n",
        "        return imgdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRCoWn7g1c5-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "08fd5317-f1a6-499e-c868-435ce3cedcf3"
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"BreakoutDeterministic-v0\")\n",
        "observation_shape = env.observation_space.shape\n",
        "print(\"observation_shape={0}\".format(observation_shape))\n",
        "env = PreprocessAtari(env)\n",
        "observation_shape = env.observation_space.shape\n",
        "print(\"observation_shape={0}\".format(observation_shape))\n",
        "n_actions = env.action_space.n\n",
        "obs = env.reset()\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation_shape=(210, 160, 3)\n",
            "self.observation_space=(64, 64)\n",
            "observation_shape=(64, 64)\n",
            "imgdata shape=(1, 64, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3TCHMs81rRg",
        "colab_type": "code",
        "outputId": "c3441a23-e1d3-48e3-986c-37788977f467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "assert obs.ndim == 3, \"observation must be [batch, time, channels] even if there's just one channel\"\n",
        "print(obs.shape)\n",
        "print(observation_shape)\n",
        "#assert obs.shape == observation_shape\n",
        "print(obs.dtype)\n",
        "assert obs.dtype == 'float32'\n",
        "assert len(np.unique(obs)) > 2, \"your image must not be binary\"\n",
        "assert 0 <= np.min(obs) and np.max(\n",
        "    obs) <= 1, \"convert image pixels to (0,1) range\"\n",
        "print (\"Formal tests seem fine. Here's an example of what you'll get.\")\n",
        "\n",
        "plt.title(\"what your network gonna see\")\n",
        "plt.imshow(obs[0], interpolation='none', cmap='gray')\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 64, 64)\n",
            "(64, 64)\n",
            "float32\n",
            "Formal tests seem fine. Here's an example of what you'll get.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7a98404588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAEICAYAAAB/KknhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLEMOvlmG/h/WaN2fsPZ02QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG-QSpvRMiH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYa6ASIdToQs",
        "colab_type": "text"
      },
      "source": [
        "### Frame Buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZgo6YI4Trro",
        "colab_type": "text"
      },
      "source": [
        "单张图片训练时无法反应图片的速度和运动反向。所以这个实验采用连续四张图片进行训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrOJjvB8Ursl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from gym.spaces.box import Box\n",
        "from gym.core import Wrapper\n",
        "\n",
        "\n",
        "class FrameBuffer(Wrapper):\n",
        "    def __init__(self, env, n_frames=4, dim_order='tensorflow'):\n",
        "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
        "        super(FrameBuffer, self).__init__(env)\n",
        "        self.dim_order = dim_order\n",
        "        if dim_order == 'tensorflow':\n",
        "            height, width, n_channels = env.observation_space.shape\n",
        "            obs_shape = [height, width, n_channels * n_frames]\n",
        "        elif dim_order == 'pytorch':\n",
        "            n_channels, height, width = env.observation_space.shape\n",
        "            obs_shape = [n_channels * n_frames, height, width]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                'dim_order should be \"tensorflow\" or \"pytorch\", got {}'.format(dim_order))\n",
        "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
        "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
        "        self.update_buffer(self.env.reset())\n",
        "        return self.framebuffer\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
        "        new_img, reward, done, info = self.env.step(action)\n",
        "        self.update_buffer(new_img)\n",
        "        return self.framebuffer, reward, done, info\n",
        "\n",
        "    def update_buffer(self, img):\n",
        "        if self.dim_order == 'tensorflow':\n",
        "            offset = self.env.observation_space.shape[-1]\n",
        "            axis = -1\n",
        "            cropped_framebuffer = self.framebuffer[:, :, :-offset]\n",
        "        elif self.dim_order == 'pytorch':\n",
        "            offset = self.env.observation_space.shape[0]\n",
        "            axis = 0\n",
        "            cropped_framebuffer = self.framebuffer[:-offset]\n",
        "        self.framebuffer = np.concatenate(\n",
        "            [img, cropped_framebuffer], axis=axis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la_vlmXi1i_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "c5488abc-db8b-46e7-a1c5-3778bd6586ad"
      },
      "source": [
        "\n",
        "#包括图片预处理和frame buffer功能的env\n",
        "def make_env():\n",
        "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
        "    env = PreprocessAtari(env)\n",
        "    env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n",
        "    return env\n",
        "\n",
        "\n",
        "env = make_env()\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "self.observation_space=(64, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-ed6d78b054b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-ed6d78b054b1>\u001b[0m in \u001b[0;36mmake_env\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BreakoutDeterministic-v4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreprocessAtari\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-86-fa4e4e98703b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, n_frames, dim_order)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim_order\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mobs_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_channels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdim_order\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pytorch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZgiIKNmU-e4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh6g-Nlt2PRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Iz3FB32v68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97rccZu7-Z6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}