{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_policy_based.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/06_policy_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXrbvR6gvcKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1b42d34-b7b5-4f80-a89e-4a2b4e9eabed"
      },
      "source": [
        "import os\n",
        "os.system('touch test.txt')\n",
        "os.system('apt-get install -y xvfb')\n",
        "os.system('apt-get install graphviz')\n",
        "os.system('pip install pydot pydot-ng')\n",
        "os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "os.system('apt-get install -y python-opengl ffmpeg')\n",
        "os.system('pip install pyglet==1.2.4')\n",
        "\n",
        "\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3ppBeSXvokD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "d18fb6f7-257f-4858-be46-19106dc102e7"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, 'env'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7a7a0e3668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neEnqlAEvSZ0y4CWpU/8B/vuDL84C7oMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwpUH52dyvUg",
        "colab_type": "text"
      },
      "source": [
        "### 构建策略网络\n",
        "\n",
        "策略接受状态，输出动作的分布\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRu1STAm5Im2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "80996488-6ebe-4873-98dc-12b62f5f4886"
      },
      "source": [
        "state_dim"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnd3Pr77v6q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input, Dense,InputLayer\n",
        "import keras\n",
        "action_number=3\n",
        "state_shape=(None,)+state_dim\n",
        "# create input variables. We only need <s,a,R> for REINFORCE\n",
        "states = tf.placeholder('float32', (None,)+state_dim, name=\"states\")\n",
        "actions = tf.placeholder('int32', name=\"action_ids\")\n",
        "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")\n",
        "\n",
        "network = keras.models.Sequential()\n",
        "network.add(InputLayer(state_shape))\n",
        "network.add(Dense(units=256, activation=\"relu\"))\n",
        "network.add(Dense(units=256, activation=\"relu\"))\n",
        "network.add(Dense(units=action_number, activation=\"sigmoid\"))\n",
        "\n",
        "logits = network(states)\n",
        "\n",
        "policy = tf.nn.softmax(logits)\n",
        "log_policy = tf.nn.log_softmax(logits)\n",
        "\n",
        "# utility function to pick action in one given state\n",
        "def get_action_proba(s): \n",
        "  return policy.eval({states: [s]})[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmSSQRds5n2X",
        "colab_type": "text"
      },
      "source": [
        "### loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoO9lUqg5um3",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "目标函数\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "\n",
        "等价于: \n",
        "\n",
        "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWEDsFmPzBqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "330ec3d7-4847-4112-a4c5-fbf97d9bc26d"
      },
      "source": [
        "# get probabilities for parti\n",
        "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), actions], axis=-1)\n",
        "log_policy_for_actions = tf.gather_nd(log_policy, indices)\n",
        "\n",
        "# policy objective as in the last formula. please use mean, not sum.\n",
        "# note: you need to use log_policy_for_actions to get log probabilities for actions taken.\n",
        "\n",
        "J = <YOUR CODE>\n",
        "\n",
        "# regularize with entropy\n",
        "entropy = <compute entropy. Don't forget the sign!>\n",
        "\n",
        "# all network weights\n",
        "all_weights = <a list of all trainable weights in your network >\n",
        "\n",
        "# weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
        "loss = -J - 0.1*entropy\n",
        "\n",
        "update = tf.train.AdamOptimizer().minimize(loss, var_list=all_weights)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-60fc7f7be28c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    We now need to define objective and update over policy gradient.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bONOi-raDDIS",
        "colab_type": "text"
      },
      "source": [
        "### 计算累积奖励"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b4kL-KLzWZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    take a list of immediate rewards r(s,a) for the whole session \n",
        "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
        "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
        "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "\n",
        "    <your code here >\n",
        "\n",
        "    return < array of cumulative rewards >\n",
        "assert len(get_cumulative_rewards(range(100))) == 100\n",
        "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
        "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M0gVH7jDVNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(_states, _actions, _rewards):\n",
        "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
        "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
        "    update.run({states: _states, actions: _actions,\n",
        "                cumulative_rewards: _cumulative_rewards})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2mFg8GFDX9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woklqOJZDYpI",
        "colab_type": "text"
      },
      "source": [
        "### 玩游戏"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HsDcY2sDa0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(t_max=1000):\n",
        "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
        "\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probas = get_action_proba(s)\n",
        "\n",
        "        a = <pick random action using action_probas >\n",
        "\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    train_step(states, actions, rewards)\n",
        "\n",
        "    return sum(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7q0NgXkDfcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = tf.InteractiveSession()\n",
        "s.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "    rewards = [generate_session() for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsKkb6QXDik2",
        "colab_type": "text"
      },
      "source": [
        "### 展示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGy9U6NtDlMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show video\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "video_names = list(\n",
        "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrZni1g7Dq-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}