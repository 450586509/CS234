{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_policy_based.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/06_policy_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXrbvR6gvcKB",
        "colab_type": "code",
        "outputId": "2a7ff492-49ff-4ac1-8784-ebe8a4c1e2fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.system('touch test.txt')\n",
        "os.system('apt-get install -y xvfb')\n",
        "os.system('apt-get install graphviz')\n",
        "os.system('pip install pydot pydot-ng')\n",
        "os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "os.system('apt-get install -y python-opengl ffmpeg')\n",
        "os.system('pip install pyglet==1.2.4')\n",
        "\n",
        "\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3ppBeSXvokD",
        "colab_type": "code",
        "outputId": "7bcb258f-5ff7-4e89-987b-2212e9c121a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, 'env'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc7dbbd95c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWqEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwpUH52dyvUg",
        "colab_type": "text"
      },
      "source": [
        "### 构建策略网络\n",
        "\n",
        "策略接受状态，输出动作的分布\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRu1STAm5Im2",
        "colab_type": "code",
        "outputId": "098fec2d-5f12-4aa8-fcbd-92ee85773cd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "state_dim"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnd3Pr77v6q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keras和tensorflow结合方式\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, Dense,InputLayer\n",
        "import keras\n",
        "action_number=3\n",
        "# create input variables. We only need <s,a,R> for REINFORCE\n",
        "states = tf.placeholder('float32', (None,)+state_dim, name=\"states\")\n",
        "actions = tf.placeholder('int32', name=\"action_ids\")\n",
        "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")\n",
        "\n",
        "layer1 = Dense(units=256, activation=\"relu\")(states)\n",
        "layer2 = Dense(units=256, activation=\"relu\")(layer1)\n",
        "\n",
        "logits = Dense(units=action_number, activation=\"sigmoid\")(layer2)\n",
        "policy = tf.nn.softmax(logits)\n",
        "log_policy = tf.nn.log_softmax(logits)\n",
        "\n",
        "# utility function to pick action in one given state\n",
        "def get_action_proba(s): \n",
        "  return policy.eval({states: [s]})[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmSSQRds5n2X",
        "colab_type": "text"
      },
      "source": [
        "### loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoO9lUqg5um3",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "目标函数\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "\n",
        "等价于: \n",
        "\n",
        "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWEDsFmPzBqq",
        "colab_type": "code",
        "outputId": "330ec3d7-4847-4112-a4c5-fbf97d9bc26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "# get probabilities for parti\n",
        "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), actions], axis=-1)\n",
        "log_policy_for_actions = tf.gather_nd(log_policy, indices)\n",
        "\n",
        "# policy objective as in the last formula. please use mean, not sum.\n",
        "# note: you need to use log_policy_for_actions to get log probabilities for actions taken.\n",
        "\n",
        "J = <YOUR CODE>\n",
        "\n",
        "# regularize with entropy\n",
        "entropy = <compute entropy. Don't forget the sign!>\n",
        "\n",
        "# all network weights\n",
        "all_weights = <a list of all trainable weights in your network >\n",
        "\n",
        "# weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
        "loss = -J - 0.1*entropy\n",
        "\n",
        "update = tf.train.AdamOptimizer().minimize(loss, var_list=all_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-60fc7f7be28c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    We now need to define objective and update over policy gradient.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bONOi-raDDIS",
        "colab_type": "text"
      },
      "source": [
        "### 计算累积奖励"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b4kL-KLzWZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    take a list of immediate rewards r(s,a) for the whole session \n",
        "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
        "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
        "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "\n",
        "    <your code here >\n",
        "\n",
        "    return < array of cumulative rewards >\n",
        "assert len(get_cumulative_rewards(range(100))) == 100\n",
        "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
        "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M0gVH7jDVNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(_states, _actions, _rewards):\n",
        "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
        "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
        "    update.run({states: _states, actions: _actions,\n",
        "                cumulative_rewards: _cumulative_rewards})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2mFg8GFDX9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woklqOJZDYpI",
        "colab_type": "text"
      },
      "source": [
        "### 玩游戏"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HsDcY2sDa0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(t_max=1000):\n",
        "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
        "\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probas = get_action_proba(s)\n",
        "\n",
        "        a = <pick random action using action_probas >\n",
        "\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    train_step(states, actions, rewards)\n",
        "\n",
        "    return sum(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7q0NgXkDfcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = tf.InteractiveSession()\n",
        "s.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "    rewards = [generate_session() for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsKkb6QXDik2",
        "colab_type": "text"
      },
      "source": [
        "### 展示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGy9U6NtDlMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show video\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "video_names = list(\n",
        "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrZni1g7Dq-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}