{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_explore.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/450586509/reinforcement-learning-practice/blob/master/05_explore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFtxivT0nKJx",
        "colab_type": "code",
        "outputId": "d3277d84-d5ca-46ba-dbab-ecc12a23dca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "# Run this if in COLAB\n",
        "!pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
        "!pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip\n",
        "    \n",
        "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week5_explore/bayes.py\n",
        "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week5_explore/action_rewards.npy\n",
        "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week5_explore/all_states.npy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/Theano/Theano/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/Theano/Theano/archive/master.zip\n",
            "\u001b[K     - 29.9MB 5.7MB/s\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Theano==1.0.4+unknown) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Theano==1.0.4+unknown) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Theano==1.0.4+unknown) (1.12.0)\n",
            "Building wheels for collected packages: Theano\n",
            "  Building wheel for Theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Theano: filename=Theano-1.0.4+unknown-cp36-none-any.whl size=2666479 sha256=c995836832df81f1637b03775d1ccd79e2354235eacd74464026c2bade92ef2d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-exs9j4q3/wheels/33/73/96/0ed263c62a86e2485ea634e0d3ae8169d50fd66e3b252541db\n",
            "Successfully built Theano\n",
            "Installing collected packages: Theano\n",
            "  Found existing installation: Theano 1.0.4\n",
            "    Uninstalling Theano-1.0.4:\n",
            "      Successfully uninstalled Theano-1.0.4\n",
            "Successfully installed Theano-1.0.4+unknown\n",
            "Collecting https://github.com/Lasagne/Lasagne/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/Lasagne/Lasagne/archive/master.zip\n",
            "\u001b[K     - 2.1MB 27.4MB/s\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from Lasagne==0.2.dev1) (1.16.5)\n",
            "Building wheels for collected packages: Lasagne\n",
            "  Building wheel for Lasagne (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Lasagne: filename=Lasagne-0.2.dev1-cp36-none-any.whl size=122750 sha256=00aa840d838b0656f976ca992d57d91808ce332d514ee919a27dc10dc8f9e98f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jutld35p/wheels/ca/4a/00/87f1777b229481fe76562df7c0cfb993bc88ed0cc37e3f0ed4\n",
            "Successfully built Lasagne\n",
            "Installing collected packages: Lasagne\n",
            "Successfully installed Lasagne-0.2.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TOl5KX8nMJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from abc import ABCMeta, abstractmethod, abstractproperty\n",
        "import enum\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "import pandas\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdIgtxS77evi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS_ErBzn_ZvM",
        "colab_type": "text"
      },
      "source": [
        "## Bernouli Bandit问题\n",
        "* 实现几个简单的探索策略，解决这个问题。\n",
        "* bandit有K个动作，每一个动作有 $0 \\le \\theta_k \\le 1$的概率获得reward=1的奖励，这个概率分布对于agent是未知的，但是固定不变的。agent的优化目标最小化下面的等式：$$\\rho = T\\theta^* - \\sum_{t=1}^T r_t$$T是选择次数,$\\theta^* = \\max_k\\{\\theta_k\\}$\n",
        "* \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wj5bxDH_Y8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BernoulliBandit:\n",
        "    def __init__(self, n_actions=5):\n",
        "        self._probs = np.random.random(n_actions)\n",
        "\n",
        "    @property\n",
        "    def action_count(self):\n",
        "        return len(self._probs)\n",
        "\n",
        "    def pull(self, action):\n",
        "        if np.any(np.random.random() > self._probs[action]):\n",
        "            return 0.0\n",
        "        return 1.0\n",
        "\n",
        "    def optimal_reward(self):\n",
        "        \"\"\" Used for regret calculation\n",
        "        \"\"\"\n",
        "        return np.max(self._probs)\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\" Used in nonstationary version\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Used in nonstationary version\n",
        "        \"\"\"\n",
        "class AbstractAgent(metaclass=ABCMeta):\n",
        "    def init_actions(self, n_actions):\n",
        "        self._successes = np.zeros(n_actions)\n",
        "        self._failures = np.zeros(n_actions)\n",
        "        self._total_pulls = 0\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_action(self):\n",
        "        \"\"\"\n",
        "        Get current best action\n",
        "        :rtype: int\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        \"\"\"\n",
        "        Observe reward from action and update agent's internal parameters\n",
        "        :type action: int\n",
        "        :type reward: int\n",
        "        \"\"\"\n",
        "        self._total_pulls += 1\n",
        "        if reward == 1:\n",
        "            self._successes[action] += 1\n",
        "        else:\n",
        "            self._failures[action] += 1\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self.__class__.__name__\n",
        "\n",
        "\n",
        "class RandomAgent(AbstractAgent):\n",
        "    def get_action(self):\n",
        "        return np.random.randint(0, len(self._successes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trPn4aEbBVtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlbBmrlaD4nr",
        "colab_type": "text"
      },
      "source": [
        "#### epsilon-greedy agent\n",
        "\n",
        "**for** $t = 1,2,...$ **do**\n",
        "\n",
        "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat\\theta_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k)$\n",
        "\n",
        "&nbsp;&nbsp; **end for** \n",
        "\n",
        "&nbsp;&nbsp; $x_t \\leftarrow argmax_{k}\\hat\\theta$ with probability $1 - \\epsilon$ or random action with probability $\\epsilon$\n",
        "\n",
        "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
        "\n",
        "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
        "\n",
        "**end for**\n",
        "\n",
        "Implement the algorithm above in the cell below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hABQcyuJD8rp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "8b758b08-8528-4ee6-b5a9-79c3f54fdd26"
      },
      "source": [
        "class EpsilonGreedyAgent(AbstractAgent):\n",
        "    def __init__(self, epsilon=0.01):\n",
        "        self._epsilon = epsilon\n",
        "    def best_action(self):\n",
        "      max_p = -1\n",
        "      best_action = -1\n",
        "      for action, (a, b) in enumerate(zip(self._successes, self._failures)):\n",
        "        if a + b == 0:\n",
        "          p = 0\n",
        "        else:\n",
        "          p = (1.0 * a) / (a + b)\n",
        "        if max_p < p :\n",
        "          max_p = p\n",
        "          best_action = action\n",
        "      return best_action\n",
        "          \n",
        "\n",
        "    def get_action(self):\n",
        "        # YOUR CODE HERE\n",
        "        # best_action\n",
        "        best_action = self.best_action()\n",
        "        is_choice_random_action = np.random.choice([True, False], p=[self._epsilon, 1-self._epsilon])\n",
        "        random_action = np.random.choice(range(len(self._successes)))\n",
        "        if is_choice_random_action:\n",
        "          return random_action\n",
        "        return best_action\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self.__class__.__name__ + \"(epsilon={})\".format(self._epsilon)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cf5148cd408a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEpsilonGreedyAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAbstractAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbest_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mmax_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AbstractAgent' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUoySLN3K8Rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T52NqQkFK_wh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKljH2QkLDTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ltE4tqULtl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opM-k9x8L4s0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpNqxIdjMu-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjOKaHiJNYau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}